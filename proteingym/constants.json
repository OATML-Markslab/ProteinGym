{
    "model_details":{
        "Tranception_L_no_retrieval":"Tranception Large model (700M params) without retrieval",
        "Tranception_M_no_retrieval":"Tranception Medium model (300M params) without retrieval",
        "Tranception_S_no_retrieval":"Tranception Small model (85M params) without retrieval",
        "Tranception_S":"Tranception Small model (85M params) with retrieval",
        "Tranception_M":"Tranception Medium model (300M params) with retrieval",
        "Tranception_L":"Tranception Large model (700M params) with retrieval",
        "EVE_single":"EVE model (single seed)",
        "EVE_ensemble":"EVE model (ensemble of 5 independently-trained models)",
        "MSA_Transformer_single":"MSA Transformer (single MSA sample)",
        "MSA_Transformer_ensemble":"MSA Transformer (ensemble of 5 MSA samples)",
        "ESM1v_single":"ESM-1v (single seed)",
        "ESM1v_ensemble":"ESM-1v (ensemble of 5 independently-trained models)",
        "Wavenet":"Wavenet model",
        "DeepSequence_single":"DeepSequence model (single seed)",
        "DeepSequence_ensemble":"DeepSequence model (ensemble of 5 independently-trained models)",
        "Site_Independent":"Site-Independent model",
        "EVmutation":"EVmutation model",
        "RITA_s":"RITA small model (85M params)",
        "RITA_m":"RITA medium model (300M params)",
        "RITA_l":"RITA large model (680M params)",
        "RITA_xl":"RITA xlarge model (1.2B params)",
        "RITA_ensemble":"Ensemble of the 4 RITA models",
        "Progen2_small":"Progen2 small model (150M params)",
        "Progen2_medium":"Progen2 medium model (760M params)",
        "Progen2_base":"Progen2 base model (760M params)",
        "Progen2_large":"Progen2 large model (2.7B params)",
        "Progen2_xlarge":"Progen2 xlarge model (6.4B params)",
        "Progen2_ensemble":"Ensemble of the 5 Progen2 models",
        "Progen3_112m":"Progen3 model (112m params)",
        "Progen3_219m":"Progen3 model (219m params)",
        "Progen3_339m":"Progen3 model (339m params)",
        "Progen3_762m":"Progen3 model (762m params)",
        "Progen3_1b":"Progen3 model (1B params)",
        "Progen3_3b":"Progen3 model (3B params)",
        "Unirep":"Unirep model",
        "Unirep_evotune":"Unirep model w/ evotuning",
        "GEMME":"GEMME model",
        "VESPA":"VESPA model",
        "VESPAl":"VESPAl model",
        "VespaG":"VespaG model",
        "ProtGPT2":"ProtGPT2 model",
        "ESM1b":"ESM-1b (w/ Brandes et al. extensions)",
        "TranceptEVE_S":"TranceptEVE Small model (Tranception Small & retrieved EVE model)",
        "TranceptEVE_M":"TranceptEVE Medium model (Tranception Medium & retrieved EVE model)",
        "TranceptEVE_L":"TranceptEVE Large model (Tranception Large & retrieved EVE model)",
        "HMM": "Profile Hidden Markov model",
        "Provean": "Provean model",
        "ESM-IF1": "ESM-IF1 model",
        "ESM2_650M": "ESM2 model (650M params)",
        "ESM2_3B": "ESM2 model (3B params)",
        "ESM2_15B": "ESM2 model (15B params)",
        "ESM2_150M": "ESM2 model (150M params)",
        "ESM2_35M":"ESM2 model (35M params)",
        "ESM2_8M":"ESM2 model (8M params)",
        "MIFST":"MIF-ST model",
        "MIF":"MIF model",
        "CARP_640M": "CARP model (640M params)",
        "CARP_76M": "CARP model (76M params)",
        "CARP_38M": "CARP model (38M params)",
        "CARP_600K": "CARP model (600K params)",
        "ProteinMPNN": "ProteinMPNN model",
        "ProtSSN_k10_h512": "ProtSSN (k=10, h=512)",
        "ProtSSN_k10_h768": "ProtSSN (k=10, h=768)",
        "ProtSSN_k10_h1280": "ProtSSN (k=10, h=1280)",
        "ProtSSN_k20_h512": "ProtSSN (k=20, h=512)",
        "ProtSSN_k20_h768": "ProtSSN (k=20, h=768)",
        "ProtSSN_k20_h1280": "ProtSSN (k=20, h=1280)",
        "ProtSSN_k30_h512": "ProtSSN (k=30, h=512)",
        "ProtSSN_k30_h768": "ProtSSN (k=30, h=768)",
        "ProtSSN_k30_h1280": "ProtSSN (k=30, h=1280)",
        "ProtSSN_ensemble": "ProtSSN (ensemble of 9 models)",
        "SaProt_650M_AF2": "SaProt (650M)",
        "SaProt_35M_AF2": "SaProt (35M)",
        "PoET": "PoET (200M)",
        "MULAN_small": "MULAN",
        "ProSST-20": "ProSST (K=20)",
        "ProSST-128": "ProSST (K=128)",
        "ProSST-512": "ProSST (K=512)",
        "ProSST-1024": "ProSST (K=1024)",
        "ProSST-2048": "ProSST (K=2048)",
        "ProSST-4096": "ProSST (K=4096)",
        "ESCOTT": "ESCOTT",
        "VenusREM": "VenusREM",
        "RSALOR": "RSALOR model",
        "S2F": "S2F (Sequence & Structure)",
        "S2F_MSA": "S2F with MSA retrieval",
        "S3F": "S3F (Sequence, Structure & Surface)",
        "S3F_MSA": "S3F with MSA retrieval",
        "SiteRM": "SiteRM",
        "ESM3": "ESM3 open (1.4B params)",
        "ESMC-300M": "ESM-C (300M params)",
        "ESMC-600M": "ESM-C (600M params)",
        "xTrimoPGLM-1B-MLM": "xTrimoPGLM (1B params, MLM)",
        "xTrimoPGLM-3B-MLM": "xTrimoPGLM (3B params, MLM)",
        "xTrimoPGLM-10B-MLM": "xTrimoPGLM (10B params, MLM)",
        "xTrimoPGLM-1B-CLM": "xTrimoPGLM (1B params, CLM)",
        "xTrimoPGLM-3B-CLM": "xTrimoPGLM (3B params, CLM)",
        "xTrimoPGLM-7B-CLM": "xTrimoPGLM (7B params, CLM)",
        "xTrimoPGLM-100B-int4": "xTrimoPGLM (100B params, int4 quantized)"
    },
    "model_references":{
        "Tranception_L_no_retrieval":"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>",
        "Tranception_M_no_retrieval":"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>",
        "Tranception_S_no_retrieval":"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>",
        "Tranception_S":"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>",
        "Tranception_M":"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>",
        "Tranception_L":"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>",
        "EVE_single":"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>",
        "EVE_ensemble":"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>",
        "MSA_Transformer_single":"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>",
        "MSA_Transformer_ensemble":"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>",
        "ESM1v_single":"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>",
        "ESM1v_ensemble":"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>",
        "Wavenet":"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>",
        "DeepSequence_single":"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>",
        "DeepSequence_ensemble":"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>",
        "Site_Independent":"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>",
        "EVmutation":"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>",
        "RITA_s":"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>",
        "RITA_m":"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>",
        "RITA_l":"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>",
        "RITA_xl":"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>",
        "RITA_ensemble":"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>",
        "Progen2_small":"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>",
        "Progen2_medium":"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>",
        "Progen2_base":"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>",
        "Progen2_large":"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>",
        "Progen2_xlarge":"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>",
        "Progen2_ensemble":"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>",
        "Unirep":"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>",
        "Unirep_evotune":"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>",
        "GEMME":"<a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a>",
        "ProtGPT2":"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>",
        "VESPA":"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>",
        "VESPAl":"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>",
        "VespaG":"<a href='https://doi.org/10.1101/2024.04.24.590982'>Marquet, C., Schlensok, J., Abakarova, M., Rost, B., & Laine, E. (2024). Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction. bioRxiv.</a>",
        "ESM1b":"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a>",
        "TranceptEVE_S":"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>",
        "TranceptEVE_M":"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>",
        "TranceptEVE_L":"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>",
        "HMM":"<a href='http://hmmer.org/'>HMMER: biosequence analysis using profile hidden Markov models</a>",
        "Provean": "<a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'>Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.</a>",
        "ESM-IF1": "<a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.</a>",
        "ESM2_650M":"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>",
        "ESM2_3B":"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>",
        "ESM2_15B":"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>",
        "ESM2_150M":"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>",
        "ESM2_35M":"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>",
        "ESM2_8M":"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>",
        "MIFST":"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>",
        "MIF":"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>",
        "CARP_640M":"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>",
        "CARP_76M":"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>",
        "CARP_38M":"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>",
        "CARP_600K":"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>",
        "ProteinMPNN":"<a href='https://www.science.org/doi/10.1126/science.add2187'>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.</a>",
        "ProtSSN_k10_h512": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k10_h768": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k10_h1280": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k20_h512": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k20_h768": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k20_h1280": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k30_h512": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k30_h768": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_k30_h1280": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "ProtSSN_ensemble": "<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>",
        "SaProt_650M_AF2": "<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>",
        "SaProt_35M_AF2": "<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>",
        "PoET": "<a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'>Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.</a>",
        "MULAN_small": "<a href='https://www.biorxiv.org/content/10.1101/2024.05.30.596565v1'>Daria Frolova, Daria Marina A. Pak, Anna Litvin, Ilya Sharov, Dmitry N. Ivankov, Ivan Oseledets. (2024). MULAN: Multimodal Protein Language Model for Sequence and Structure Encoding.</a>",
        "ProSST-20": "<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>",
        "ProSST-128": "<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>",
        "ProSST-512": "<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>",
        "ProSST-1024": "<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>",
        "ProSST-2048": "<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>",
        "ProSST-4096": "<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>",
        "ESCOTT": "<a href='https://www.medrxiv.org/content/10.1101/2024.02.03.24302219v1'>Mustafa Tekpinar, Laurent David, Thomas Henry, Alessandra Carbone. (2024). PRESCOTT: a population aware, epistatic and structural model accurately predicts missense effect. medRxiv.</a>",
        "VenusREM": "<a href='https://arxiv.org/abs/2410.21127'>Yang Tan, Ruilin Wang, Banghao Wu, Liang Hong, Bingxin Zhou. (2024). Retrieval-Enhanced Mutation Mastery: Augmenting Zero-Shot Prediction of Protein Language Model. ArXiv, abs/2410.21127.</a>",
        "RSALOR": "<a href='https://www.biorxiv.org/content/10.1101/2025.02.03.636212v1'>Matsvei Tsishyn, Pauline Hermans, Fabrizio Pucci, Marianne Rooman. (2025). Residue conservation and solvent accessibility are (almost) all you need for predicting mutational effects in proteins. bioRxiv.</a>",
        "S2F": "<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>",
        "S2F_MSA": "<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>",
        "S3F": "<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>",
        "S3F_MSA": "<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>",
        "SiteRM": "<a href='https://papers.nips.cc/paper_files/paper/2024/hash/eb2f4fb51ac3b8dc4aac9cf71b0e7799-Abstract-Conference.html'>Sebastian Prillo, Wilson Wu, Yun Song. (2024). Ultrafast classical phylogenetic method beats large protein language models on variant effect prediction. NeurIPS.</a>",
        "ESM3": "<a href='https://www.science.org/doi/10.1126/science.ads0018'>Hayes, T., Rao, R., Akin, H., Sofroniew, N.J., Oktay, D., Lin, Z., Verkuil, R., Tran, V.Q., Deaton, J., Wiggert, M., Badkundri, R., Shafkat, I., Gong, J., Derry, A., Molina, R.S., Thomas, N., Khan, Y.A., Mishra, C., Kim, C., Bartie, L.J., Nemeth, M., Hsu, P.D., Sercu, T., Candido, S., & Rives, A. (2025). Simulating 500 million years of evolution with a language model. Science.</a>",
        "ESMC-300M": "<a href='https://evolutionaryscale.ai/blog/esm-cambrian'>ESM Team. ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning. EvolutionaryScale Website, December 4, 2024</a>",
        "ESMC-600M": "<a href='https://evolutionaryscale.ai/blog/esm-cambrian'>ESM Team. ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning. EvolutionaryScale Website, December 4, 2024</a>",
        "xTrimoPGLM-1B-MLM": "<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>",
        "xTrimoPGLM-3B-MLM": "<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>",
        "xTrimoPGLM-10B-MLM": "<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>",
        "xTrimoPGLM-1B-CLM": "<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>",
        "xTrimoPGLM-3B-CLM": "<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>",
        "xTrimoPGLM-7B-CLM": "<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>",
        "xTrimoPGLM-100B-int4": "<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
    },
    "clean_names":{
        "Tranception_L_no_retrieval":"Tranception L no retrieval",
        "Tranception_M_no_retrieval": "Tranception M no retrieval",
        "Tranception_S_no_retrieval": "Tranception S no retrieval",
        "Tranception_S":"Tranception S",
        "Tranception_M":"Tranception M",
        "Tranception_L":"Tranception L",
        "EVE_single":"EVE (single)",
        "EVE_ensemble":"EVE (ensemble)",
        "MSA_Transformer_single":"MSA Transformer (single)",
        "MSA_Transformer_ensemble":"MSA Transformer (ensemble)",
        "ESM1v_single":"ESM-1v (single)",
        "ESM1v_ensemble":"ESM-1v (ensemble)",
        "Wavenet":"Wavenet",
        "DeepSequence_single":"DeepSequence (single)",
        "DeepSequence_ensemble":"DeepSequence (ensemble)",
        "Site_Independent":"Site-Independent",
        "EVmutation":"EVmutation",
        "RITA_s":"RITA S",
        "RITA_m":"RITA M",
        "RITA_l":"RITA L",
        "RITA_xl":"RITA XL",
        "RITA_ensemble":"RITA (ensemble)",
        "Progen2_small":"Progen2 S",
        "Progen2_medium":"Progen2 M",
        "Progen2_base":"Progen2 Base",
        "Progen2_large":"Progen2 L",
        "Progen2_xlarge":"Progen2 XL",
        "Progen2_ensemble":"Progen2 (ensemble)",
        "Unirep":"Unirep",
        "Unirep_evotune":"Unirep evotuned",
        "GEMME":"GEMME",
        "ProtGPT2":"ProtGPT2",
        "VESPA":"VESPA",
        "VESPAl":"VESPAl",
        "VespaG":"VespaG",
        "ESM1b": "ESM-1b",
        "TranceptEVE_S": "TranceptEVE S",
        "TranceptEVE_M":"TranceptEVE M",
        "TranceptEVE_L":"TranceptEVE L",
        "HMM":"Hidden Markov Model",
        "Provean": "Provean",
        "ESM-IF1": "ESM-IF1",
        "ESM2_650M": "ESM2 (650M)",
        "ESM2_3B": "ESM2 (3B)",
        "ESM2_15B": "ESM2 (15B)",
        "ESM2_150M": "ESM2 (150M)",
        "ESM2_35M": "ESM2 (35M)",
        "ESM2_8M": "ESM2 (8M)",
        "MIFST":"MIF-ST",
        "MIF":"MIF",
        "CARP_640M":"CARP (640M)",
        "CARP_76M":"CARP (76M)",
        "CARP_38M":"CARP (38M)",
        "CARP_600K":"CARP (600K)",
        "ProteinMPNN":"ProteinMPNN",
        "DMS_id":"DMS ID",
        "number_mutants":"Number of Mutants",
        "UniProt_ID":"UniProt ID",
        "Neff_L_category": "Neff/L Category",
        "Model_rank":"Rank",
        "Model_name":"Model name",
        "ProtSSN_k10_h512": "ProtSSN (k=10, h=512)",
        "ProtSSN_k10_h768": "ProtSSN (k=10, h=768)",
        "ProtSSN_k10_h1280": "ProtSSN (k=10, h=1280)",
        "ProtSSN_k20_h512": "ProtSSN (k=20, h=512)",
        "ProtSSN_k20_h768": "ProtSSN (k=20, h=768)",
        "ProtSSN_k20_h1280": "ProtSSN (k=20, h=1280)",
        "ProtSSN_k30_h512": "ProtSSN (k=30, h=512)",
        "ProtSSN_k30_h768": "ProtSSN (k=30, h=768)",
        "ProtSSN_k30_h1280": "ProtSSN (k=30, h=1280)",
        "ProtSSN_ensemble": "ProtSSN (ensemble)",
        "SaProt_650M_AF2": "SaProt (650M)",
        "SaProt_35M_AF2": "SaProt (35M)",
        "PoET": "PoET (200M)",
        "MULAN_small": "MULAN",
        "ProSST-20": "ProSST (K=20)",
        "ProSST-128": "ProSST (K=128)",
        "ProSST-512": "ProSST (K=512)",
        "ProSST-1024": "ProSST (K=1024)",
        "ProSST-2048": "ProSST (K=2048)",
        "ProSST-4096": "ProSST (K=4096)",
        "ESCOTT": "ESCOTT",
        "VenusREM": "VenusREM",
        "RSALOR": "RSALOR",
        "S2F": "S2F",
        "S2F_MSA": "S2F-MSA",
        "S3F": "S3F",
        "S3F_MSA": "S3F-MSA",
        "SiteRM": "SiteRM",
        "ESM3": "ESM3 open (1.4B)",
        "ESMC-300M": "ESM-C (300M)",
        "ESMC-600M": "ESM-C (600M)",
        "xTrimoPGLM-1B-MLM": "xTrimoPGLM-1B-MLM",
        "xTrimoPGLM-3B-MLM": "xTrimoPGLM-3B-MLM",
        "xTrimoPGLM-10B-MLM": "xTrimoPGLM-10B-MLM",
        "xTrimoPGLM-1B-CLM": "xTrimoPGLM-1B-CLM",
        "xTrimoPGLM-3B-CLM": "xTrimoPGLM-3B-CLM",
        "xTrimoPGLM-7B-CLM": "xTrimoPGLM-7B-CLM",
        "xTrimoPGLM-100B-int4": "xTrimoPGLM-100B-int4"
    },
    "supervised_model_details": {
        "ProteinNPT":"ProteinNPT Model",
        "MSA Transformer Embeddings": "MSA Transformer Embeddings",
        "ESM-1v Embeddings": "ESM-1v Embeddings",
        "Tranception Embeddings": "Tranception Embeddings",
        "TranceptEVE + One-Hot Encodings": "TranceptEVE + One-Hot Encodings",
        "ESM-1v + One-Hot Encodings": "ESM-1v + One-Hot Encodings",
        "Tranception + One-Hot Encodings": "Tranception + One-Hot Encodings",
        "DeepSequence + One-Hot Encodings": "DeepSequence + One-Hot Encodings",
        "MSA_Transformer + One-Hot Encodings": "MSA Transformer + One-Hot Encodings",
        "One-Hot Encodings": "One-Hot Encodings",
        "Kermut": "Kermut GP"
    },
    "supervised_model_references":{
        "ProteinNPT": "<a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "MSA Transformer Embeddings":"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "ESM-1v Embeddings":"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "Tranception Embeddings":"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "TranceptEVE + One-Hot Encodings":"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "Tranception + One-Hot Encodings":"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "ESM-1v + One-Hot Encodings":"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "DeepSequence + One-Hot Encodings":"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",
        "MSA_Transformer + One-Hot Encodings":"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "One-Hot Encodings":"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",
        "Kermut":"<a href='https://openreview.net/forum?id=jM9atrvUii'>Groth, P. M., Kerrn, M. H., Olsen, L., Salomon, J., & Boomsma, W. (2024). Kermut: Composite kernel regression for protein variant effects. Thirty-Eighth Conference on Neural Information Processing Systems</a>",
        "Embeddings_MSA_Transformer_indels":"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "Embeddings_ESM1v_indels":"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",
        "Embeddings_Tranception_indels":"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>"
    },
    "supervised_clean_names":{
        "ProteinNPT":"ProteinNPT",
        "Embeddings - Augmented - MSA Transformer":"MSA Transformer Embeddings",
        "Embeddings - Augmented - ESM1v":"ESM-1v Embeddings",
        "Embeddings - Augmented - Tranception":"Tranception Embeddings",
        "OHE - Augmented - TranceptEVE":"TranceptEVE + One-Hot Encodings",
        "OHE - Augmented - Tranception":"Tranception + One-Hot Encodings",
        "OHE - Augmented - MSA Transformer":"MSA_Transformer + One-Hot Encodings",
        "OHE - Augmented - DeepSequence":"DeepSequence + One-Hot Encodings",
        "OHE - Augmented - ESM1v":"ESM-1v + One-Hot Encodings",
        "OHE - Not augmented":"One-Hot Encodings",
        "Kermut":"Kermut",
        "Embeddings_ESM1v_indels": "ESM-1v Embeddings",
        "Embeddings_MSA_Transformer_indels": "MSA Transformer Embeddings",
        "Embeddings_Tranception_indels": "Tranception Embeddings"
    },
    "supervised_model_types":{
        "ProteinNPT":"Sequence embedding",
        "MSA Transformer Embeddings":"Sequence embedding",
        "Tranception Embeddings":"Sequence embedding",
        "ESM-1v Embeddings":"Sequence embedding",
        "TranceptEVE + One-Hot Encodings":"One-hot Encoding",
        "Tranception + One-Hot Encodings":"One-hot Encoding",
        "MSA_Transformer + One-Hot Encodings":"One-hot Encoding",
        "DeepSequence + One-Hot Encodings":"One-hot Encoding",
        "ESM-1v + One-Hot Encodings":"One-hot Encoding",
        "One-Hot Encodings":"One-hot Encoding",
        "Kermut": "Structure & sequence embedding",
        "Embeddings_ESM1v_indels": "Sequence embedding",
        "Embeddings_MSA_Transformer_indels": "Sequence embedding",
        "Embeddings_Tranception_indels": "Sequence embedding"
    }
}