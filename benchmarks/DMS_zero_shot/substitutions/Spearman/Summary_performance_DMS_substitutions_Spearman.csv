Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,AIDO Protein-RAG (16B),Structure & MSA,0.518,0.0,0.517,0.426,0.522,0.491,0.635,0.498,0.534,0.585,0.531,0.587,0.558,0.522,0.527,0.414,0.419,0.394,0.414,AIDO Protein-RAG (16B),"<a href='https://www.biorxiv.org/content/10.1101/2024.11.29.625425v1'>Sun, N., Zou, S., Tao, T., Mahbub, S., Li, D., Zhuang, Y., Wang, H., Cheng, X., Song, L., & Xing, E.P. (2024). Mixture of Experts Enable Efficient and Effective Protein Understanding and Design. bioRxiv.</a>"
2,VenusREM,Structure & MSA,0.518,0.005,0.495,0.454,0.533,0.459,0.65,0.495,0.524,0.577,0.529,0.582,0.549,0.492,0.534,0.397,0.355,0.322,0.368,VenusREM,"<a href='https://arxiv.org/abs/2410.21127'>Yang Tan, Ruilin Wang, Banghao Wu, Liang Hong, Bingxin Zhou. (2024). Retrieval-Enhanced Mutation Mastery: Augmenting Zero-Shot Prediction of Protein Language Model. ArXiv, abs/2410.21127.</a>"
3,ProSST (K=2048),Single sequence & Structure,0.507,0.006,0.476,0.445,0.53,0.431,0.653,0.465,0.507,0.58,0.516,0.573,0.549,0.454,0.521,0.394,0.317,0.277,0.332,ProSST (K=2048),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
4,ProSST (K=4096),Single sequence & Structure,0.498,0.009,0.444,0.472,0.507,0.416,0.652,0.472,0.481,0.583,0.497,0.574,0.547,0.44,0.505,0.426,0.388,0.342,0.408,ProSST (K=4096),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
5,S3F-MSA,Structure & MSA,0.496,0.007,0.502,0.44,0.479,0.477,0.581,0.469,0.509,0.547,0.502,0.558,0.521,0.502,0.499,0.333,0.378,0.346,0.383,S3F with MSA retrieval,"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
6,S2F-MSA,Structure & MSA,0.488,0.007,0.498,0.432,0.472,0.472,0.567,0.463,0.502,0.536,0.495,0.546,0.513,0.493,0.491,0.303,0.346,0.318,0.362,S2F with MSA retrieval,"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
7,ProSST (K=1024),Single sequence & Structure,0.485,0.009,0.433,0.436,0.499,0.414,0.642,0.457,0.466,0.585,0.483,0.568,0.539,0.436,0.492,0.434,0.373,0.341,0.403,ProSST (K=1024),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
8,ESCOTT,Structure & MSA,0.476,0.005,0.499,0.389,0.468,0.466,0.557,0.462,0.496,0.524,0.486,0.537,0.5,0.502,0.476,0.314,0.377,0.385,0.434,ESCOTT,"<a href='https://www.medrxiv.org/content/10.1101/2024.02.03.24302219v1'>Mustafa Tekpinar, Laurent David, Thomas Henry, Alessandra Carbone. (2024). PRESCOTT: a population aware, epistatic and structural model accurately predicts missense effect. medRxiv.</a>"
9,ProSST (K=512),Single sequence & Structure,0.471,0.009,0.423,0.431,0.479,0.394,0.629,0.437,0.451,0.572,0.472,0.55,0.529,0.408,0.476,0.41,0.354,0.314,0.383,ProSST (K=512),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
10,S3F,Single sequence & Structure,0.47,0.006,0.468,0.404,0.472,0.413,0.594,0.42,0.471,0.551,0.486,0.536,0.512,0.415,0.485,0.328,0.331,0.293,0.337,"S3F (Sequence, Structure & Surface)","<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
11,PoET (200M),MSA,0.47,0.007,0.494,0.396,0.466,0.475,0.519,0.478,0.478,0.51,0.482,0.541,0.464,0.491,0.466,0.299,0.419,0.395,0.418,PoET (200M),"<a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'>Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.</a>"
12,ProSST (K=128),Single sequence & Structure,0.469,0.009,0.417,0.44,0.473,0.387,0.628,0.441,0.447,0.566,0.466,0.545,0.523,0.415,0.472,0.418,0.376,0.332,0.394,ProSST (K=128),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
13,ESM3 open (1.4B),"Single sequence, Structure & Function annotations",0.466,0.006,0.43,0.4,0.47,0.389,0.641,0.397,0.466,0.575,0.48,0.548,0.53,0.406,0.487,0.337,0.307,0.285,0.363,ESM3 open (1.4B params),"<a href='https://www.science.org/doi/10.1126/science.ads0018'>Hayes, T., Rao, R., Akin, H., Sofroniew, N.J., Oktay, D., Lin, Z., Verkuil, R., Tran, V.Q., Deaton, J., Wiggert, M., Badkundri, R., Shafkat, I., Gong, J., Derry, A., Molina, R.S., Thomas, N., Khan, Y.A., Mishra, C., Kim, C., Bartie, L.J., Nemeth, M., Hsu, P.D., Sercu, T., Candido, S., & Rives, A. (2025). Simulating 500 million years of evolution with a language model. Science.</a>"
14,RSALOR,Structure & MSA,0.465,0.007,0.479,0.416,0.427,0.426,0.575,0.467,0.468,0.529,0.473,0.53,0.496,0.477,0.474,0.292,0.369,0.392,0.453,RSALOR model,"<a href='https://www.biorxiv.org/content/10.1101/2025.02.03.636212v1'>Matsvei Tsishyn, Pauline Hermans, Fabrizio Pucci, Marianne Rooman. (2025). Residue conservation and solvent accessibility are (almost) all you need for predicting mutational effects in proteins. bioRxiv.</a>"
15,VespaG,Single sequence,0.458,0.006,0.493,0.37,0.456,0.437,0.533,0.427,0.472,0.511,0.474,0.532,0.484,0.421,0.456,0.247,0.346,0.323,0.367,VespaG model,"<a href='https://doi.org/10.1101/2024.04.24.590982'>Marquet, C., Schlensok, J., Abakarova, M., Rost, B., & Laine, E. (2024). Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction. bioRxiv.</a>"
16,SaProt (650M),Single sequence & Structure,0.457,0.008,0.458,0.378,0.488,0.366,0.592,0.394,0.446,0.546,0.478,0.529,0.514,0.32,0.458,0.313,0.275,0.268,0.336,SaProt (650M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
17,TranceptEVE L,MSA,0.456,0.007,0.487,0.376,0.457,0.459,0.5,0.436,0.472,0.49,0.473,0.513,0.455,0.461,0.446,0.278,0.349,0.331,0.382,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
18,S2F,Single sequence & Structure,0.456,0.006,0.459,0.391,0.456,0.404,0.568,0.404,0.457,0.531,0.473,0.514,0.496,0.396,0.47,0.281,0.289,0.251,0.297,S2F (Sequence & Structure),"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
19,GEMME,MSA,0.455,0.009,0.482,0.383,0.438,0.452,0.519,0.446,0.474,0.493,0.469,0.516,0.467,0.472,0.448,0.276,0.33,0.341,0.414,GEMME model,"<a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a>"
20,TranceptEVE M,MSA,0.455,0.008,0.479,0.386,0.452,0.454,0.502,0.424,0.472,0.487,0.474,0.513,0.45,0.446,0.442,0.282,0.312,0.318,0.375,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
21,TranceptEVE S,MSA,0.452,0.008,0.475,0.396,0.443,0.449,0.497,0.434,0.465,0.482,0.47,0.504,0.453,0.438,0.437,0.276,0.311,0.315,0.372,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
22,ProtSSN (ensemble),Single sequence & Structure,0.449,0.006,0.466,0.366,0.449,0.396,0.568,0.409,0.454,0.524,0.47,0.528,0.492,0.37,0.459,0.291,0.292,0.244,0.298,ProtSSN (ensemble of 9 models),"<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
23,"ProtSSN (k=20, h=1280)",Single sequence & Structure,0.442,0.006,0.458,0.366,0.435,0.385,0.566,0.402,0.445,0.52,0.464,0.525,0.483,0.363,0.45,0.279,0.288,0.24,0.297,"ProtSSN (k=20, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
24,"ProtSSN (k=20, h=512)",Single sequence & Structure,0.44,0.005,0.457,0.353,0.437,0.391,0.563,0.4,0.45,0.515,0.458,0.524,0.488,0.373,0.45,0.29,0.306,0.262,0.31,"ProtSSN (k=20, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
25,EVE (ensemble),MSA,0.439,0.009,0.464,0.386,0.408,0.447,0.491,0.42,0.457,0.477,0.454,0.495,0.457,0.434,0.428,0.273,0.31,0.31,0.355,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
26,"ProtSSN (k=20, h=768)",Single sequence & Structure,0.439,0.006,0.459,0.347,0.439,0.387,0.563,0.394,0.446,0.52,0.462,0.525,0.485,0.357,0.448,0.294,0.282,0.242,0.31,"ProtSSN (k=20, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
27,ProSST (K=20),Single sequence & Structure,0.438,0.009,0.392,0.41,0.447,0.354,0.589,0.408,0.42,0.525,0.443,0.506,0.477,0.387,0.44,0.375,0.311,0.278,0.348,ProSST (K=20),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
28,"ProtSSN (k=30, h=512)",Single sequence & Structure,0.438,0.006,0.455,0.35,0.443,0.384,0.557,0.4,0.444,0.511,0.461,0.519,0.479,0.356,0.445,0.275,0.278,0.238,0.291,"ProtSSN (k=30, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
29,"ProtSSN (k=30, h=768)",Single sequence & Structure,0.438,0.006,0.457,0.351,0.435,0.384,0.561,0.393,0.446,0.515,0.458,0.525,0.484,0.356,0.446,0.284,0.281,0.236,0.297,"ProtSSN (k=30, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
30,"ProtSSN (k=30, h=1280)",Single sequence & Structure,0.437,0.006,0.454,0.357,0.431,0.384,0.561,0.395,0.44,0.52,0.46,0.522,0.483,0.35,0.444,0.299,0.282,0.23,0.3,"ProtSSN (k=30, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
31,VESPA,Single sequence,0.435,0.005,0.468,0.366,0.404,0.439,0.5,0.42,0.455,0.484,0.44,0.503,0.474,0.443,0.435,0.181,0.348,0.308,0.328,VESPA model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
32,Tranception L,MSA,0.434,0.007,0.465,0.349,0.45,0.435,0.471,0.421,0.443,0.471,0.455,0.497,0.414,0.438,0.422,0.254,0.346,0.325,0.387,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
33,"ProtSSN (k=10, h=1280)",Single sequence & Structure,0.433,0.006,0.446,0.352,0.436,0.381,0.548,0.4,0.437,0.503,0.455,0.512,0.47,0.358,0.441,0.245,0.257,0.217,0.282,"ProtSSN (k=10, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
34,EVE (single),MSA,0.432,0.008,0.458,0.372,0.404,0.441,0.487,0.411,0.452,0.473,0.446,0.491,0.454,0.43,0.422,0.271,0.305,0.308,0.359,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
35,MSA Transformer (ensemble),MSA,0.432,0.01,0.473,0.329,0.446,0.419,0.492,0.375,0.456,0.479,0.439,0.516,0.446,0.421,0.427,0.221,0.365,0.369,0.397,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
36,"ProtSSN (k=10, h=512)",Single sequence & Structure,0.43,0.006,0.451,0.344,0.42,0.376,0.557,0.398,0.434,0.51,0.454,0.521,0.478,0.338,0.437,0.281,0.266,0.217,0.287,"ProtSSN (k=10, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
37,Tranception M,MSA,0.428,0.008,0.448,0.361,0.441,0.424,0.465,0.406,0.437,0.456,0.452,0.489,0.397,0.414,0.412,0.251,0.255,0.273,0.353,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
38,SiteRM,MSA,0.424,0.009,0.453,0.388,0.39,0.437,0.449,0.446,0.438,0.429,0.437,0.471,0.399,0.462,0.401,0.223,0.313,0.309,0.363,SiteRM,"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/eb2f4fb51ac3b8dc4aac9cf71b0e7799-Abstract-Conference.html'>Sebastian Prillo, Wilson Wu, Yun Song. (2024). Ultrafast classical phylogenetic method beats large protein language models on variant effect prediction. NeurIPS.</a>"
39,ESM-IF1,Structure,0.422,0.011,0.368,0.389,0.407,0.324,0.624,0.292,0.427,0.549,0.417,0.502,0.498,0.389,0.44,0.35,0.301,0.294,0.358,ESM-IF1 model,"<a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.</a>"
40,"ProtSSN (k=10, h=768)",Single sequence & Structure,0.422,0.006,0.441,0.337,0.417,0.37,0.547,0.382,0.431,0.497,0.446,0.503,0.467,0.346,0.43,0.269,0.27,0.215,0.278,"ProtSSN (k=10, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
41,DeepSequence (ensemble),MSA,0.419,0.011,0.455,0.363,0.39,0.411,0.476,0.37,0.431,0.474,0.443,0.486,0.439,0.358,0.405,0.263,0.314,0.322,0.378,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
42,Tranception S,MSA,0.418,0.008,0.436,0.372,0.42,0.412,0.452,0.418,0.421,0.442,0.44,0.476,0.391,0.401,0.398,0.244,0.264,0.279,0.352,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
43,MSA Transformer (single),MSA,0.417,0.01,0.455,0.312,0.429,0.412,0.475,0.364,0.441,0.465,0.43,0.501,0.426,0.406,0.412,0.208,0.363,0.361,0.39,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
44,ESM2 (650M),Single sequence,0.414,0.012,0.425,0.337,0.415,0.368,0.523,0.338,0.409,0.513,0.457,0.486,0.458,0.261,0.422,0.248,0.205,0.163,0.218,ESM2 model (650M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
45,DeepSequence (single),MSA,0.407,0.012,0.447,0.349,0.371,0.396,0.473,0.372,0.419,0.464,0.437,0.479,0.43,0.338,0.392,0.253,0.285,0.306,0.37,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
46,ESM-1v (ensemble),Single sequence,0.407,0.013,0.42,0.32,0.429,0.386,0.477,0.316,0.409,0.495,0.458,0.464,0.413,0.294,0.403,0.215,0.173,0.153,0.218,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
47,SaProt (35M),Single sequence & Structure,0.407,0.009,0.372,0.357,0.439,0.289,0.575,0.325,0.385,0.514,0.443,0.492,0.432,0.242,0.41,0.356,0.187,0.187,0.245,SaProt (35M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
48,ESM-C (300M),Single sequence,0.406,0.013,0.423,0.315,0.408,0.36,0.526,0.337,0.399,0.52,0.468,0.481,0.441,0.242,0.415,0.258,0.191,0.155,0.217,ESM-C (300M params),"<a href='https://evolutionaryscale.ai/blog/esm-cambrian'>ESM Team. ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning. EvolutionaryScale Website, December 4, 2024</a>"
49,ESM2 (3B),Single sequence,0.406,0.011,0.417,0.321,0.403,0.378,0.509,0.336,0.423,0.485,0.442,0.477,0.458,0.294,0.414,0.215,0.194,0.166,0.217,ESM2 model (3B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
50,ESM-C (600M),Single sequence,0.405,0.013,0.423,0.294,0.42,0.362,0.528,0.331,0.407,0.515,0.462,0.481,0.459,0.241,0.416,0.246,0.197,0.15,0.202,ESM-C (600M params),"<a href='https://evolutionaryscale.ai/blog/esm-cambrian'>ESM Team. ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning. EvolutionaryScale Website, December 4, 2024</a>"
51,ESM2 (15B),Single sequence,0.4,0.01,0.405,0.317,0.405,0.387,0.488,0.341,0.422,0.467,0.432,0.467,0.436,0.333,0.405,0.202,0.219,0.173,0.234,ESM2 model (15B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
52,MIF-ST,Single sequence & Structure,0.4,0.009,0.39,0.321,0.438,0.366,0.485,0.371,0.404,0.453,0.399,0.413,0.457,0.405,0.431,0.263,0.327,0.298,0.298,MIF-ST model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
53,xTrimoPGLM-3B-MLM,Single sequence,0.399,0.012,0.408,0.308,0.396,0.4,0.482,0.332,0.423,0.474,0.426,0.449,0.442,0.375,0.399,0.217,0.183,0.136,0.189,"xTrimoPGLM (3B params, MLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
54,xTrimoPGLM-10B-MLM,Single sequence,0.396,0.011,0.405,0.301,0.398,0.394,0.481,0.327,0.414,0.48,0.421,0.445,0.444,0.368,0.395,0.202,0.176,0.137,0.19,"xTrimoPGLM (10B params, MLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
55,EVmutation,MSA,0.395,0.008,0.44,0.317,0.378,0.41,0.43,0.399,0.426,0.404,0.41,0.448,0.414,0.393,0.376,0.273,0.327,0.318,0.394,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
56,ESM-1b,Single sequence,0.394,0.011,0.428,0.287,0.406,0.349,0.5,0.365,0.396,0.48,0.435,0.491,0.438,0.26,0.383,0.226,0.172,0.15,0.27,ESM-1b (w/ Brandes et al. extensions),"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a>"
57,VESPAl,Single sequence,0.393,0.007,0.429,0.347,0.326,0.403,0.461,0.377,0.411,0.449,0.394,0.472,0.434,0.403,0.386,0.141,0.321,0.282,0.32,VESPAl model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
58,Progen3 3B,Single sequence,0.392,0.008,0.41,0.287,0.428,0.4,0.438,0.325,0.407,0.453,0.41,0.433,0.392,0.414,0.388,0.179,0.226,0.204,0.256,Progen3 model (3B params),"<a href='https://doi.org/10.1101/2025.04.15.649055'> Bhatnagar, A., Jain, S., Beazer, J., Curran, S.C., Hoffnagle, A.M., Ching, K., Martyn, M., Nayfach, S., Ruffolo, J.A., & Madani, A. (2025). Scaling unlocks broader generation and deeper functional understanding of proteins. bioRxiv, 2025.04.15.649055. </a>"
59,Progen2 XL,Single sequence,0.391,0.008,0.402,0.302,0.418,0.387,0.445,0.323,0.412,0.442,0.386,0.458,0.418,0.402,0.384,0.182,0.271,0.234,0.28,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
60,Progen3 1B,Single sequence,0.389,0.009,0.409,0.303,0.428,0.396,0.408,0.321,0.396,0.438,0.408,0.418,0.368,0.401,0.382,0.164,0.217,0.192,0.243,Progen3 model (1B params),"<a href='https://doi.org/10.1101/2025.04.15.649055'> Bhatnagar, A., Jain, S., Beazer, J., Curran, S.C., Hoffnagle, A.M., Ching, K., Martyn, M., Nayfach, S., Ruffolo, J.A., & Madani, A. (2025). Scaling unlocks broader generation and deeper functional understanding of proteins. bioRxiv, 2025.04.15.649055. </a>"
61,ESM2 (150M),Single sequence,0.387,0.013,0.391,0.326,0.402,0.305,0.51,0.319,0.359,0.494,0.45,0.475,0.398,0.157,0.387,0.246,0.146,0.16,0.231,ESM2 model (150M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
62,MIF,Structure,0.383,0.01,0.327,0.336,0.43,0.3,0.522,0.344,0.374,0.448,0.398,0.387,0.417,0.365,0.412,0.272,0.272,0.245,0.24,MIF model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
63,Progen2 L,Single sequence,0.38,0.009,0.406,0.293,0.427,0.379,0.396,0.339,0.39,0.414,0.411,0.432,0.364,0.323,0.371,0.142,0.225,0.207,0.258,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
64,Progen2 M,Single sequence,0.38,0.009,0.393,0.295,0.433,0.383,0.396,0.307,0.391,0.422,0.413,0.417,0.356,0.335,0.373,0.131,0.152,0.137,0.177,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
65,xTrimoPGLM-1B-MLM,Single sequence,0.379,0.012,0.39,0.298,0.406,0.357,0.445,0.324,0.372,0.463,0.426,0.441,0.402,0.236,0.374,0.187,0.189,0.168,0.227,"xTrimoPGLM (1B params, MLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
66,Progen2 Base,Single sequence,0.378,0.01,0.396,0.294,0.437,0.381,0.383,0.327,0.379,0.421,0.422,0.422,0.333,0.316,0.371,0.131,0.149,0.156,0.208,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
67,Tranception L no retrieval,Single sequence,0.374,0.009,0.401,0.288,0.413,0.387,0.381,0.332,0.379,0.414,0.391,0.394,0.354,0.404,0.363,0.171,0.294,0.26,0.334,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
68,ESM-1v (single),Single sequence,0.374,0.014,0.396,0.268,0.405,0.361,0.437,0.289,0.373,0.47,0.427,0.433,0.385,0.258,0.375,0.188,0.177,0.146,0.209,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
69,Wavenet,MSA,0.373,0.012,0.379,0.325,0.35,0.364,0.449,0.277,0.39,0.455,0.393,0.425,0.406,0.34,0.357,0.203,0.235,0.219,0.293,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
70,RITA XL,Single sequence,0.373,0.01,0.366,0.302,0.414,0.384,0.398,0.3,0.39,0.412,0.396,0.399,0.346,0.394,0.357,0.141,0.148,0.16,0.233,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
71,CARP (640M),Single sequence,0.369,0.012,0.395,0.273,0.397,0.366,0.412,0.331,0.374,0.424,0.417,0.395,0.381,0.284,0.391,0.213,0.172,0.159,0.162,CARP model (640M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
72,RITA L,Single sequence,0.366,0.009,0.359,0.29,0.42,0.378,0.383,0.301,0.379,0.402,0.396,0.399,0.315,0.384,0.349,0.139,0.143,0.149,0.21,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
73,xTrimoPGLM-100B-int4,Single sequence,0.364,0.01,0.378,0.259,0.38,0.355,0.45,0.314,0.391,0.43,0.363,0.45,0.422,0.36,0.355,0.177,0.276,0.232,0.313,"xTrimoPGLM (100B params, int4 quantized)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
74,Progen3 762m,Single sequence,0.364,0.01,0.376,0.296,0.386,0.373,0.386,0.29,0.368,0.42,0.385,0.368,0.362,0.369,0.355,0.166,0.158,0.143,0.186,Progen3 model (762m params),"<a href='https://doi.org/10.1101/2025.04.15.649055'> Bhatnagar, A., Jain, S., Beazer, J., Curran, S.C., Hoffnagle, A.M., Ching, K., Martyn, M., Nayfach, S., Ruffolo, J.A., & Madani, A. (2025). Scaling unlocks broader generation and deeper functional understanding of proteins. bioRxiv, 2025.04.15.649055. </a>"
75,Site-Independent,MSA,0.359,0.012,0.369,0.344,0.343,0.383,0.358,0.427,0.376,0.317,0.38,0.389,0.318,0.375,0.337,0.24,0.241,0.273,0.35,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
76,RITA M,Single sequence,0.35,0.01,0.352,0.273,0.405,0.374,0.348,0.286,0.359,0.39,0.379,0.367,0.302,0.375,0.338,0.115,0.141,0.162,0.222,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
77,Tranception M no retrieval,Single sequence,0.349,0.01,0.349,0.284,0.406,0.364,0.342,0.269,0.358,0.378,0.38,0.357,0.303,0.34,0.333,0.144,0.114,0.139,0.21,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
78,Unirep evotuned,MSA,0.347,0.01,0.355,0.305,0.365,0.345,0.366,0.322,0.345,0.376,0.357,0.378,0.331,0.348,0.32,0.155,0.244,0.236,0.294,Unirep model w/ evotuning,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
79,Progen3 339m,Single sequence,0.347,0.011,0.345,0.294,0.38,0.355,0.359,0.253,0.355,0.392,0.357,0.341,0.354,0.347,0.335,0.16,0.14,0.137,0.189,Progen3 model (339m params),"<a href='https://doi.org/10.1101/2025.04.15.649055'> Bhatnagar, A., Jain, S., Beazer, J., Curran, S.C., Hoffnagle, A.M., Ching, K., Martyn, M., Nayfach, S., Ruffolo, J.A., & Madani, A. (2025). Scaling unlocks broader generation and deeper functional understanding of proteins. bioRxiv, 2025.04.15.649055. </a>"
80,xTrimoPGLM-1B-CLM,Single sequence,0.343,0.009,0.347,0.296,0.4,0.336,0.333,0.293,0.337,0.37,0.382,0.354,0.301,0.27,0.319,0.18,0.126,0.12,0.166,"xTrimoPGLM (1B params, CLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
81,Progen2 S,Single sequence,0.336,0.011,0.333,0.275,0.384,0.338,0.349,0.293,0.321,0.39,0.385,0.358,0.291,0.271,0.328,0.114,0.117,0.123,0.187,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
82,CARP (76M),Single sequence,0.328,0.014,0.342,0.282,0.369,0.271,0.376,0.254,0.302,0.403,0.388,0.366,0.301,0.157,0.336,0.206,0.115,0.124,0.127,CARP model (76M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
83,ESM2 (35M),Single sequence,0.321,0.015,0.314,0.291,0.343,0.217,0.439,0.247,0.271,0.448,0.371,0.408,0.315,0.12,0.306,0.245,0.136,0.152,0.229,ESM2 model (35M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
84,Progen3 219m,Single sequence,0.318,0.011,0.31,0.293,0.347,0.335,0.305,0.246,0.332,0.332,0.319,0.288,0.335,0.326,0.304,0.148,0.139,0.145,0.194,Progen3 model (219m params),"<a href='https://doi.org/10.1101/2025.04.15.649055'> Bhatnagar, A., Jain, S., Beazer, J., Curran, S.C., Hoffnagle, A.M., Ching, K., Martyn, M., Nayfach, S., Ruffolo, J.A., & Madani, A. (2025). Scaling unlocks broader generation and deeper functional understanding of proteins. bioRxiv, 2025.04.15.649055. </a>"
85,MULAN,Single sequence & Structure,0.317,0.01,0.283,0.326,0.383,0.225,0.369,0.273,0.291,0.338,0.343,0.315,0.313,0.167,0.292,0.23,0.079,0.11,0.17,MULAN,"<a href='https://www.biorxiv.org/content/10.1101/2024.05.30.596565v1'>Daria Frolova, Daria Marina A. Pak, Anna Litvin, Ilya Sharov, Dmitry N. Ivankov, Ivan Oseledets. (2024). MULAN: Multimodal Protein Language Model for Sequence and Structure Encoding.</a>"
86,RITA S,Single sequence,0.305,0.011,0.294,0.275,0.336,0.329,0.289,0.249,0.305,0.332,0.331,0.299,0.238,0.344,0.287,0.114,0.104,0.122,0.19,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
87,Tranception S no retrieval,Single sequence,0.303,0.011,0.288,0.286,0.349,0.321,0.27,0.235,0.304,0.315,0.318,0.283,0.263,0.304,0.279,0.116,0.107,0.128,0.189,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
88,Progen3 112m,Single sequence,0.282,0.013,0.273,0.301,0.302,0.301,0.233,0.221,0.291,0.272,0.262,0.248,0.295,0.301,0.265,0.103,0.121,0.138,0.199,Progen3 model (112m params),"<a href='https://doi.org/10.1101/2025.04.15.649055'> Bhatnagar, A., Jain, S., Beazer, J., Curran, S.C., Hoffnagle, A.M., Ching, K., Martyn, M., Nayfach, S., Ruffolo, J.A., & Madani, A. (2025). Scaling unlocks broader generation and deeper functional understanding of proteins. bioRxiv, 2025.04.15.649055. </a>"
89,CARP (38M),Single sequence,0.28,0.015,0.285,0.268,0.312,0.218,0.314,0.206,0.24,0.354,0.322,0.307,0.253,0.127,0.279,0.173,0.106,0.126,0.143,CARP model (38M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
90,ProteinMPNN,Structure,0.257,0.01,0.197,0.163,0.198,0.164,0.565,0.187,0.271,0.439,0.284,0.394,0.348,0.262,0.292,0.26,0.174,0.183,0.278,ProteinMPNN model,"<a href='https://www.science.org/doi/10.1126/science.add2187'>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.</a>"
91,xTrimoPGLM-3B-CLM,Single sequence,0.233,0.012,0.214,0.249,0.27,0.236,0.196,0.201,0.215,0.243,0.283,0.219,0.134,0.19,0.205,0.1,0.009,0.028,0.05,"xTrimoPGLM (3B params, CLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
92,ESM2 (8M),Single sequence,0.226,0.015,0.201,0.26,0.266,0.14,0.262,0.2,0.177,0.262,0.241,0.248,0.201,0.09,0.203,0.14,0.11,0.126,0.195,ESM2 model (8M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
93,Unirep,Single sequence,0.19,0.016,0.182,0.202,0.216,0.139,0.21,0.183,0.158,0.209,0.215,0.233,0.152,0.066,0.176,0.073,0.109,0.136,0.191,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
94,ProtGPT2,Single sequence,0.188,0.011,0.176,0.149,0.193,0.166,0.257,0.188,0.171,0.252,0.244,0.242,0.134,0.138,0.181,0.14,0.048,0.036,0.078,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
95,CARP (600K),Single sequence,0.106,0.016,0.112,0.084,0.171,0.059,0.105,0.101,0.085,0.1,0.122,0.086,0.075,0.055,0.108,0.029,0.045,0.076,0.096,CARP model (600K params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
96,xTrimoPGLM-7B-CLM,Single sequence,0.102,0.014,0.116,0.154,0.078,0.061,0.099,0.082,0.072,0.117,0.13,0.115,0.044,0.012,0.085,0.039,0.01,0.029,0.081,"xTrimoPGLM (7B params, CLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
