Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,SaProt (650M),Hybrid - Structure & PLM,0.456,0.0,0.458,0.378,0.488,0.363,0.592,0.395,0.45,0.543,0.478,0.529,0.51,0.32,0.458,0.311,0.266,0.25,0.318,SaProt (650M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
2,TranceptEVE L,Hybrid - Alignment & PLM,0.456,0.008,0.487,0.376,0.457,0.458,0.5,0.451,0.467,0.49,0.473,0.513,0.453,0.461,0.447,0.282,0.363,0.322,0.38,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
3,GEMME,Alignment-based model,0.454,0.011,0.482,0.383,0.438,0.45,0.519,0.455,0.47,0.496,0.469,0.516,0.465,0.472,0.448,0.279,0.335,0.334,0.422,GEMME model,"<a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a>"
4,TranceptEVE M,Hybrid - Alignment & PLM,0.454,0.009,0.479,0.386,0.452,0.452,0.502,0.44,0.468,0.486,0.474,0.513,0.448,0.446,0.442,0.283,0.316,0.306,0.368,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
5,TranceptEVE S,Hybrid - Alignment & PLM,0.452,0.009,0.475,0.396,0.443,0.447,0.497,0.449,0.46,0.483,0.47,0.504,0.451,0.438,0.437,0.278,0.318,0.306,0.367,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
6,ProtSSN (ensemble),Hybrid - Structure & PLM,0.449,0.006,0.466,0.366,0.449,0.396,0.568,0.401,0.458,0.521,0.47,0.528,0.491,0.37,0.459,0.293,0.297,0.237,0.293,ProtSSN (ensemble of 9 models),"<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
7,ProtSSN (k=20 h=1280),Hybrid - Structure & PLM,0.442,0.007,0.458,0.366,0.435,0.385,0.566,0.395,0.448,0.519,0.464,0.525,0.482,0.363,0.45,0.283,0.298,0.235,0.294,"ProtSSN (k=20, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
8,ProtSSN (k=20 h=512),Hybrid - Structure & PLM,0.44,0.006,0.457,0.353,0.437,0.39,0.563,0.394,0.453,0.512,0.458,0.524,0.487,0.373,0.45,0.293,0.315,0.254,0.303,"ProtSSN (k=20, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
9,EVE (ensemble),Alignment-based model,0.439,0.01,0.464,0.386,0.408,0.446,0.491,0.425,0.453,0.48,0.454,0.495,0.455,0.434,0.429,0.277,0.326,0.304,0.352,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
10,ProtSSN (k=20 h=768),Hybrid - Structure & PLM,0.439,0.006,0.459,0.347,0.439,0.386,0.563,0.387,0.45,0.516,0.462,0.525,0.483,0.357,0.448,0.296,0.286,0.228,0.294,"ProtSSN (k=20, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
11,ProtSSN (k=30 h=512),Hybrid - Structure & PLM,0.438,0.006,0.455,0.35,0.443,0.384,0.557,0.393,0.447,0.508,0.461,0.519,0.478,0.356,0.445,0.277,0.279,0.233,0.29,"ProtSSN (k=30, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
12,ProtSSN (k=30 h=768),Hybrid - Structure & PLM,0.437,0.006,0.457,0.351,0.435,0.383,0.561,0.386,0.449,0.512,0.458,0.525,0.482,0.356,0.447,0.285,0.282,0.226,0.291,"ProtSSN (k=30, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
13,ProtSSN (k=30 h=1280),Hybrid - Structure & PLM,0.437,0.007,0.454,0.357,0.431,0.383,0.561,0.388,0.445,0.515,0.46,0.522,0.481,0.35,0.444,0.3,0.287,0.221,0.291,"ProtSSN (k=30, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
14,VESPA,Protein language model,0.435,0.007,0.468,0.366,0.404,0.438,0.5,0.427,0.455,0.483,0.44,0.503,0.472,0.443,0.435,0.183,0.359,0.298,0.323,VESPA model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
15,Tranception L,Hybrid - Alignment & PLM,0.434,0.008,0.465,0.349,0.45,0.433,0.471,0.432,0.438,0.471,0.455,0.497,0.412,0.438,0.423,0.256,0.354,0.31,0.381,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
16,ProtSSN (k=10 h=1280),Hybrid - Structure & PLM,0.433,0.007,0.446,0.352,0.436,0.38,0.548,0.391,0.44,0.502,0.455,0.512,0.469,0.358,0.441,0.246,0.258,0.209,0.277,"ProtSSN (k=10, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
17,EVE (single),Alignment-based model,0.432,0.009,0.458,0.372,0.404,0.44,0.487,0.417,0.448,0.476,0.446,0.491,0.452,0.43,0.423,0.275,0.321,0.3,0.355,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
18,MSA Transformer (ensemble),Hybrid - Alignment & PLM,0.432,0.009,0.473,0.329,0.446,0.418,0.492,0.403,0.45,0.482,0.439,0.516,0.445,0.421,0.428,0.225,0.371,0.366,0.412,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
19,ProtSSN (k=10 h=512),Hybrid - Structure & PLM,0.429,0.007,0.451,0.344,0.42,0.376,0.557,0.391,0.436,0.509,0.454,0.521,0.477,0.338,0.437,0.283,0.272,0.214,0.289,"ProtSSN (k=10, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
20,Tranception M,Hybrid - Alignment & PLM,0.427,0.009,0.448,0.361,0.441,0.419,0.465,0.417,0.432,0.454,0.452,0.489,0.39,0.414,0.411,0.248,0.241,0.257,0.339,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
21,ProtSSN (k=10 h=768),Hybrid - Structure & PLM,0.422,0.007,0.441,0.337,0.417,0.369,0.547,0.372,0.434,0.496,0.446,0.503,0.466,0.346,0.431,0.271,0.276,0.208,0.272,"ProtSSN (k=10, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
22,ESM-IF1,Inverse folding model,0.422,0.011,0.368,0.389,0.407,0.32,0.624,0.3,0.431,0.544,0.417,0.502,0.492,0.389,0.439,0.346,0.285,0.27,0.353,ESM-IF1 model,"<a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.</a>"
23,DeepSequence (ensemble),Alignment-based model,0.419,0.012,0.455,0.363,0.39,0.411,0.476,0.383,0.428,0.471,0.443,0.486,0.438,0.358,0.405,0.268,0.332,0.316,0.376,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
24,Tranception S,Hybrid - Alignment & PLM,0.418,0.009,0.436,0.372,0.42,0.409,0.452,0.428,0.415,0.443,0.44,0.476,0.385,0.401,0.398,0.242,0.255,0.267,0.342,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
25,MSA Transformer (single),Hybrid - Alignment & PLM,0.416,0.009,0.455,0.312,0.429,0.411,0.475,0.388,0.436,0.465,0.43,0.501,0.425,0.406,0.412,0.211,0.372,0.358,0.403,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
26,ESM2 (650M),Protein language model,0.414,0.011,0.425,0.337,0.415,0.368,0.523,0.335,0.406,0.516,0.457,0.486,0.457,0.261,0.422,0.25,0.209,0.16,0.213,ESM2 model (650M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
27,DeepSequence (single),Alignment-based model,0.407,0.012,0.447,0.349,0.371,0.395,0.473,0.382,0.415,0.463,0.437,0.479,0.428,0.338,0.392,0.256,0.295,0.302,0.368,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
28,ESM-1v (ensemble),Protein language model,0.406,0.012,0.42,0.32,0.429,0.386,0.477,0.326,0.405,0.5,0.458,0.464,0.413,0.294,0.403,0.218,0.178,0.151,0.215,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
29,SaProt (35M),Hybrid - Structure & PLM,0.406,0.008,0.372,0.357,0.439,0.285,0.575,0.323,0.392,0.506,0.443,0.492,0.425,0.242,0.409,0.354,0.178,0.172,0.231,SaProt (35M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
30,ESM2 (3B),Protein language model,0.405,0.01,0.417,0.321,0.403,0.378,0.509,0.348,0.415,0.49,0.442,0.477,0.457,0.294,0.414,0.216,0.194,0.165,0.212,ESM2 model (3B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
31,ESM2 (15B),Protein language model,0.4,0.01,0.405,0.317,0.405,0.387,0.488,0.357,0.414,0.472,0.432,0.467,0.436,0.333,0.405,0.205,0.226,0.169,0.228,ESM2 model (15B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
32,MIF-ST,Hybrid - Structure & PLM,0.4,0.009,0.39,0.321,0.438,0.364,0.485,0.376,0.403,0.455,0.399,0.413,0.455,0.405,0.431,0.263,0.333,0.282,0.307,MIF-ST model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
33,EVmutation,Alignment-based model,0.395,0.008,0.44,0.317,0.378,0.409,0.43,0.403,0.423,0.408,0.41,0.448,0.413,0.393,0.376,0.277,0.344,0.31,0.395,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
34,ESM-1b,Protein language model,0.394,0.009,0.428,0.287,0.406,0.349,0.5,0.35,0.398,0.482,0.435,0.491,0.438,0.26,0.384,0.229,0.184,0.148,0.268,ESM-1b (w/ Brandes et al. extensions),"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a>"
35,VESPAl,Protein language model,0.393,0.009,0.429,0.347,0.326,0.402,0.461,0.382,0.412,0.447,0.394,0.472,0.433,0.403,0.386,0.143,0.332,0.272,0.31,VESPAl model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
36,Progen2 XL,Protein language model,0.391,0.009,0.402,0.302,0.418,0.387,0.445,0.354,0.405,0.445,0.386,0.458,0.418,0.402,0.385,0.187,0.283,0.235,0.289,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
37,ESM2 (150M),Protein language model,0.386,0.012,0.391,0.326,0.402,0.3,0.51,0.306,0.358,0.495,0.45,0.475,0.392,0.157,0.386,0.242,0.122,0.141,0.2,ESM2 model (150M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
38,MIF,Inverse folding model,0.382,0.01,0.327,0.336,0.43,0.297,0.522,0.349,0.375,0.446,0.398,0.387,0.413,0.365,0.411,0.268,0.261,0.229,0.251,MIF model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
39,Progen2 L,Protein language model,0.38,0.009,0.406,0.293,0.427,0.379,0.396,0.348,0.381,0.42,0.411,0.432,0.363,0.323,0.372,0.146,0.23,0.205,0.265,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
40,Progen2 M,Protein language model,0.379,0.009,0.393,0.295,0.433,0.381,0.396,0.318,0.382,0.426,0.413,0.417,0.353,0.335,0.373,0.132,0.144,0.133,0.174,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
41,Progen2 Base,Protein language model,0.377,0.01,0.396,0.294,0.437,0.378,0.383,0.342,0.368,0.423,0.422,0.422,0.328,0.316,0.37,0.128,0.136,0.144,0.196,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
42,Tranception L no retrieval,Protein language model,0.374,0.01,0.401,0.288,0.413,0.386,0.381,0.358,0.371,0.416,0.391,0.394,0.353,0.404,0.363,0.174,0.305,0.247,0.325,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
43,ESM-1v (single),Protein language model,0.374,0.011,0.396,0.268,0.405,0.361,0.437,0.287,0.371,0.476,0.427,0.433,0.385,0.258,0.376,0.192,0.184,0.146,0.207,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
44,Wavenet,Alignment-based model,0.373,0.013,0.379,0.325,0.35,0.363,0.449,0.299,0.389,0.451,0.393,0.425,0.405,0.34,0.358,0.207,0.241,0.209,0.29,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
45,RITA XL,Protein language model,0.372,0.011,0.366,0.302,0.414,0.381,0.398,0.315,0.382,0.413,0.396,0.399,0.342,0.394,0.358,0.141,0.144,0.157,0.23,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
46,CARP (640M),Protein language model,0.368,0.01,0.395,0.273,0.397,0.365,0.412,0.314,0.374,0.428,0.417,0.395,0.38,0.284,0.391,0.213,0.173,0.151,0.171,CARP model (640M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
47,RITA L,Protein language model,0.365,0.01,0.359,0.29,0.42,0.374,0.383,0.316,0.369,0.403,0.396,0.399,0.309,0.384,0.348,0.136,0.13,0.138,0.204,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
48,Site-Independent,Alignment-based model,0.359,0.014,0.369,0.344,0.343,0.379,0.358,0.426,0.373,0.316,0.38,0.389,0.312,0.375,0.337,0.238,0.232,0.261,0.338,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
49,RITA M,Protein language model,0.349,0.01,0.352,0.273,0.405,0.369,0.348,0.304,0.349,0.388,0.379,0.367,0.295,0.375,0.337,0.112,0.124,0.143,0.2,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
50,Tranception M no retrieval,Protein language model,0.348,0.01,0.349,0.284,0.406,0.359,0.342,0.293,0.349,0.376,0.38,0.357,0.295,0.34,0.332,0.14,0.094,0.12,0.182,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
51,Unirep evotuned,Hybrid - Alignment & PLM,0.347,0.012,0.355,0.305,0.365,0.344,0.366,0.33,0.344,0.371,0.357,0.378,0.33,0.348,0.319,0.156,0.247,0.228,0.295,Unirep model w/ evotuning,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
52,Progen2 S,Protein language model,0.335,0.011,0.333,0.275,0.384,0.334,0.349,0.283,0.321,0.389,0.385,0.358,0.286,0.271,0.327,0.111,0.106,0.111,0.162,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
53,CARP (76M),Protein language model,0.327,0.012,0.342,0.282,0.369,0.268,0.376,0.247,0.301,0.406,0.388,0.366,0.296,0.157,0.336,0.204,0.096,0.109,0.117,CARP model (76M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
54,ESM2 (35M),Protein language model,0.32,0.013,0.314,0.291,0.343,0.212,0.439,0.239,0.271,0.448,0.371,0.408,0.308,0.12,0.305,0.24,0.113,0.131,0.198,ESM2 model (35M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
55,RITA S,Protein language model,0.304,0.012,0.294,0.275,0.336,0.325,0.289,0.276,0.297,0.332,0.331,0.299,0.232,0.344,0.287,0.111,0.09,0.112,0.173,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
56,Tranception S no retrieval,Protein language model,0.302,0.012,0.288,0.286,0.349,0.317,0.27,0.258,0.295,0.317,0.318,0.283,0.257,0.304,0.279,0.113,0.096,0.117,0.167,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
57,CARP (38M),Protein language model,0.279,0.014,0.285,0.268,0.312,0.215,0.314,0.196,0.239,0.356,0.322,0.307,0.248,0.127,0.279,0.168,0.083,0.111,0.133,CARP model (38M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
58,ProteinMPNN,Inverse folding model,0.257,0.011,0.197,0.163,0.198,0.161,0.565,0.173,0.279,0.433,0.284,0.394,0.343,0.262,0.292,0.259,0.167,0.17,0.273,ProteinMPNN model,"<a href='https://www.science.org/doi/10.1126/science.add2187'>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.</a>"
59,ESM2 (8M),Protein language model,0.225,0.014,0.201,0.26,0.266,0.136,0.262,0.194,0.179,0.261,0.241,0.248,0.194,0.09,0.203,0.135,0.087,0.115,0.176,ESM2 model (8M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
60,Unirep,Protein language model,0.189,0.016,0.182,0.202,0.216,0.135,0.21,0.181,0.161,0.205,0.215,0.233,0.146,0.066,0.175,0.07,0.098,0.123,0.171,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
61,ProtGPT2,Protein language model,0.188,0.012,0.176,0.149,0.193,0.164,0.257,0.177,0.173,0.253,0.244,0.242,0.132,0.138,0.181,0.139,0.041,0.036,0.064,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
62,CARP (600K),Protein language model,0.106,0.016,0.112,0.084,0.171,0.056,0.105,0.095,0.087,0.098,0.122,0.086,0.072,0.055,0.108,0.025,0.026,0.068,0.1,CARP model (600K params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
