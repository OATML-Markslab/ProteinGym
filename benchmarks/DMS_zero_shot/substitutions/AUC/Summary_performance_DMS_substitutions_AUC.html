<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model_name</th>
      <th>Model type</th>
      <th>Average_AUC</th>
      <th>Bootstrap_standard_error_AUC</th>
      <th>Function_Activity</th>
      <th>Function_Binding</th>
      <th>Function_Expression</th>
      <th>Function_OrganismalFitness</th>
      <th>Function_Stability</th>
      <th>Low_MSA_depth</th>
      <th>Medium_MSA_depth</th>
      <th>High_MSA_depth</th>
      <th>Taxa_Human</th>
      <th>Taxa_Other_Eukaryote</th>
      <th>Taxa_Prokaryote</th>
      <th>Taxa_Virus</th>
      <th>Depth_1</th>
      <th>Depth_2</th>
      <th>Depth_3</th>
      <th>Depth_4</th>
      <th>Depth_5+</th>
      <th>Model details</th>
      <th>References</th>
    </tr>
    <tr>
      <th>Model_rank</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>GEMME</td>
      <td>Alignment-based model</td>
      <td>0.716</td>
      <td>0.000</td>
      <td>0.627</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.765</td>
      <td>0.757</td>
      <td>0.757</td>
      <td>0.702</td>
      <td>0.778</td>
      <td>0.757</td>
      <td>0.730</td>
      <td>0.709</td>
      <td>0.747</td>
      <td>0.741</td>
      <td>0.750</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>GEMME model</td>
      <td><a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a></td>
    </tr>
    <tr>
      <th>2</th>
      <td>Tranception L</td>
      <td>Hybrid model</td>
      <td>0.699</td>
      <td>0.009</td>
      <td>0.598</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.755</td>
      <td>0.743</td>
      <td>0.743</td>
      <td>0.693</td>
      <td>0.753</td>
      <td>0.743</td>
      <td>0.666</td>
      <td>0.708</td>
      <td>0.734</td>
      <td>0.724</td>
      <td>0.736</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Tranception Large model (700M params) with retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>3</th>
      <td>TranceptEVE L</td>
      <td>Hybrid model</td>
      <td>0.696</td>
      <td>0.012</td>
      <td>0.590</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.771</td>
      <td>0.728</td>
      <td>0.728</td>
      <td>0.708</td>
      <td>0.755</td>
      <td>0.728</td>
      <td>0.641</td>
      <td>0.720</td>
      <td>0.751</td>
      <td>0.737</td>
      <td>0.723</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TranceptEVE Large model (Tranception Large & retrieved EVE model)</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a></td>
    </tr>
    <tr>
      <th>4</th>
      <td>TranceptEVE S</td>
      <td>Hybrid model</td>
      <td>0.695</td>
      <td>0.012</td>
      <td>0.582</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.763</td>
      <td>0.739</td>
      <td>0.739</td>
      <td>0.701</td>
      <td>0.746</td>
      <td>0.739</td>
      <td>0.636</td>
      <td>0.710</td>
      <td>0.744</td>
      <td>0.730</td>
      <td>0.730</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TranceptEVE Small model (Tranception Small & retrieved EVE model)</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>5</th>
      <td>MSA Transformer (ensemble)</td>
      <td>Hybrid model</td>
      <td>0.691</td>
      <td>0.012</td>
      <td>0.606</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.762</td>
      <td>0.704</td>
      <td>0.704</td>
      <td>0.703</td>
      <td>0.756</td>
      <td>0.704</td>
      <td>0.660</td>
      <td>0.720</td>
      <td>0.741</td>
      <td>0.723</td>
      <td>0.701</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>MSA Transformer (ensemble of 5 MSA samples)</td>
      <td><a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a></td>
    </tr>
    <tr>
      <th>6</th>
      <td>Tranception S</td>
      <td>Hybrid model</td>
      <td>0.689</td>
      <td>0.008</td>
      <td>0.588</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.740</td>
      <td>0.740</td>
      <td>0.740</td>
      <td>0.686</td>
      <td>0.729</td>
      <td>0.740</td>
      <td>0.661</td>
      <td>0.681</td>
      <td>0.728</td>
      <td>0.716</td>
      <td>0.733</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Tranception Small model (85M params) with retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>7</th>
      <td>MSA Transformer (single)</td>
      <td>Hybrid model</td>
      <td>0.689</td>
      <td>0.011</td>
      <td>0.612</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.751</td>
      <td>0.704</td>
      <td>0.704</td>
      <td>0.685</td>
      <td>0.769</td>
      <td>0.704</td>
      <td>0.674</td>
      <td>0.728</td>
      <td>0.718</td>
      <td>0.716</td>
      <td>0.700</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>MSA Transformer (single MSA sample)</td>
      <td><a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a></td>
    </tr>
    <tr>
      <th>8</th>
      <td>EVE (ensemble)</td>
      <td>Alignment-based model</td>
      <td>0.687</td>
      <td>0.003</td>
      <td>0.622</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.762</td>
      <td>0.676</td>
      <td>0.676</td>
      <td>0.700</td>
      <td>0.772</td>
      <td>0.676</td>
      <td>0.717</td>
      <td>0.709</td>
      <td>0.743</td>
      <td>0.734</td>
      <td>0.669</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>EVE model (ensemble of 5 independently-trained models)</td>
      <td><a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a></td>
    </tr>
    <tr>
      <th>9</th>
      <td>TranceptEVE M</td>
      <td>Hybrid model</td>
      <td>0.687</td>
      <td>0.013</td>
      <td>0.580</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.763</td>
      <td>0.716</td>
      <td>0.716</td>
      <td>0.702</td>
      <td>0.744</td>
      <td>0.716</td>
      <td>0.631</td>
      <td>0.711</td>
      <td>0.744</td>
      <td>0.725</td>
      <td>0.709</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>TranceptEVE Medium model (Tranception Medium & retrieved EVE model)</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a></td>
    </tr>
    <tr>
      <th>10</th>
      <td>EVmutation</td>
      <td>Alignment-based model</td>
      <td>0.686</td>
      <td>0.004</td>
      <td>0.626</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.725</td>
      <td>0.707</td>
      <td>0.707</td>
      <td>0.670</td>
      <td>0.750</td>
      <td>0.707</td>
      <td>0.733</td>
      <td>0.678</td>
      <td>0.708</td>
      <td>0.712</td>
      <td>0.703</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>EVmutation model</td>
      <td><a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a></td>
    </tr>
    <tr>
      <th>11</th>
      <td>EVE (single)</td>
      <td>Alignment-based model</td>
      <td>0.684</td>
      <td>0.003</td>
      <td>0.622</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.758</td>
      <td>0.673</td>
      <td>0.673</td>
      <td>0.697</td>
      <td>0.769</td>
      <td>0.673</td>
      <td>0.716</td>
      <td>0.706</td>
      <td>0.740</td>
      <td>0.729</td>
      <td>0.666</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>EVE model (single seed)</td>
      <td><a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a></td>
    </tr>
    <tr>
      <th>12</th>
      <td>MIF-ST</td>
      <td>Inverse folding model</td>
      <td>0.677</td>
      <td>0.008</td>
      <td>0.628</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.706</td>
      <td>0.697</td>
      <td>0.697</td>
      <td>0.639</td>
      <td>0.766</td>
      <td>0.697</td>
      <td>0.724</td>
      <td>0.702</td>
      <td>0.665</td>
      <td>0.702</td>
      <td>0.696</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>MIF-ST model</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a></td>
    </tr>
    <tr>
      <th>13</th>
      <td>Tranception M</td>
      <td>Hybrid model</td>
      <td>0.676</td>
      <td>0.009</td>
      <td>0.589</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.745</td>
      <td>0.695</td>
      <td>0.695</td>
      <td>0.691</td>
      <td>0.732</td>
      <td>0.695</td>
      <td>0.657</td>
      <td>0.686</td>
      <td>0.734</td>
      <td>0.711</td>
      <td>0.687</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Tranception Medium model (300M params) with retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>14</th>
      <td>Site-Independent</td>
      <td>Alignment-based model</td>
      <td>0.676</td>
      <td>0.010</td>
      <td>0.609</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.705</td>
      <td>0.714</td>
      <td>0.714</td>
      <td>0.681</td>
      <td>0.681</td>
      <td>0.714</td>
      <td>0.727</td>
      <td>0.602</td>
      <td>0.728</td>
      <td>0.694</td>
      <td>0.707</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Site-Independent model</td>
      <td><a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a></td>
    </tr>
    <tr>
      <th>15</th>
      <td>DeepSequence (single)</td>
      <td>Alignment-based model</td>
      <td>0.669</td>
      <td>0.024</td>
      <td>0.562</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.710</td>
      <td>0.735</td>
      <td>0.735</td>
      <td>0.660</td>
      <td>0.696</td>
      <td>0.735</td>
      <td>0.568</td>
      <td>0.692</td>
      <td>0.686</td>
      <td>0.680</td>
      <td>0.731</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>DeepSequence model (single seed)</td>
      <td><a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a></td>
    </tr>
    <tr>
      <th>16</th>
      <td>ESM2 (3B)</td>
      <td>Protein language model</td>
      <td>0.667</td>
      <td>0.022</td>
      <td>0.534</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.707</td>
      <td>0.758</td>
      <td>0.758</td>
      <td>0.622</td>
      <td>0.734</td>
      <td>0.758</td>
      <td>0.557</td>
      <td>0.719</td>
      <td>0.650</td>
      <td>0.685</td>
      <td>0.752</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM2 model (3B params)</td>
      <td><a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a></td>
    </tr>
    <tr>
      <th>17</th>
      <td>DeepSequence (ensemble)</td>
      <td>Alignment-based model</td>
      <td>0.661</td>
      <td>0.027</td>
      <td>0.540</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.721</td>
      <td>0.722</td>
      <td>0.722</td>
      <td>0.660</td>
      <td>0.702</td>
      <td>0.722</td>
      <td>0.534</td>
      <td>0.706</td>
      <td>0.688</td>
      <td>0.688</td>
      <td>0.717</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>DeepSequence model (ensemble of 5 independently-trained models)</td>
      <td><a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a></td>
    </tr>
    <tr>
      <th>18</th>
      <td>Tranception L no retrieval</td>
      <td>Protein language model</td>
      <td>0.659</td>
      <td>0.026</td>
      <td>0.544</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.740</td>
      <td>0.692</td>
      <td>0.692</td>
      <td>0.688</td>
      <td>0.695</td>
      <td>0.692</td>
      <td>0.538</td>
      <td>0.699</td>
      <td>0.723</td>
      <td>0.694</td>
      <td>0.684</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Tranception Large model (700M params) without retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>19</th>
      <td>VESPA</td>
      <td>Protein language model</td>
      <td>0.655</td>
      <td>0.008</td>
      <td>0.589</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.754</td>
      <td>0.622</td>
      <td>0.622</td>
      <td>0.674</td>
      <td>0.778</td>
      <td>0.622</td>
      <td>0.668</td>
      <td>0.725</td>
      <td>0.715</td>
      <td>0.707</td>
      <td>0.612</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>VESPA model</td>
      <td><a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a></td>
    </tr>
    <tr>
      <th>20</th>
      <td>ESM2 (650M)</td>
      <td>Protein language model</td>
      <td>0.651</td>
      <td>0.024</td>
      <td>0.532</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.685</td>
      <td>0.736</td>
      <td>0.736</td>
      <td>0.595</td>
      <td>0.734</td>
      <td>0.736</td>
      <td>0.549</td>
      <td>0.723</td>
      <td>0.615</td>
      <td>0.671</td>
      <td>0.732</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM2 model (650M params)</td>
      <td><a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a></td>
    </tr>
    <tr>
      <th>21</th>
      <td>Progen2 XL</td>
      <td>Protein language model</td>
      <td>0.651</td>
      <td>0.022</td>
      <td>0.552</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.740</td>
      <td>0.662</td>
      <td>0.662</td>
      <td>0.677</td>
      <td>0.720</td>
      <td>0.662</td>
      <td>0.561</td>
      <td>0.713</td>
      <td>0.710</td>
      <td>0.697</td>
      <td>0.653</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Progen2 xlarge model (6.4B params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>22</th>
      <td>ESM-1v (ensemble)</td>
      <td>Protein language model</td>
      <td>0.648</td>
      <td>0.025</td>
      <td>0.542</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.696</td>
      <td>0.706</td>
      <td>0.706</td>
      <td>0.617</td>
      <td>0.724</td>
      <td>0.706</td>
      <td>0.558</td>
      <td>0.713</td>
      <td>0.641</td>
      <td>0.650</td>
      <td>0.706</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM-1v (ensemble of 5 independently-trained models)</td>
      <td><a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a></td>
    </tr>
    <tr>
      <th>23</th>
      <td>CARP (640M)</td>
      <td>Protein language model</td>
      <td>0.648</td>
      <td>0.023</td>
      <td>0.537</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.676</td>
      <td>0.730</td>
      <td>0.730</td>
      <td>0.607</td>
      <td>0.699</td>
      <td>0.730</td>
      <td>0.550</td>
      <td>0.690</td>
      <td>0.628</td>
      <td>0.664</td>
      <td>0.731</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>CARP model (640M params)</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a></td>
    </tr>
    <tr>
      <th>24</th>
      <td>Tranception S no retrieval</td>
      <td>Protein language model</td>
      <td>0.646</td>
      <td>0.024</td>
      <td>0.526</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.698</td>
      <td>0.713</td>
      <td>0.713</td>
      <td>0.671</td>
      <td>0.630</td>
      <td>0.713</td>
      <td>0.532</td>
      <td>0.626</td>
      <td>0.708</td>
      <td>0.673</td>
      <td>0.706</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Tranception Small model (85M params) without retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>25</th>
      <td>RITA XL</td>
      <td>Protein language model</td>
      <td>0.645</td>
      <td>0.024</td>
      <td>0.544</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.733</td>
      <td>0.659</td>
      <td>0.659</td>
      <td>0.690</td>
      <td>0.678</td>
      <td>0.659</td>
      <td>0.547</td>
      <td>0.676</td>
      <td>0.727</td>
      <td>0.691</td>
      <td>0.650</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>RITA xlarge model (1.2B params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>26</th>
      <td>ESM2 (15B)</td>
      <td>Protein language model</td>
      <td>0.644</td>
      <td>0.022</td>
      <td>0.536</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.709</td>
      <td>0.688</td>
      <td>0.688</td>
      <td>0.634</td>
      <td>0.718</td>
      <td>0.688</td>
      <td>0.555</td>
      <td>0.705</td>
      <td>0.664</td>
      <td>0.679</td>
      <td>0.680</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM2 model (15B params)</td>
      <td><a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a></td>
    </tr>
    <tr>
      <th>27</th>
      <td>Progen2 L</td>
      <td>Protein language model</td>
      <td>0.642</td>
      <td>0.025</td>
      <td>0.537</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.725</td>
      <td>0.664</td>
      <td>0.664</td>
      <td>0.667</td>
      <td>0.697</td>
      <td>0.664</td>
      <td>0.535</td>
      <td>0.698</td>
      <td>0.699</td>
      <td>0.685</td>
      <td>0.656</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Progen2 large model (2.7B params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>28</th>
      <td>MIF</td>
      <td>Inverse folding model</td>
      <td>0.642</td>
      <td>0.010</td>
      <td>0.614</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.661</td>
      <td>0.650</td>
      <td>0.650</td>
      <td>0.601</td>
      <td>0.730</td>
      <td>0.650</td>
      <td>0.713</td>
      <td>0.664</td>
      <td>0.622</td>
      <td>0.663</td>
      <td>0.648</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>MIF model</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a></td>
    </tr>
    <tr>
      <th>29</th>
      <td>Progen2 M</td>
      <td>Protein language model</td>
      <td>0.642</td>
      <td>0.024</td>
      <td>0.540</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.731</td>
      <td>0.654</td>
      <td>0.654</td>
      <td>0.674</td>
      <td>0.698</td>
      <td>0.654</td>
      <td>0.543</td>
      <td>0.696</td>
      <td>0.709</td>
      <td>0.688</td>
      <td>0.646</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Progen2 medium model (760M params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>30</th>
      <td>RITA L</td>
      <td>Protein language model</td>
      <td>0.641</td>
      <td>0.024</td>
      <td>0.533</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.724</td>
      <td>0.666</td>
      <td>0.666</td>
      <td>0.687</td>
      <td>0.658</td>
      <td>0.666</td>
      <td>0.535</td>
      <td>0.657</td>
      <td>0.726</td>
      <td>0.682</td>
      <td>0.657</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>RITA large model (680M params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>31</th>
      <td>Progen2 Base</td>
      <td>Protein language model</td>
      <td>0.641</td>
      <td>0.027</td>
      <td>0.526</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.729</td>
      <td>0.667</td>
      <td>0.667</td>
      <td>0.672</td>
      <td>0.688</td>
      <td>0.667</td>
      <td>0.514</td>
      <td>0.695</td>
      <td>0.706</td>
      <td>0.687</td>
      <td>0.659</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Progen2 base model (760M params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>32</th>
      <td>Progen2 S</td>
      <td>Protein language model</td>
      <td>0.636</td>
      <td>0.027</td>
      <td>0.509</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.680</td>
      <td>0.718</td>
      <td>0.718</td>
      <td>0.636</td>
      <td>0.641</td>
      <td>0.718</td>
      <td>0.501</td>
      <td>0.646</td>
      <td>0.665</td>
      <td>0.658</td>
      <td>0.710</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Progen2 small model (150M params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>33</th>
      <td>VESPAl</td>
      <td>Protein language model</td>
      <td>0.636</td>
      <td>0.007</td>
      <td>0.588</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.731</td>
      <td>0.588</td>
      <td>0.588</td>
      <td>0.657</td>
      <td>0.759</td>
      <td>0.588</td>
      <td>0.672</td>
      <td>0.703</td>
      <td>0.696</td>
      <td>0.686</td>
      <td>0.575</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>VESPAl model</td>
      <td><a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a></td>
    </tr>
    <tr>
      <th>34</th>
      <td>ESM-1v (single)</td>
      <td>Protein language model</td>
      <td>0.629</td>
      <td>0.025</td>
      <td>0.540</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.682</td>
      <td>0.665</td>
      <td>0.665</td>
      <td>0.604</td>
      <td>0.717</td>
      <td>0.665</td>
      <td>0.558</td>
      <td>0.705</td>
      <td>0.625</td>
      <td>0.653</td>
      <td>0.657</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM-1v (single seed)</td>
      <td><a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a></td>
    </tr>
    <tr>
      <th>35</th>
      <td>Tranception M no retrieval</td>
      <td>Protein language model</td>
      <td>0.627</td>
      <td>0.025</td>
      <td>0.524</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.718</td>
      <td>0.639</td>
      <td>0.639</td>
      <td>0.682</td>
      <td>0.649</td>
      <td>0.639</td>
      <td>0.522</td>
      <td>0.650</td>
      <td>0.721</td>
      <td>0.672</td>
      <td>0.631</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Tranception Medium model (300M params) without retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>36</th>
      <td>RITA M</td>
      <td>Protein language model</td>
      <td>0.626</td>
      <td>0.025</td>
      <td>0.517</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.717</td>
      <td>0.645</td>
      <td>0.645</td>
      <td>0.684</td>
      <td>0.640</td>
      <td>0.645</td>
      <td>0.516</td>
      <td>0.641</td>
      <td>0.725</td>
      <td>0.669</td>
      <td>0.637</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>RITA medium model (300M params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>37</th>
      <td>RITA S</td>
      <td>Protein language model</td>
      <td>0.624</td>
      <td>0.022</td>
      <td>0.504</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.695</td>
      <td>0.672</td>
      <td>0.672</td>
      <td>0.669</td>
      <td>0.610</td>
      <td>0.672</td>
      <td>0.518</td>
      <td>0.601</td>
      <td>0.714</td>
      <td>0.660</td>
      <td>0.664</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>RITA small model (85M params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>38</th>
      <td>ESM-1b</td>
      <td>Protein language model</td>
      <td>0.618</td>
      <td>0.024</td>
      <td>0.538</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.650</td>
      <td>0.666</td>
      <td>0.666</td>
      <td>0.568</td>
      <td>0.713</td>
      <td>0.666</td>
      <td>0.556</td>
      <td>0.701</td>
      <td>0.580</td>
      <td>0.637</td>
      <td>0.657</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM-1b (w/ Brandes et al. extensions)</td>
      <td>[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a></td>
    </tr>
    <tr>
      <th>39</th>
      <td>ProtGPT2</td>
      <td>Protein language model</td>
      <td>0.608</td>
      <td>0.023</td>
      <td>0.525</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.564</td>
      <td>0.736</td>
      <td>0.736</td>
      <td>0.552</td>
      <td>0.558</td>
      <td>0.736</td>
      <td>0.537</td>
      <td>0.550</td>
      <td>0.562</td>
      <td>0.570</td>
      <td>0.781</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ProtGPT2 model</td>
      <td><a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a></td>
    </tr>
    <tr>
      <th>40</th>
      <td>CARP (76M)</td>
      <td>Protein language model</td>
      <td>0.607</td>
      <td>0.020</td>
      <td>0.516</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.609</td>
      <td>0.696</td>
      <td>0.696</td>
      <td>0.536</td>
      <td>0.668</td>
      <td>0.696</td>
      <td>0.556</td>
      <td>0.641</td>
      <td>0.552</td>
      <td>0.611</td>
      <td>0.696</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>CARP model (76M params)</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a></td>
    </tr>
    <tr>
      <th>41</th>
      <td>ESM2 (150M)</td>
      <td>Protein language model</td>
      <td>0.605</td>
      <td>0.020</td>
      <td>0.527</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.584</td>
      <td>0.703</td>
      <td>0.703</td>
      <td>0.503</td>
      <td>0.681</td>
      <td>0.703</td>
      <td>0.564</td>
      <td>0.656</td>
      <td>0.506</td>
      <td>0.594</td>
      <td>0.701</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM2 model (150M params)</td>
      <td><a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a></td>
    </tr>
    <tr>
      <th>42</th>
      <td>CARP (38M)</td>
      <td>Protein language model</td>
      <td>0.601</td>
      <td>0.020</td>
      <td>0.504</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.596</td>
      <td>0.703</td>
      <td>0.703</td>
      <td>0.535</td>
      <td>0.637</td>
      <td>0.703</td>
      <td>0.536</td>
      <td>0.616</td>
      <td>0.550</td>
      <td>0.599</td>
      <td>0.702</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>CARP model (38M params)</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a></td>
    </tr>
    <tr>
      <th>43</th>
      <td>Unirep evotuned</td>
      <td>Hybrid model</td>
      <td>0.600</td>
      <td>0.027</td>
      <td>0.532</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.687</td>
      <td>0.581</td>
      <td>0.581</td>
      <td>0.655</td>
      <td>0.637</td>
      <td>0.581</td>
      <td>0.526</td>
      <td>0.641</td>
      <td>0.684</td>
      <td>0.630</td>
      <td>0.569</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Unirep model w/ evotuning</td>
      <td><a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a></td>
    </tr>
    <tr>
      <th>44</th>
      <td>ESM2 (35M)</td>
      <td>Protein language model</td>
      <td>0.593</td>
      <td>0.017</td>
      <td>0.511</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.569</td>
      <td>0.700</td>
      <td>0.700</td>
      <td>0.494</td>
      <td>0.655</td>
      <td>0.700</td>
      <td>0.553</td>
      <td>0.627</td>
      <td>0.500</td>
      <td>0.581</td>
      <td>0.698</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM2 model (35M params)</td>
      <td><a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a></td>
    </tr>
    <tr>
      <th>45</th>
      <td>ESM2 (8M)</td>
      <td>Protein language model</td>
      <td>0.590</td>
      <td>0.018</td>
      <td>0.510</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.552</td>
      <td>0.709</td>
      <td>0.709</td>
      <td>0.490</td>
      <td>0.626</td>
      <td>0.709</td>
      <td>0.542</td>
      <td>0.604</td>
      <td>0.493</td>
      <td>0.571</td>
      <td>0.705</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM2 model (8M params)</td>
      <td><a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a></td>
    </tr>
    <tr>
      <th>46</th>
      <td>Wavenet</td>
      <td>Alignment-based model</td>
      <td>0.590</td>
      <td>0.025</td>
      <td>0.536</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.705</td>
      <td>0.529</td>
      <td>0.529</td>
      <td>0.652</td>
      <td>0.680</td>
      <td>0.529</td>
      <td>0.534</td>
      <td>0.682</td>
      <td>0.680</td>
      <td>0.640</td>
      <td>0.515</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Wavenet model</td>
      <td><a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a></td>
    </tr>
    <tr>
      <th>47</th>
      <td>Unirep</td>
      <td>Protein language model</td>
      <td>0.578</td>
      <td>0.022</td>
      <td>0.502</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.539</td>
      <td>0.693</td>
      <td>0.693</td>
      <td>0.485</td>
      <td>0.603</td>
      <td>0.693</td>
      <td>0.515</td>
      <td>0.594</td>
      <td>0.485</td>
      <td>0.559</td>
      <td>0.691</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Unirep model</td>
      <td><a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a></td>
    </tr>
    <tr>
      <th>48</th>
      <td>ProteinMPNN</td>
      <td>Inverse folding model</td>
      <td>0.575</td>
      <td>0.005</td>
      <td>0.600</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.598</td>
      <td>0.527</td>
      <td>0.527</td>
      <td>0.559</td>
      <td>0.665</td>
      <td>0.527</td>
      <td>0.683</td>
      <td>0.609</td>
      <td>0.570</td>
      <td>0.595</td>
      <td>0.526</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ProteinMPNN model</td>
      <td><a href='https://www.science.org/doi/10.1126/science.add2187'>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.</a></td>
    </tr>
    <tr>
      <th>49</th>
      <td>ESM-IF1</td>
      <td>Inverse folding model</td>
      <td>0.570</td>
      <td>0.007</td>
      <td>0.626</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.678</td>
      <td>0.405</td>
      <td>0.405</td>
      <td>0.613</td>
      <td>0.752</td>
      <td>0.405</td>
      <td>0.738</td>
      <td>0.677</td>
      <td>0.638</td>
      <td>0.644</td>
      <td>0.403</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ESM-IF1 model</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.</a></td>
    </tr>
    <tr>
      <th>50</th>
      <td>CARP (600K)</td>
      <td>Protein language model</td>
      <td>0.568</td>
      <td>0.020</td>
      <td>0.492</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.513</td>
      <td>0.699</td>
      <td>0.699</td>
      <td>0.485</td>
      <td>0.547</td>
      <td>0.699</td>
      <td>0.514</td>
      <td>0.532</td>
      <td>0.489</td>
      <td>0.539</td>
      <td>0.699</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>CARP model (600K params)</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a></td>
    </tr>
  </tbody>
</table>