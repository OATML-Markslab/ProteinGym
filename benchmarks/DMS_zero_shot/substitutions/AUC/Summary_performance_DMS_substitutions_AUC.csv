Model_rank,Model_name,Model type,Average_AUC,Bootstrap_standard_error_AUC,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,VenusREM,Structure & MSA,0.783,0.0,0.769,0.745,0.789,0.758,0.854,0.774,0.79,0.814,0.796,0.819,0.796,0.764,0.8,0.733,0.725,0.699,0.756,VenusREM,"<a href='https://arxiv.org/abs/2410.21127'>Yang Tan, Ruilin Wang, Banghao Wu, Liang Hong, Bingxin Zhou. (2024). Retrieval-Enhanced Mutation Mastery: Augmenting Zero-Shot Prediction of Protein Language Model. ArXiv, abs/2410.21127.</a>"
2,ProSST (K=2048),Single sequence & Structure,0.777,0.001,0.76,0.741,0.787,0.743,0.853,0.761,0.779,0.815,0.79,0.814,0.794,0.741,0.793,0.733,0.708,0.679,0.736,ProSST (K=2048),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
3,ProSST (K=4096),Single sequence & Structure,0.773,0.004,0.744,0.759,0.776,0.735,0.85,0.762,0.766,0.815,0.779,0.811,0.792,0.735,0.784,0.742,0.739,0.717,0.777,ProSST (K=4096),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
4,S3F-MSA,Structure & MSA,0.771,0.004,0.773,0.736,0.76,0.768,0.82,0.762,0.782,0.799,0.782,0.804,0.785,0.768,0.781,0.708,0.722,0.704,0.75,S3F with MSA retrieval,"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
5,S2F-MSA,Structure & MSA,0.767,0.004,0.77,0.731,0.756,0.765,0.815,0.758,0.779,0.794,0.779,0.798,0.781,0.763,0.776,0.694,0.703,0.687,0.735,S2F with MSA retrieval,"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
6,ProSST (K=1024),Single sequence & Structure,0.764,0.004,0.737,0.739,0.772,0.733,0.838,0.753,0.755,0.813,0.77,0.805,0.784,0.731,0.775,0.737,0.731,0.712,0.77,ProSST (K=1024),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
7,ESCOTT,Structure & MSA,0.761,0.004,0.769,0.711,0.754,0.761,0.812,0.753,0.776,0.79,0.772,0.794,0.779,0.769,0.772,0.691,0.711,0.719,0.779,ESCOTT,"<a href='https://www.medrxiv.org/content/10.1101/2024.02.03.24302219v1'>Mustafa Tekpinar, Laurent David, Thomas Henry, Alessandra Carbone. (2024). PRESCOTT: a population aware, epistatic and structural model accurately predicts missense effect. medRxiv.</a>"
8,PoET (200M),MSA,0.759,0.004,0.766,0.716,0.755,0.765,0.794,0.764,0.767,0.783,0.772,0.796,0.759,0.764,0.765,0.696,0.748,0.738,0.766,PoET (200M),"<a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'>Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.</a>"
9,ProSST (K=512),Single sequence & Structure,0.757,0.004,0.732,0.734,0.762,0.72,0.837,0.738,0.75,0.807,0.764,0.799,0.78,0.716,0.767,0.734,0.724,0.695,0.758,ProSST (K=512),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
10,S3F,Single sequence & Structure,0.757,0.004,0.754,0.722,0.753,0.733,0.822,0.739,0.759,0.799,0.773,0.79,0.775,0.721,0.771,0.702,0.707,0.682,0.729,"S3F (Sequence, Structure & Surface)","<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
11,ProSST (K=128),Single sequence & Structure,0.757,0.004,0.725,0.746,0.757,0.719,0.836,0.744,0.747,0.804,0.761,0.794,0.779,0.722,0.764,0.74,0.73,0.704,0.764,ProSST (K=128),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
12,ESM3 open (1.4B),"Single sequence, Structure & Function annotations",0.755,0.004,0.733,0.721,0.754,0.719,0.851,0.723,0.759,0.813,0.77,0.799,0.789,0.717,0.771,0.717,0.701,0.685,0.756,ESM3 open (1.4B params),"<a href='https://www.science.org/doi/10.1126/science.ads0018'>Hayes, T., Rao, R., Akin, H., Sofroniew, N.J., Oktay, D., Lin, Z., Verkuil, R., Tran, V.Q., Deaton, J., Wiggert, M., Badkundri, R., Shafkat, I., Gong, J., Derry, A., Molina, R.S., Thomas, N., Khan, Y.A., Mishra, C., Kim, C., Bartie, L.J., Nemeth, M., Hsu, P.D., Sercu, T., Candido, S., & Rives, A. (2025). Simulating 500 million years of evolution with a language model. Science.</a>"
13,RSALOR,Structure & MSA,0.754,0.005,0.755,0.72,0.732,0.738,0.823,0.753,0.76,0.794,0.764,0.789,0.775,0.759,0.768,0.69,0.709,0.721,0.783,RSALOR model,"<a href='https://www.biorxiv.org/content/10.1101/2025.02.03.636212v1'>Matsvei Tsishyn, Pauline Hermans, Fabrizio Pucci, Marianne Rooman. (2025). Residue conservation and solvent accessibility are (almost) all you need for predicting mutational effects in proteins. bioRxiv.</a>"
14,VespaG,Single sequence,0.754,0.004,0.769,0.7,0.749,0.746,0.804,0.738,0.766,0.787,0.77,0.795,0.77,0.727,0.762,0.666,0.708,0.693,0.738,VespaG model,"<a href='https://doi.org/10.1101/2024.04.24.590982'>Marquet, C., Schlensok, J., Abakarova, M., Rost, B., & Laine, E. (2024). Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction. bioRxiv.</a>"
15,TranceptEVE L,MSA,0.751,0.004,0.764,0.701,0.75,0.757,0.783,0.741,0.765,0.772,0.765,0.782,0.756,0.746,0.752,0.681,0.698,0.693,0.744,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
16,SaProt (650M),Single sequence & Structure,0.751,0.005,0.747,0.711,0.766,0.705,0.826,0.72,0.747,0.799,0.768,0.788,0.78,0.671,0.756,0.705,0.685,0.674,0.735,SaProt (650M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
17,S2F,Single sequence & Structure,0.75,0.004,0.749,0.715,0.745,0.728,0.812,0.729,0.752,0.79,0.767,0.78,0.767,0.712,0.762,0.68,0.683,0.657,0.698,S2F (Sequence & Structure),"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html'>Zuobai Zhang, Pascal Notin, Yining Huang, Aurelie C. Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang. (2024). Multi-Scale Representation Learning for Protein Fitness Prediction. NeurIPS.</a>"
18,TranceptEVE M,MSA,0.749,0.005,0.759,0.707,0.747,0.754,0.78,0.735,0.763,0.768,0.765,0.781,0.752,0.737,0.749,0.68,0.673,0.683,0.741,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
19,GEMME,MSA,0.749,0.006,0.759,0.704,0.737,0.753,0.792,0.743,0.765,0.773,0.762,0.783,0.76,0.754,0.754,0.674,0.693,0.698,0.766,GEMME model,"<a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a>"
20,TranceptEVE S,MSA,0.748,0.005,0.756,0.712,0.741,0.752,0.779,0.741,0.759,0.767,0.764,0.777,0.753,0.733,0.746,0.678,0.673,0.683,0.738,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
21,ProtSSN (ensemble),Single sequence & Structure,0.747,0.004,0.752,0.702,0.739,0.722,0.817,0.73,0.753,0.788,0.765,0.792,0.769,0.699,0.755,0.687,0.687,0.662,0.698,ProtSSN (ensemble of 9 models),"<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
22,"ProtSSN (k=20, h=1280)",Single sequence & Structure,0.743,0.004,0.748,0.702,0.732,0.716,0.817,0.726,0.748,0.786,0.761,0.789,0.766,0.695,0.751,0.686,0.683,0.657,0.698,"ProtSSN (k=20, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
23,VESPA,Single sequence,0.742,0.004,0.756,0.695,0.724,0.747,0.79,0.734,0.757,0.776,0.753,0.78,0.766,0.743,0.752,0.636,0.708,0.683,0.706,VESPA model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
24,"ProtSSN (k=20, h=512)",Single sequence & Structure,0.742,0.004,0.749,0.694,0.732,0.719,0.814,0.724,0.751,0.782,0.758,0.789,0.767,0.701,0.75,0.683,0.691,0.67,0.706,"ProtSSN (k=20, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
25,"ProtSSN (k=20, h=768)",Single sequence & Structure,0.741,0.004,0.748,0.691,0.734,0.717,0.814,0.721,0.748,0.785,0.76,0.789,0.765,0.692,0.749,0.687,0.682,0.657,0.713,"ProtSSN (k=20, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
26,"ProtSSN (k=30, h=1280)",Single sequence & Structure,0.741,0.004,0.746,0.698,0.732,0.715,0.813,0.723,0.745,0.786,0.759,0.788,0.765,0.687,0.747,0.688,0.682,0.654,0.698,"ProtSSN (k=30, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
27,EVE (ensemble),MSA,0.741,0.006,0.753,0.702,0.723,0.749,0.777,0.729,0.756,0.764,0.755,0.772,0.755,0.732,0.741,0.673,0.666,0.675,0.727,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
28,"ProtSSN (k=30, h=768)",Single sequence & Structure,0.741,0.004,0.747,0.695,0.732,0.715,0.814,0.72,0.748,0.783,0.758,0.789,0.765,0.692,0.748,0.684,0.68,0.654,0.697,"ProtSSN (k=30, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
29,"ProtSSN (k=30, h=512)",Single sequence & Structure,0.74,0.004,0.746,0.692,0.735,0.716,0.81,0.724,0.747,0.78,0.759,0.785,0.762,0.691,0.748,0.677,0.679,0.658,0.69,"ProtSSN (k=30, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
30,Tranception L,MSA,0.739,0.005,0.752,0.688,0.747,0.744,0.764,0.732,0.748,0.759,0.755,0.771,0.733,0.733,0.738,0.671,0.71,0.695,0.746,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
31,ProSST (K=20),Single sequence & Structure,0.739,0.004,0.713,0.727,0.746,0.699,0.81,0.724,0.73,0.779,0.744,0.773,0.753,0.704,0.746,0.716,0.694,0.675,0.74,ProSST (K=20),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
32,"ProtSSN (k=10, h=1280)",Single sequence & Structure,0.739,0.004,0.742,0.696,0.733,0.715,0.809,0.726,0.744,0.778,0.757,0.784,0.758,0.694,0.745,0.667,0.668,0.644,0.686,"ProtSSN (k=10, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
33,EVE (single),MSA,0.737,0.005,0.748,0.696,0.721,0.746,0.774,0.725,0.753,0.761,0.75,0.77,0.754,0.729,0.738,0.669,0.663,0.673,0.729,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
34,"ProtSSN (k=10, h=512)",Single sequence & Structure,0.737,0.004,0.746,0.691,0.725,0.712,0.811,0.724,0.742,0.781,0.757,0.788,0.762,0.682,0.744,0.679,0.673,0.65,0.687,"ProtSSN (k=10, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
35,MSA Transformer (ensemble),MSA,0.737,0.005,0.756,0.675,0.742,0.733,0.777,0.703,0.757,0.763,0.746,0.781,0.749,0.724,0.74,0.642,0.711,0.721,0.744,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
36,SiteRM,MSA,0.734,0.006,0.744,0.707,0.716,0.745,0.758,0.745,0.746,0.742,0.746,0.76,0.727,0.749,0.73,0.659,0.678,0.673,0.729,SiteRM,"<a href='https://papers.nips.cc/paper_files/paper/2024/hash/eb2f4fb51ac3b8dc4aac9cf71b0e7799-Abstract-Conference.html'>Sebastian Prillo, Wilson Wu, Yun Song. (2024). Ultrafast classical phylogenetic method beats large protein language models on variant effect prediction. NeurIPS.</a>"
37,Tranception M,MSA,0.734,0.005,0.741,0.694,0.74,0.739,0.756,0.724,0.743,0.749,0.751,0.766,0.722,0.719,0.731,0.665,0.654,0.662,0.727,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
38,"ProtSSN (k=10, h=768)",Single sequence & Structure,0.732,0.005,0.739,0.686,0.721,0.707,0.806,0.715,0.74,0.773,0.751,0.777,0.756,0.686,0.738,0.671,0.675,0.646,0.685,"ProtSSN (k=10, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
39,ESM-IF1,Structure,0.73,0.007,0.697,0.716,0.721,0.676,0.838,0.65,0.736,0.796,0.728,0.767,0.769,0.71,0.741,0.722,0.707,0.696,0.75,ESM-IF1 model,"<a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.</a>"
40,DeepSequence (ensemble),MSA,0.729,0.007,0.746,0.689,0.713,0.73,0.769,0.703,0.741,0.762,0.748,0.767,0.747,0.691,0.728,0.67,0.683,0.692,0.738,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
41,MSA Transformer (single),MSA,0.729,0.006,0.746,0.666,0.733,0.73,0.769,0.699,0.749,0.755,0.742,0.774,0.738,0.716,0.732,0.637,0.709,0.713,0.742,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
42,Tranception S,MSA,0.729,0.005,0.734,0.7,0.728,0.732,0.749,0.731,0.734,0.742,0.745,0.758,0.718,0.712,0.724,0.666,0.659,0.665,0.724,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
43,ESM2 (650M),Single sequence,0.729,0.007,0.73,0.691,0.725,0.709,0.789,0.697,0.727,0.78,0.758,0.766,0.749,0.639,0.737,0.664,0.634,0.606,0.619,ESM2 model (650M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
44,ESM-C (300M),Single sequence,0.727,0.008,0.73,0.68,0.724,0.708,0.793,0.696,0.725,0.788,0.766,0.767,0.743,0.634,0.736,0.666,0.629,0.603,0.621,ESM-C (300M params),"<a href='https://evolutionaryscale.ai/blog/esm-cambrian'>ESM Team. ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning. EvolutionaryScale Website, December 4, 2024</a>"
45,ESM-C (600M),Single sequence,0.726,0.008,0.728,0.668,0.729,0.707,0.797,0.691,0.73,0.784,0.762,0.768,0.753,0.63,0.736,0.661,0.627,0.597,0.615,ESM-C (600M params),"<a href='https://evolutionaryscale.ai/blog/esm-cambrian'>ESM Team. ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning. EvolutionaryScale Website, December 4, 2024</a>"
46,ESM2 (3B),Single sequence,0.724,0.007,0.724,0.675,0.722,0.712,0.786,0.692,0.736,0.767,0.749,0.765,0.753,0.655,0.733,0.645,0.619,0.605,0.621,ESM2 model (3B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
47,ESM-1v (ensemble),Single sequence,0.724,0.008,0.726,0.674,0.735,0.718,0.764,0.675,0.728,0.773,0.754,0.756,0.729,0.659,0.725,0.643,0.617,0.599,0.62,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
48,DeepSequence (single),MSA,0.723,0.007,0.741,0.68,0.704,0.722,0.768,0.704,0.735,0.757,0.745,0.763,0.742,0.68,0.721,0.664,0.666,0.685,0.734,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
49,SaProt (35M),Single sequence & Structure,0.722,0.005,0.702,0.702,0.736,0.664,0.807,0.683,0.712,0.775,0.748,0.764,0.73,0.626,0.729,0.72,0.631,0.621,0.667,SaProt (35M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
50,xTrimoPGLM-3B-MLM,Single sequence,0.722,0.008,0.718,0.67,0.716,0.728,0.775,0.691,0.738,0.765,0.744,0.749,0.747,0.701,0.726,0.647,0.624,0.589,0.599,"xTrimoPGLM (3B params, MLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
51,VESPAl,Single sequence,0.721,0.004,0.737,0.683,0.683,0.729,0.771,0.711,0.736,0.758,0.729,0.765,0.748,0.723,0.728,0.613,0.689,0.663,0.706,VESPAl model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
52,ESM2 (15B),Single sequence,0.72,0.006,0.717,0.668,0.725,0.715,0.775,0.69,0.736,0.758,0.742,0.759,0.742,0.676,0.727,0.639,0.638,0.611,0.64,ESM2 model (15B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
53,xTrimoPGLM-10B-MLM,Single sequence,0.719,0.007,0.716,0.659,0.721,0.724,0.775,0.685,0.733,0.769,0.74,0.747,0.747,0.699,0.724,0.636,0.613,0.585,0.602,"xTrimoPGLM (10B params, MLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
54,ESM-1b,Single sequence,0.719,0.007,0.73,0.662,0.72,0.699,0.782,0.707,0.722,0.767,0.747,0.771,0.742,0.642,0.715,0.642,0.614,0.596,0.661,ESM-1b (w/ Brandes et al. extensions),"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a>"
55,Progen2 XL,Single sequence,0.717,0.006,0.72,0.662,0.73,0.718,0.757,0.682,0.734,0.747,0.722,0.752,0.736,0.718,0.718,0.64,0.68,0.646,0.677,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
56,MIF-ST,Single sequence & Structure,0.717,0.006,0.709,0.674,0.738,0.703,0.762,0.701,0.72,0.745,0.721,0.724,0.743,0.715,0.737,0.674,0.704,0.681,0.71,MIF-ST model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
57,EVmutation,MSA,0.716,0.005,0.737,0.666,0.707,0.728,0.741,0.716,0.738,0.722,0.728,0.746,0.731,0.707,0.711,0.671,0.677,0.678,0.758,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
58,ESM2 (150M),Single sequence,0.714,0.008,0.71,0.685,0.718,0.677,0.779,0.687,0.701,0.767,0.754,0.761,0.716,0.582,0.718,0.657,0.601,0.602,0.639,ESM2 model (150M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
59,Progen2 M,Single sequence,0.711,0.005,0.713,0.666,0.735,0.716,0.726,0.675,0.721,0.734,0.735,0.73,0.7,0.681,0.71,0.61,0.605,0.59,0.595,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
60,Progen2 L,Single sequence,0.711,0.005,0.718,0.662,0.734,0.713,0.726,0.692,0.72,0.729,0.733,0.739,0.704,0.672,0.708,0.613,0.65,0.63,0.656,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
61,xTrimoPGLM-1B-MLM,Single sequence,0.711,0.007,0.71,0.665,0.72,0.706,0.751,0.685,0.71,0.756,0.743,0.744,0.723,0.628,0.711,0.633,0.621,0.605,0.629,"xTrimoPGLM (1B params, MLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
62,Progen2 Base,Single sequence,0.709,0.006,0.713,0.664,0.739,0.716,0.712,0.685,0.711,0.731,0.739,0.733,0.68,0.668,0.707,0.598,0.605,0.598,0.622,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
63,RITA XL,Single sequence,0.708,0.006,0.702,0.665,0.728,0.717,0.727,0.669,0.721,0.731,0.727,0.721,0.696,0.711,0.7,0.616,0.607,0.604,0.634,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
64,ESM-1v (single),Single sequence,0.707,0.008,0.713,0.652,0.723,0.705,0.744,0.663,0.71,0.76,0.738,0.74,0.714,0.642,0.71,0.622,0.619,0.595,0.614,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
65,Wavenet,MSA,0.707,0.007,0.709,0.673,0.691,0.706,0.757,0.654,0.724,0.753,0.723,0.737,0.73,0.685,0.704,0.646,0.656,0.636,0.671,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
66,Tranception L no retrieval,Single sequence,0.707,0.006,0.717,0.659,0.727,0.717,0.714,0.684,0.713,0.728,0.719,0.717,0.7,0.715,0.704,0.62,0.693,0.664,0.715,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
67,MIF,Structure,0.706,0.006,0.673,0.685,0.731,0.665,0.775,0.685,0.703,0.736,0.717,0.704,0.718,0.695,0.723,0.675,0.676,0.653,0.684,MIF model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
68,RITA L,Single sequence,0.704,0.006,0.698,0.66,0.729,0.713,0.718,0.672,0.715,0.722,0.727,0.72,0.678,0.704,0.697,0.611,0.597,0.591,0.623,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
69,xTrimoPGLM-100B-int4,Single sequence,0.702,0.007,0.703,0.638,0.708,0.701,0.761,0.677,0.721,0.742,0.709,0.75,0.737,0.696,0.701,0.629,0.679,0.652,0.708,"xTrimoPGLM (100B params, int4 quantized)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
70,CARP (640M),Single sequence,0.701,0.007,0.711,0.649,0.718,0.708,0.722,0.69,0.705,0.729,0.732,0.715,0.704,0.653,0.719,0.637,0.61,0.598,0.586,CARP model (640M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
71,Site-Independent,MSA,0.696,0.007,0.699,0.684,0.687,0.716,0.695,0.735,0.71,0.671,0.71,0.713,0.678,0.698,0.691,0.657,0.629,0.649,0.735,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
72,RITA M,Single sequence,0.694,0.006,0.691,0.649,0.72,0.712,0.698,0.664,0.702,0.716,0.716,0.702,0.669,0.699,0.688,0.593,0.594,0.6,0.636,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
73,Unirep evotuned,MSA,0.693,0.006,0.695,0.657,0.7,0.696,0.714,0.679,0.697,0.713,0.704,0.711,0.691,0.688,0.685,0.632,0.653,0.643,0.696,Unirep model w/ evotuning,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
74,Tranception M no retrieval,Single sequence,0.691,0.006,0.687,0.656,0.721,0.704,0.687,0.648,0.7,0.705,0.71,0.694,0.67,0.681,0.684,0.598,0.59,0.592,0.639,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
75,xTrimoPGLM-1B-CLM,Single sequence,0.689,0.005,0.686,0.667,0.72,0.69,0.685,0.67,0.689,0.7,0.715,0.695,0.666,0.644,0.678,0.621,0.59,0.582,0.595,"xTrimoPGLM (1B params, CLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
76,Progen2 S,Single sequence,0.686,0.006,0.68,0.652,0.71,0.69,0.695,0.669,0.68,0.714,0.718,0.698,0.66,0.643,0.684,0.584,0.578,0.575,0.609,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
77,CARP (76M),Single sequence,0.678,0.008,0.682,0.656,0.701,0.655,0.697,0.643,0.665,0.715,0.714,0.698,0.657,0.584,0.685,0.629,0.57,0.57,0.58,CARP model (76M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
78,ESM2 (35M),Single sequence,0.676,0.008,0.669,0.668,0.684,0.625,0.734,0.64,0.652,0.739,0.706,0.722,0.668,0.562,0.67,0.65,0.587,0.591,0.643,ESM2 model (35M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
79,MULAN,Single sequence & Structure,0.672,0.006,0.654,0.682,0.704,0.626,0.693,0.65,0.66,0.676,0.685,0.669,0.67,0.589,0.659,0.636,0.57,0.582,0.614,MULAN,"<a href='https://www.biorxiv.org/content/10.1101/2024.05.30.596565v1'>Daria Frolova, Daria Marina A. Pak, Anna Litvin, Ilya Sharov, Dmitry N. Ivankov, Ivan Oseledets. (2024). MULAN: Multimodal Protein Language Model for Sequence and Structure Encoding.</a>"
80,RITA S,Single sequence,0.668,0.006,0.659,0.652,0.684,0.682,0.66,0.635,0.672,0.679,0.687,0.659,0.632,0.682,0.657,0.584,0.581,0.581,0.611,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
81,Tranception S no retrieval,Single sequence,0.665,0.006,0.652,0.657,0.689,0.682,0.645,0.629,0.669,0.671,0.678,0.648,0.645,0.661,0.653,0.588,0.584,0.587,0.615,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
82,CARP (38M),Single sequence,0.653,0.008,0.655,0.652,0.671,0.625,0.661,0.618,0.633,0.687,0.679,0.664,0.631,0.567,0.655,0.612,0.569,0.577,0.59,CARP model (38M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
83,ProteinMPNN,Structure,0.639,0.006,0.605,0.591,0.603,0.592,0.805,0.606,0.65,0.734,0.658,0.71,0.687,0.64,0.655,0.673,0.606,0.617,0.698,ProteinMPNN model,"<a href='https://www.science.org/doi/10.1126/science.add2187'>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.</a>"
84,xTrimoPGLM-3B-CLM,Single sequence,0.625,0.007,0.616,0.625,0.647,0.632,0.607,0.612,0.616,0.633,0.658,0.624,0.566,0.599,0.612,0.575,0.495,0.506,0.511,"xTrimoPGLM (3B params, CLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
85,ESM2 (8M),Single sequence,0.624,0.008,0.607,0.65,0.647,0.579,0.638,0.614,0.6,0.636,0.635,0.635,0.604,0.546,0.609,0.595,0.578,0.578,0.616,ESM2 model (8M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
86,Unirep,Single sequence,0.605,0.009,0.597,0.617,0.621,0.579,0.61,0.604,0.59,0.605,0.622,0.62,0.579,0.532,0.594,0.558,0.572,0.588,0.608,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
87,ProtGPT2,Single sequence,0.603,0.007,0.593,0.585,0.602,0.596,0.639,0.601,0.597,0.637,0.636,0.631,0.571,0.576,0.601,0.603,0.53,0.509,0.544,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
88,CARP (600K),Single sequence,0.557,0.01,0.555,0.551,0.596,0.533,0.549,0.555,0.544,0.548,0.568,0.539,0.533,0.526,0.554,0.535,0.534,0.551,0.55,CARP model (600K params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
89,xTrimoPGLM-7B-CLM,Single sequence,0.556,0.008,0.56,0.591,0.541,0.537,0.555,0.544,0.544,0.56,0.571,0.562,0.526,0.507,0.547,0.535,0.508,0.524,0.539,"xTrimoPGLM (7B params, CLM)","<a href='https://www.nature.com/articles/s41592-025-02636-z'>Chen, B., Cheng, X., Li, P., Geng, Y., Gong, J., Li, S., Bei, Z., Tan, X., Wang, B., Zeng, X., Liu, C., Zeng, A., Dong, Y., Tang, J., & Song, L. (2025). xTrimoPGLM: unified 100-billion-parameter pretrained transformer for deciphering the language of proteins. Nature methods.</a>"
