Model_rank,Model_name,Model type,Average_Top_recall,Bootstrap_standard_error_Top_recall,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,ProSST (K=2048),Hybrid - Structure & PLM,0.236,0.0,0.182,0.232,0.226,0.174,0.366,0.214,0.237,0.295,0.248,0.278,0.27,0.203,0.221,0.277,0.17,0.168,0.242,ProSST (K=2048),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
2,SaProt (650M),Hybrid - Structure & PLM,0.232,0.007,0.191,0.233,0.235,0.171,0.331,0.191,0.232,0.279,0.23,0.269,0.268,0.196,0.208,0.26,0.199,0.196,0.268,SaProt (650M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
3,ProSST (K=4096),Hybrid - Structure & PLM,0.232,0.006,0.176,0.256,0.201,0.195,0.331,0.236,0.23,0.277,0.235,0.267,0.264,0.219,0.214,0.267,0.204,0.201,0.291,ProSST (K=4096),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
4,TranceptEVE S,Hybrid - Alignment & PLM,0.232,0.007,0.198,0.223,0.226,0.226,0.286,0.231,0.242,0.252,0.226,0.27,0.228,0.286,0.211,0.256,0.209,0.205,0.271,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
5,ProSST (K=1024),Hybrid - Structure & PLM,0.23,0.005,0.177,0.239,0.217,0.189,0.328,0.237,0.226,0.277,0.227,0.26,0.276,0.216,0.214,0.269,0.2,0.198,0.286,ProSST (K=1024),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
6,TranceptEVE M,Hybrid - Alignment & PLM,0.23,0.007,0.198,0.218,0.227,0.227,0.28,0.23,0.242,0.246,0.224,0.271,0.224,0.286,0.209,0.251,0.211,0.214,0.279,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
7,EVE (single),Alignment-based model,0.23,0.007,0.199,0.217,0.223,0.225,0.284,0.226,0.241,0.251,0.225,0.27,0.231,0.281,0.208,0.256,0.228,0.213,0.271,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
8,TranceptEVE L,Hybrid - Alignment & PLM,0.229,0.008,0.2,0.218,0.224,0.226,0.279,0.227,0.241,0.248,0.222,0.271,0.229,0.283,0.209,0.255,0.217,0.203,0.271,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
9,EVE (ensemble),Alignment-based model,0.229,0.007,0.199,0.214,0.225,0.225,0.283,0.227,0.241,0.25,0.225,0.271,0.228,0.281,0.209,0.256,0.234,0.218,0.264,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
10,ProSST (K=128),Hybrid - Structure & PLM,0.227,0.006,0.165,0.266,0.21,0.184,0.308,0.236,0.217,0.258,0.221,0.25,0.254,0.211,0.206,0.271,0.208,0.196,0.289,ProSST (K=128),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
11,ProtSSN (ensemble),Hybrid - Structure & PLM,0.226,0.007,0.191,0.196,0.22,0.187,0.337,0.192,0.239,0.281,0.228,0.278,0.27,0.215,0.215,0.246,0.202,0.18,0.22,ProtSSN (ensemble of 9 models),"<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
12,"ProtSSN (k=30, h=768)",Hybrid - Structure & PLM,0.226,0.006,0.19,0.202,0.228,0.182,0.329,0.193,0.233,0.279,0.227,0.275,0.265,0.208,0.209,0.239,0.19,0.178,0.216,"ProtSSN (k=30, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
13,PoET (200M),Hybrid - Alignment & PLM,0.226,0.008,0.201,0.224,0.2,0.23,0.276,0.229,0.24,0.248,0.215,0.272,0.232,0.291,0.209,0.261,0.231,0.232,0.305,PoET (200M),"<a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'>Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.</a>"
14,DeepSequence (ensemble),Alignment-based model,0.225,0.008,0.194,0.213,0.218,0.219,0.282,0.216,0.235,0.252,0.223,0.27,0.225,0.267,0.204,0.255,0.208,0.196,0.266,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
15,MIF-ST,Hybrid - Structure & PLM,0.225,0.008,0.191,0.223,0.221,0.191,0.3,0.194,0.232,0.26,0.215,0.258,0.263,0.226,0.206,0.228,0.213,0.187,0.266,MIF-ST model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
16,"ProtSSN (k=20, h=1280)",Hybrid - Structure & PLM,0.225,0.007,0.187,0.2,0.215,0.187,0.335,0.19,0.236,0.281,0.228,0.28,0.262,0.213,0.21,0.25,0.2,0.184,0.229,"ProtSSN (k=20, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
17,DeepSequence (single),Alignment-based model,0.225,0.008,0.199,0.216,0.214,0.216,0.28,0.218,0.235,0.25,0.225,0.271,0.222,0.257,0.205,0.253,0.189,0.193,0.26,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
18,"ProtSSN (k=10, h=1280)",Hybrid - Structure & PLM,0.225,0.007,0.187,0.195,0.22,0.186,0.335,0.189,0.237,0.279,0.229,0.27,0.265,0.214,0.212,0.23,0.178,0.161,0.203,"ProtSSN (k=10, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
19,"ProtSSN (k=10, h=768)",Hybrid - Structure & PLM,0.224,0.007,0.193,0.195,0.22,0.184,0.33,0.186,0.239,0.275,0.231,0.27,0.266,0.203,0.209,0.231,0.195,0.16,0.202,"ProtSSN (k=10, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
20,"ProtSSN (k=20, h=512)",Hybrid - Structure & PLM,0.224,0.007,0.196,0.196,0.212,0.183,0.331,0.188,0.237,0.276,0.226,0.276,0.267,0.208,0.208,0.245,0.203,0.187,0.23,"ProtSSN (k=20, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
21,MSA Transformer (ensemble),Hybrid - Alignment & PLM,0.223,0.009,0.2,0.204,0.218,0.217,0.278,0.227,0.232,0.249,0.216,0.264,0.23,0.275,0.204,0.229,0.211,0.227,0.267,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
22,"ProtSSN (k=20, h=768)",Hybrid - Structure & PLM,0.223,0.007,0.189,0.19,0.218,0.187,0.332,0.18,0.235,0.281,0.227,0.27,0.264,0.213,0.21,0.252,0.183,0.181,0.251,"ProtSSN (k=20, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
23,"ProtSSN (k=30, h=512)",Hybrid - Structure & PLM,0.223,0.007,0.189,0.2,0.218,0.181,0.327,0.183,0.234,0.275,0.223,0.274,0.262,0.205,0.206,0.241,0.183,0.169,0.206,"ProtSSN (k=30, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
24,ESM-IF1,Inverse folding model,0.222,0.008,0.184,0.208,0.205,0.171,0.344,0.181,0.228,0.29,0.224,0.279,0.27,0.194,0.211,0.272,0.186,0.181,0.275,ESM-IF1 model,"<a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.</a>"
25,"ProtSSN (k=30, h=1280)",Hybrid - Structure & PLM,0.222,0.007,0.187,0.199,0.218,0.186,0.321,0.195,0.229,0.274,0.225,0.266,0.261,0.207,0.209,0.246,0.185,0.175,0.218,"ProtSSN (k=30, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
26,"ProtSSN (k=10, h=512)",Hybrid - Structure & PLM,0.222,0.007,0.188,0.198,0.216,0.182,0.325,0.192,0.23,0.274,0.224,0.273,0.265,0.195,0.205,0.242,0.18,0.16,0.199,"ProtSSN (k=10, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
27,ProSST (K=512),Hybrid - Structure & PLM,0.222,0.005,0.16,0.241,0.214,0.188,0.305,0.218,0.212,0.268,0.226,0.24,0.253,0.199,0.205,0.258,0.206,0.192,0.271,ProSST (K=512),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
28,EVmutation,Alignment-based model,0.221,0.008,0.201,0.204,0.209,0.22,0.272,0.213,0.235,0.246,0.212,0.262,0.232,0.278,0.2,0.254,0.219,0.215,0.283,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
29,Tranception L,Hybrid - Alignment & PLM,0.22,0.008,0.2,0.205,0.21,0.219,0.265,0.226,0.232,0.237,0.216,0.259,0.219,0.271,0.201,0.25,0.198,0.202,0.272,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
30,Tranception M,Hybrid - Alignment & PLM,0.218,0.008,0.188,0.208,0.211,0.212,0.271,0.227,0.228,0.234,0.218,0.26,0.204,0.27,0.201,0.247,0.173,0.2,0.282,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
31,Tranception S,Hybrid - Alignment & PLM,0.217,0.007,0.186,0.215,0.208,0.204,0.271,0.222,0.22,0.24,0.214,0.256,0.21,0.257,0.198,0.243,0.173,0.194,0.267,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
32,MIF,Inverse folding model,0.216,0.008,0.178,0.201,0.214,0.162,0.327,0.194,0.218,0.266,0.221,0.25,0.248,0.206,0.211,0.246,0.199,0.168,0.241,MIF model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
33,ESM2 (650M),Protein language model,0.216,0.007,0.178,0.213,0.215,0.175,0.3,0.178,0.213,0.268,0.221,0.257,0.248,0.168,0.197,0.232,0.171,0.145,0.167,ESM2 model (650M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
34,ESM2 (3B),Protein language model,0.213,0.007,0.176,0.207,0.212,0.182,0.286,0.187,0.215,0.253,0.213,0.256,0.244,0.18,0.193,0.23,0.169,0.162,0.167,ESM2 model (3B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
35,MSA Transformer (single),Hybrid - Alignment & PLM,0.212,0.009,0.19,0.194,0.201,0.217,0.259,0.214,0.225,0.235,0.206,0.258,0.218,0.26,0.192,0.213,0.218,0.227,0.269,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
36,SaProt (35M),Hybrid - Structure & PLM,0.211,0.006,0.168,0.198,0.225,0.154,0.311,0.172,0.209,0.263,0.218,0.261,0.24,0.147,0.205,0.274,0.161,0.171,0.196,SaProt (35M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
37,GEMME,Alignment-based model,0.211,0.009,0.196,0.198,0.198,0.221,0.24,0.213,0.228,0.22,0.202,0.231,0.215,0.284,0.186,0.225,0.179,0.207,0.281,GEMME model,"<a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a>"
38,ESM-1v (ensemble),Protein language model,0.21,0.008,0.173,0.213,0.21,0.19,0.265,0.182,0.21,0.249,0.209,0.236,0.23,0.203,0.193,0.211,0.159,0.144,0.159,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
39,ProSST (K=20),Hybrid - Structure & PLM,0.21,0.005,0.162,0.243,0.187,0.175,0.28,0.224,0.199,0.241,0.207,0.223,0.236,0.203,0.2,0.236,0.19,0.179,0.26,ProSST (K=20),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
40,ESM2 (15B),Protein language model,0.207,0.008,0.18,0.196,0.199,0.193,0.27,0.184,0.217,0.242,0.209,0.242,0.239,0.192,0.189,0.219,0.155,0.15,0.168,ESM2 model (15B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
41,CARP (640M),Protein language model,0.207,0.007,0.176,0.206,0.212,0.178,0.265,0.174,0.209,0.24,0.215,0.231,0.222,0.18,0.189,0.214,0.159,0.148,0.173,CARP model (640M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
42,VespaG,Protein language model,0.205,0.008,0.186,0.196,0.19,0.202,0.251,0.192,0.221,0.224,0.197,0.243,0.219,0.238,0.177,0.228,0.191,0.215,0.255,VespaG model,"<a href='https://doi.org/10.1101/2024.04.24.590982'>Marquet, C., Schlensok, J., Abakarova, M., Rost, B., & Laine, E. (2024). Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction. bioRxiv.</a>"
43,ESM2 (150M),Protein language model,0.204,0.007,0.168,0.204,0.196,0.153,0.299,0.162,0.195,0.267,0.222,0.249,0.226,0.129,0.188,0.234,0.14,0.148,0.193,ESM2 model (150M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
44,Wavenet,Alignment-based model,0.203,0.008,0.167,0.204,0.19,0.201,0.252,0.175,0.215,0.234,0.203,0.219,0.211,0.243,0.182,0.236,0.166,0.153,0.188,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
45,ESM-1b,Protein language model,0.202,0.008,0.18,0.178,0.197,0.173,0.282,0.161,0.214,0.248,0.209,0.241,0.243,0.169,0.183,0.213,0.159,0.144,0.186,ESM-1b (w/ Brandes et al. extensions),"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a>"
46,Progen2 M,Protein language model,0.202,0.008,0.172,0.19,0.221,0.197,0.229,0.177,0.21,0.217,0.204,0.202,0.205,0.224,0.185,0.19,0.147,0.129,0.134,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
47,Progen2 L,Protein language model,0.201,0.008,0.185,0.175,0.223,0.2,0.224,0.193,0.208,0.214,0.204,0.203,0.216,0.214,0.184,0.193,0.173,0.152,0.178,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
48,Site-Independent,Alignment-based model,0.201,0.007,0.17,0.198,0.179,0.198,0.261,0.209,0.207,0.226,0.202,0.246,0.194,0.245,0.192,0.247,0.18,0.194,0.272,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
49,VESPA,Protein language model,0.2,0.008,0.18,0.192,0.184,0.204,0.242,0.192,0.218,0.217,0.19,0.231,0.218,0.247,0.176,0.206,0.201,0.205,0.246,VESPA model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
50,Progen2 XL,Protein language model,0.198,0.009,0.181,0.179,0.184,0.211,0.237,0.194,0.221,0.212,0.185,0.231,0.226,0.251,0.18,0.215,0.189,0.157,0.198,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
51,Progen2 Base,Protein language model,0.198,0.008,0.183,0.182,0.205,0.199,0.219,0.19,0.206,0.212,0.205,0.195,0.203,0.218,0.184,0.19,0.154,0.14,0.151,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
52,Tranception L no retrieval,Protein language model,0.197,0.009,0.181,0.185,0.191,0.21,0.216,0.199,0.209,0.205,0.191,0.201,0.208,0.253,0.182,0.208,0.193,0.166,0.232,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
53,RITA L,Protein language model,0.195,0.009,0.173,0.178,0.203,0.205,0.215,0.181,0.208,0.205,0.199,0.185,0.196,0.246,0.178,0.195,0.135,0.14,0.147,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
54,ESM-1v (single),Protein language model,0.194,0.008,0.166,0.175,0.204,0.179,0.245,0.151,0.196,0.239,0.195,0.226,0.214,0.178,0.182,0.199,0.15,0.14,0.15,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
55,RITA XL,Protein language model,0.193,0.008,0.17,0.173,0.202,0.203,0.216,0.178,0.212,0.198,0.194,0.188,0.201,0.243,0.176,0.202,0.14,0.143,0.174,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
56,CARP (76M),Protein language model,0.188,0.007,0.155,0.185,0.195,0.154,0.254,0.151,0.181,0.235,0.197,0.22,0.202,0.145,0.178,0.199,0.141,0.147,0.183,CARP model (76M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
57,RITA M,Protein language model,0.188,0.009,0.168,0.163,0.199,0.201,0.207,0.179,0.194,0.207,0.194,0.177,0.182,0.25,0.176,0.185,0.137,0.148,0.18,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
58,ProteinMPNN,Inverse folding model,0.186,0.007,0.149,0.146,0.152,0.14,0.342,0.146,0.201,0.272,0.196,0.267,0.229,0.189,0.186,0.244,0.178,0.143,0.223,ProteinMPNN model,"<a href='https://www.science.org/doi/10.1126/science.add2187'>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.</a>"
59,ESM2 (35M),Protein language model,0.186,0.007,0.155,0.197,0.181,0.141,0.254,0.153,0.163,0.248,0.197,0.219,0.197,0.124,0.175,0.224,0.138,0.15,0.206,ESM2 model (35M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
60,Tranception M no retrieval,Protein language model,0.185,0.009,0.163,0.176,0.19,0.201,0.194,0.178,0.194,0.191,0.188,0.174,0.18,0.234,0.175,0.182,0.138,0.151,0.181,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
61,VESPAl,Protein language model,0.184,0.008,0.172,0.183,0.159,0.192,0.214,0.181,0.197,0.199,0.176,0.216,0.193,0.225,0.155,0.188,0.185,0.193,0.243,VESPAl model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
62,Progen2 S,Protein language model,0.183,0.009,0.155,0.166,0.204,0.173,0.218,0.144,0.184,0.213,0.196,0.177,0.193,0.17,0.169,0.178,0.135,0.141,0.147,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
63,Unirep evotuned,Hybrid - Alignment & PLM,0.18,0.009,0.163,0.174,0.186,0.187,0.192,0.19,0.188,0.178,0.177,0.182,0.175,0.227,0.158,0.175,0.152,0.15,0.207,Unirep model w/ evotuning,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
64,MULAN_small,Protein language model,0.18,0.008,0.137,0.222,0.193,0.143,0.204,0.172,0.166,0.184,0.18,0.169,0.184,0.137,0.169,0.185,0.108,0.138,0.138,,
65,RITA S,Protein language model,0.178,0.008,0.155,0.169,0.194,0.186,0.186,0.177,0.174,0.192,0.181,0.163,0.166,0.226,0.166,0.168,0.119,0.128,0.151,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
66,CARP (38M),Protein language model,0.177,0.007,0.147,0.188,0.177,0.141,0.233,0.148,0.165,0.219,0.185,0.198,0.183,0.141,0.166,0.188,0.141,0.15,0.177,CARP model (38M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
67,Tranception S no retrieval,Protein language model,0.177,0.008,0.154,0.188,0.182,0.185,0.177,0.173,0.174,0.185,0.174,0.159,0.173,0.215,0.167,0.165,0.128,0.134,0.146,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
68,ESM2 (8M),Protein language model,0.153,0.007,0.126,0.178,0.162,0.118,0.181,0.141,0.136,0.174,0.155,0.155,0.152,0.12,0.145,0.178,0.129,0.128,0.163,ESM2 model (8M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
69,Unirep,Protein language model,0.139,0.008,0.116,0.138,0.141,0.124,0.176,0.145,0.129,0.166,0.148,0.158,0.145,0.113,0.138,0.157,0.111,0.128,0.149,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
70,ProtGPT2,Protein language model,0.133,0.008,0.113,0.138,0.126,0.122,0.168,0.122,0.136,0.148,0.144,0.156,0.116,0.133,0.125,0.175,0.115,0.116,0.123,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
71,CARP (600K),Protein language model,0.131,0.008,0.116,0.124,0.132,0.106,0.176,0.13,0.122,0.157,0.14,0.148,0.131,0.115,0.132,0.143,0.107,0.12,0.127,CARP model (600K params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
