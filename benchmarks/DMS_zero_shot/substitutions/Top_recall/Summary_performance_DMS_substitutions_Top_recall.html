<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model_name</th>
      <th>Model type</th>
      <th>Average_Top_recall</th>
      <th>Bootstrap_standard_error_Top_recall</th>
      <th>Function_Activity</th>
      <th>Function_Binding</th>
      <th>Function_Expression</th>
      <th>Function_OrganismalFitness</th>
      <th>Function_Stability</th>
      <th>Low_MSA_depth</th>
      <th>Medium_MSA_depth</th>
      <th>High_MSA_depth</th>
      <th>Taxa_Human</th>
      <th>Taxa_Other_Eukaryote</th>
      <th>Taxa_Prokaryote</th>
      <th>Taxa_Virus</th>
      <th>Depth_1</th>
      <th>Depth_2</th>
      <th>Depth_3</th>
      <th>Depth_4</th>
      <th>Depth_5+</th>
      <th>Model details</th>
      <th>References</th>
    </tr>
    <tr>
      <th>Model_rank</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>ProSST (K=2048)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.236</td>
      <td>0.000</td>
      <td>0.182</td>
      <td>0.232</td>
      <td>0.226</td>
      <td>0.174</td>
      <td>0.366</td>
      <td>0.214</td>
      <td>0.237</td>
      <td>0.295</td>
      <td>0.248</td>
      <td>0.278</td>
      <td>0.270</td>
      <td>0.203</td>
      <td>0.221</td>
      <td>0.277</td>
      <td>0.170</td>
      <td>0.168</td>
      <td>0.242</td>
      <td>ProSST (K=2048)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'&gt;Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SaProt (650M)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.232</td>
      <td>0.007</td>
      <td>0.191</td>
      <td>0.233</td>
      <td>0.235</td>
      <td>0.171</td>
      <td>0.331</td>
      <td>0.191</td>
      <td>0.232</td>
      <td>0.279</td>
      <td>0.230</td>
      <td>0.269</td>
      <td>0.268</td>
      <td>0.196</td>
      <td>0.208</td>
      <td>0.260</td>
      <td>0.199</td>
      <td>0.196</td>
      <td>0.268</td>
      <td>SaProt (650M)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'&gt;Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ProSST (K=4096)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.232</td>
      <td>0.006</td>
      <td>0.176</td>
      <td>0.256</td>
      <td>0.201</td>
      <td>0.195</td>
      <td>0.331</td>
      <td>0.236</td>
      <td>0.230</td>
      <td>0.277</td>
      <td>0.235</td>
      <td>0.267</td>
      <td>0.264</td>
      <td>0.219</td>
      <td>0.214</td>
      <td>0.267</td>
      <td>0.204</td>
      <td>0.201</td>
      <td>0.291</td>
      <td>ProSST (K=4096)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'&gt;Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>4</th>
      <td>TranceptEVE S</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.232</td>
      <td>0.007</td>
      <td>0.198</td>
      <td>0.223</td>
      <td>0.226</td>
      <td>0.226</td>
      <td>0.286</td>
      <td>0.231</td>
      <td>0.242</td>
      <td>0.252</td>
      <td>0.226</td>
      <td>0.270</td>
      <td>0.228</td>
      <td>0.286</td>
      <td>0.211</td>
      <td>0.256</td>
      <td>0.209</td>
      <td>0.205</td>
      <td>0.271</td>
      <td>TranceptEVE Small model (Tranception Small &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ProSST (K=1024)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.230</td>
      <td>0.005</td>
      <td>0.177</td>
      <td>0.239</td>
      <td>0.217</td>
      <td>0.189</td>
      <td>0.328</td>
      <td>0.237</td>
      <td>0.226</td>
      <td>0.277</td>
      <td>0.227</td>
      <td>0.260</td>
      <td>0.276</td>
      <td>0.216</td>
      <td>0.214</td>
      <td>0.269</td>
      <td>0.200</td>
      <td>0.198</td>
      <td>0.286</td>
      <td>ProSST (K=1024)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'&gt;Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>6</th>
      <td>TranceptEVE M</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.230</td>
      <td>0.007</td>
      <td>0.198</td>
      <td>0.218</td>
      <td>0.227</td>
      <td>0.227</td>
      <td>0.280</td>
      <td>0.230</td>
      <td>0.242</td>
      <td>0.246</td>
      <td>0.224</td>
      <td>0.271</td>
      <td>0.224</td>
      <td>0.286</td>
      <td>0.209</td>
      <td>0.251</td>
      <td>0.211</td>
      <td>0.214</td>
      <td>0.279</td>
      <td>TranceptEVE Medium model (Tranception Medium &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'&gt;Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. &amp; Marks, D.S. &amp;  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>7</th>
      <td>EVE (single)</td>
      <td>Alignment-based model</td>
      <td>0.230</td>
      <td>0.007</td>
      <td>0.199</td>
      <td>0.217</td>
      <td>0.223</td>
      <td>0.225</td>
      <td>0.284</td>
      <td>0.226</td>
      <td>0.241</td>
      <td>0.251</td>
      <td>0.225</td>
      <td>0.270</td>
      <td>0.231</td>
      <td>0.281</td>
      <td>0.208</td>
      <td>0.256</td>
      <td>0.228</td>
      <td>0.213</td>
      <td>0.271</td>
      <td>EVE model (single seed)</td>
      <td>&lt;a href='https://www.nature.com/articles/s41586-021-04043-8'&gt;Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., &amp; Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>8</th>
      <td>TranceptEVE L</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.229</td>
      <td>0.008</td>
      <td>0.200</td>
      <td>0.218</td>
      <td>0.224</td>
      <td>0.226</td>
      <td>0.279</td>
      <td>0.227</td>
      <td>0.241</td>
      <td>0.248</td>
      <td>0.222</td>
      <td>0.271</td>
      <td>0.229</td>
      <td>0.283</td>
      <td>0.209</td>
      <td>0.255</td>
      <td>0.217</td>
      <td>0.203</td>
      <td>0.271</td>
      <td>TranceptEVE Large model (Tranception Large &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'&gt;Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. &amp; Marks, D.S. &amp;  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>9</th>
      <td>EVE (ensemble)</td>
      <td>Alignment-based model</td>
      <td>0.229</td>
      <td>0.007</td>
      <td>0.199</td>
      <td>0.214</td>
      <td>0.225</td>
      <td>0.225</td>
      <td>0.283</td>
      <td>0.227</td>
      <td>0.241</td>
      <td>0.250</td>
      <td>0.225</td>
      <td>0.271</td>
      <td>0.228</td>
      <td>0.281</td>
      <td>0.209</td>
      <td>0.256</td>
      <td>0.234</td>
      <td>0.218</td>
      <td>0.264</td>
      <td>EVE model (ensemble of 5 independently-trained models)</td>
      <td>&lt;a href='https://www.nature.com/articles/s41586-021-04043-8'&gt;Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., &amp; Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>10</th>
      <td>ProSST (K=128)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.227</td>
      <td>0.006</td>
      <td>0.165</td>
      <td>0.266</td>
      <td>0.210</td>
      <td>0.184</td>
      <td>0.308</td>
      <td>0.236</td>
      <td>0.217</td>
      <td>0.258</td>
      <td>0.221</td>
      <td>0.250</td>
      <td>0.254</td>
      <td>0.211</td>
      <td>0.206</td>
      <td>0.271</td>
      <td>0.208</td>
      <td>0.196</td>
      <td>0.289</td>
      <td>ProSST (K=128)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'&gt;Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>11</th>
      <td>ProtSSN (ensemble)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.226</td>
      <td>0.007</td>
      <td>0.191</td>
      <td>0.196</td>
      <td>0.220</td>
      <td>0.187</td>
      <td>0.337</td>
      <td>0.192</td>
      <td>0.239</td>
      <td>0.281</td>
      <td>0.228</td>
      <td>0.278</td>
      <td>0.270</td>
      <td>0.215</td>
      <td>0.215</td>
      <td>0.246</td>
      <td>0.202</td>
      <td>0.180</td>
      <td>0.220</td>
      <td>ProtSSN (ensemble of 9 models)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>12</th>
      <td>ProtSSN (k=30, h=768)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.226</td>
      <td>0.006</td>
      <td>0.190</td>
      <td>0.202</td>
      <td>0.228</td>
      <td>0.182</td>
      <td>0.329</td>
      <td>0.193</td>
      <td>0.233</td>
      <td>0.279</td>
      <td>0.227</td>
      <td>0.275</td>
      <td>0.265</td>
      <td>0.208</td>
      <td>0.209</td>
      <td>0.239</td>
      <td>0.190</td>
      <td>0.178</td>
      <td>0.216</td>
      <td>ProtSSN (k=30, h=768)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>13</th>
      <td>PoET (200M)</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.226</td>
      <td>0.008</td>
      <td>0.201</td>
      <td>0.224</td>
      <td>0.200</td>
      <td>0.230</td>
      <td>0.276</td>
      <td>0.229</td>
      <td>0.240</td>
      <td>0.248</td>
      <td>0.215</td>
      <td>0.272</td>
      <td>0.232</td>
      <td>0.291</td>
      <td>0.209</td>
      <td>0.261</td>
      <td>0.231</td>
      <td>0.232</td>
      <td>0.305</td>
      <td>PoET (200M)</td>
      <td>&lt;a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'&gt;Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>14</th>
      <td>DeepSequence (ensemble)</td>
      <td>Alignment-based model</td>
      <td>0.225</td>
      <td>0.008</td>
      <td>0.194</td>
      <td>0.213</td>
      <td>0.218</td>
      <td>0.219</td>
      <td>0.282</td>
      <td>0.216</td>
      <td>0.235</td>
      <td>0.252</td>
      <td>0.223</td>
      <td>0.270</td>
      <td>0.225</td>
      <td>0.267</td>
      <td>0.204</td>
      <td>0.255</td>
      <td>0.208</td>
      <td>0.196</td>
      <td>0.266</td>
      <td>DeepSequence model (ensemble of 5 independently-trained models)</td>
      <td>&lt;a href='https://www.nature.com/articles/s41592-018-0138-4'&gt;Riesselman, A.J., Ingraham, J., &amp; Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>15</th>
      <td>MIF-ST</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.225</td>
      <td>0.008</td>
      <td>0.191</td>
      <td>0.223</td>
      <td>0.221</td>
      <td>0.191</td>
      <td>0.300</td>
      <td>0.194</td>
      <td>0.232</td>
      <td>0.260</td>
      <td>0.215</td>
      <td>0.258</td>
      <td>0.263</td>
      <td>0.226</td>
      <td>0.206</td>
      <td>0.228</td>
      <td>0.213</td>
      <td>0.187</td>
      <td>0.266</td>
      <td>MIF-ST model</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'&gt;Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>16</th>
      <td>ProtSSN (k=20, h=1280)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.225</td>
      <td>0.007</td>
      <td>0.187</td>
      <td>0.200</td>
      <td>0.215</td>
      <td>0.187</td>
      <td>0.335</td>
      <td>0.190</td>
      <td>0.236</td>
      <td>0.281</td>
      <td>0.228</td>
      <td>0.280</td>
      <td>0.262</td>
      <td>0.213</td>
      <td>0.210</td>
      <td>0.250</td>
      <td>0.200</td>
      <td>0.184</td>
      <td>0.229</td>
      <td>ProtSSN (k=20, h=1280)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>17</th>
      <td>DeepSequence (single)</td>
      <td>Alignment-based model</td>
      <td>0.225</td>
      <td>0.008</td>
      <td>0.199</td>
      <td>0.216</td>
      <td>0.214</td>
      <td>0.216</td>
      <td>0.280</td>
      <td>0.218</td>
      <td>0.235</td>
      <td>0.250</td>
      <td>0.225</td>
      <td>0.271</td>
      <td>0.222</td>
      <td>0.257</td>
      <td>0.205</td>
      <td>0.253</td>
      <td>0.189</td>
      <td>0.193</td>
      <td>0.260</td>
      <td>DeepSequence model (single seed)</td>
      <td>&lt;a href='https://www.nature.com/articles/s41592-018-0138-4'&gt;Riesselman, A.J., Ingraham, J., &amp; Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>18</th>
      <td>ProtSSN (k=10, h=1280)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.225</td>
      <td>0.007</td>
      <td>0.187</td>
      <td>0.195</td>
      <td>0.220</td>
      <td>0.186</td>
      <td>0.335</td>
      <td>0.189</td>
      <td>0.237</td>
      <td>0.279</td>
      <td>0.229</td>
      <td>0.270</td>
      <td>0.265</td>
      <td>0.214</td>
      <td>0.212</td>
      <td>0.230</td>
      <td>0.178</td>
      <td>0.161</td>
      <td>0.203</td>
      <td>ProtSSN (k=10, h=1280)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>19</th>
      <td>ProtSSN (k=10, h=768)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.224</td>
      <td>0.007</td>
      <td>0.193</td>
      <td>0.195</td>
      <td>0.220</td>
      <td>0.184</td>
      <td>0.330</td>
      <td>0.186</td>
      <td>0.239</td>
      <td>0.275</td>
      <td>0.231</td>
      <td>0.270</td>
      <td>0.266</td>
      <td>0.203</td>
      <td>0.209</td>
      <td>0.231</td>
      <td>0.195</td>
      <td>0.160</td>
      <td>0.202</td>
      <td>ProtSSN (k=10, h=768)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>20</th>
      <td>ProtSSN (k=20, h=512)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.224</td>
      <td>0.007</td>
      <td>0.196</td>
      <td>0.196</td>
      <td>0.212</td>
      <td>0.183</td>
      <td>0.331</td>
      <td>0.188</td>
      <td>0.237</td>
      <td>0.276</td>
      <td>0.226</td>
      <td>0.276</td>
      <td>0.267</td>
      <td>0.208</td>
      <td>0.208</td>
      <td>0.245</td>
      <td>0.203</td>
      <td>0.187</td>
      <td>0.230</td>
      <td>ProtSSN (k=20, h=512)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>21</th>
      <td>MSA Transformer (ensemble)</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.223</td>
      <td>0.009</td>
      <td>0.200</td>
      <td>0.204</td>
      <td>0.218</td>
      <td>0.217</td>
      <td>0.278</td>
      <td>0.227</td>
      <td>0.232</td>
      <td>0.249</td>
      <td>0.216</td>
      <td>0.264</td>
      <td>0.230</td>
      <td>0.275</td>
      <td>0.204</td>
      <td>0.229</td>
      <td>0.211</td>
      <td>0.227</td>
      <td>0.267</td>
      <td>MSA Transformer (ensemble of 5 MSA samples)</td>
      <td>&lt;a href='http://proceedings.mlr.press/v139/rao21a.html'&gt;Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., &amp; Rives, A. (2021). MSA Transformer. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>22</th>
      <td>ProtSSN (k=20, h=768)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.223</td>
      <td>0.007</td>
      <td>0.189</td>
      <td>0.190</td>
      <td>0.218</td>
      <td>0.187</td>
      <td>0.332</td>
      <td>0.180</td>
      <td>0.235</td>
      <td>0.281</td>
      <td>0.227</td>
      <td>0.270</td>
      <td>0.264</td>
      <td>0.213</td>
      <td>0.210</td>
      <td>0.252</td>
      <td>0.183</td>
      <td>0.181</td>
      <td>0.251</td>
      <td>ProtSSN (k=20, h=768)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>23</th>
      <td>ProtSSN (k=30, h=512)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.223</td>
      <td>0.007</td>
      <td>0.189</td>
      <td>0.200</td>
      <td>0.218</td>
      <td>0.181</td>
      <td>0.327</td>
      <td>0.183</td>
      <td>0.234</td>
      <td>0.275</td>
      <td>0.223</td>
      <td>0.274</td>
      <td>0.262</td>
      <td>0.205</td>
      <td>0.206</td>
      <td>0.241</td>
      <td>0.183</td>
      <td>0.169</td>
      <td>0.206</td>
      <td>ProtSSN (k=30, h=512)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>24</th>
      <td>ESM-IF1</td>
      <td>Inverse folding model</td>
      <td>0.222</td>
      <td>0.008</td>
      <td>0.184</td>
      <td>0.208</td>
      <td>0.205</td>
      <td>0.171</td>
      <td>0.344</td>
      <td>0.181</td>
      <td>0.228</td>
      <td>0.290</td>
      <td>0.224</td>
      <td>0.279</td>
      <td>0.270</td>
      <td>0.194</td>
      <td>0.211</td>
      <td>0.272</td>
      <td>0.186</td>
      <td>0.181</td>
      <td>0.275</td>
      <td>ESM-IF1 model</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'&gt;Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>25</th>
      <td>ProtSSN (k=30, h=1280)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.222</td>
      <td>0.007</td>
      <td>0.187</td>
      <td>0.199</td>
      <td>0.218</td>
      <td>0.186</td>
      <td>0.321</td>
      <td>0.195</td>
      <td>0.229</td>
      <td>0.274</td>
      <td>0.225</td>
      <td>0.266</td>
      <td>0.261</td>
      <td>0.207</td>
      <td>0.209</td>
      <td>0.246</td>
      <td>0.185</td>
      <td>0.175</td>
      <td>0.218</td>
      <td>ProtSSN (k=30, h=1280)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>26</th>
      <td>ProtSSN (k=10, h=512)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.222</td>
      <td>0.007</td>
      <td>0.188</td>
      <td>0.198</td>
      <td>0.216</td>
      <td>0.182</td>
      <td>0.325</td>
      <td>0.192</td>
      <td>0.230</td>
      <td>0.274</td>
      <td>0.224</td>
      <td>0.273</td>
      <td>0.265</td>
      <td>0.195</td>
      <td>0.205</td>
      <td>0.242</td>
      <td>0.180</td>
      <td>0.160</td>
      <td>0.199</td>
      <td>ProtSSN (k=10, h=512)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'&gt;Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>27</th>
      <td>ProSST (K=512)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.222</td>
      <td>0.005</td>
      <td>0.160</td>
      <td>0.241</td>
      <td>0.214</td>
      <td>0.188</td>
      <td>0.305</td>
      <td>0.218</td>
      <td>0.212</td>
      <td>0.268</td>
      <td>0.226</td>
      <td>0.240</td>
      <td>0.253</td>
      <td>0.199</td>
      <td>0.205</td>
      <td>0.258</td>
      <td>0.206</td>
      <td>0.192</td>
      <td>0.271</td>
      <td>ProSST (K=512)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'&gt;Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>28</th>
      <td>EVmutation</td>
      <td>Alignment-based model</td>
      <td>0.221</td>
      <td>0.008</td>
      <td>0.201</td>
      <td>0.204</td>
      <td>0.209</td>
      <td>0.220</td>
      <td>0.272</td>
      <td>0.213</td>
      <td>0.235</td>
      <td>0.246</td>
      <td>0.212</td>
      <td>0.262</td>
      <td>0.232</td>
      <td>0.278</td>
      <td>0.200</td>
      <td>0.254</td>
      <td>0.219</td>
      <td>0.215</td>
      <td>0.283</td>
      <td>EVmutation model</td>
      <td>&lt;a href='https://www.nature.com/articles/nbt.3769'&gt;Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., &amp; Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Tranception L</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.220</td>
      <td>0.008</td>
      <td>0.200</td>
      <td>0.205</td>
      <td>0.210</td>
      <td>0.219</td>
      <td>0.265</td>
      <td>0.226</td>
      <td>0.232</td>
      <td>0.237</td>
      <td>0.216</td>
      <td>0.259</td>
      <td>0.219</td>
      <td>0.271</td>
      <td>0.201</td>
      <td>0.250</td>
      <td>0.198</td>
      <td>0.202</td>
      <td>0.272</td>
      <td>Tranception Large model (700M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Tranception M</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.218</td>
      <td>0.008</td>
      <td>0.188</td>
      <td>0.208</td>
      <td>0.211</td>
      <td>0.212</td>
      <td>0.271</td>
      <td>0.227</td>
      <td>0.228</td>
      <td>0.234</td>
      <td>0.218</td>
      <td>0.260</td>
      <td>0.204</td>
      <td>0.270</td>
      <td>0.201</td>
      <td>0.247</td>
      <td>0.173</td>
      <td>0.200</td>
      <td>0.282</td>
      <td>Tranception Medium model (300M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Tranception S</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.217</td>
      <td>0.007</td>
      <td>0.186</td>
      <td>0.215</td>
      <td>0.208</td>
      <td>0.204</td>
      <td>0.271</td>
      <td>0.222</td>
      <td>0.220</td>
      <td>0.240</td>
      <td>0.214</td>
      <td>0.256</td>
      <td>0.210</td>
      <td>0.257</td>
      <td>0.198</td>
      <td>0.243</td>
      <td>0.173</td>
      <td>0.194</td>
      <td>0.267</td>
      <td>Tranception Small model (85M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>32</th>
      <td>MIF</td>
      <td>Inverse folding model</td>
      <td>0.216</td>
      <td>0.008</td>
      <td>0.178</td>
      <td>0.201</td>
      <td>0.214</td>
      <td>0.162</td>
      <td>0.327</td>
      <td>0.194</td>
      <td>0.218</td>
      <td>0.266</td>
      <td>0.221</td>
      <td>0.250</td>
      <td>0.248</td>
      <td>0.206</td>
      <td>0.211</td>
      <td>0.246</td>
      <td>0.199</td>
      <td>0.168</td>
      <td>0.241</td>
      <td>MIF model</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'&gt;Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>33</th>
      <td>ESM2 (650M)</td>
      <td>Protein language model</td>
      <td>0.216</td>
      <td>0.007</td>
      <td>0.178</td>
      <td>0.213</td>
      <td>0.215</td>
      <td>0.175</td>
      <td>0.300</td>
      <td>0.178</td>
      <td>0.213</td>
      <td>0.268</td>
      <td>0.221</td>
      <td>0.257</td>
      <td>0.248</td>
      <td>0.168</td>
      <td>0.197</td>
      <td>0.232</td>
      <td>0.171</td>
      <td>0.145</td>
      <td>0.167</td>
      <td>ESM2 model (650M params)</td>
      <td>&lt;a href='https://www.science.org/doi/abs/10.1126/science.ade2574'&gt;Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>34</th>
      <td>ESM2 (3B)</td>
      <td>Protein language model</td>
      <td>0.213</td>
      <td>0.007</td>
      <td>0.176</td>
      <td>0.207</td>
      <td>0.212</td>
      <td>0.182</td>
      <td>0.286</td>
      <td>0.187</td>
      <td>0.215</td>
      <td>0.253</td>
      <td>0.213</td>
      <td>0.256</td>
      <td>0.244</td>
      <td>0.180</td>
      <td>0.193</td>
      <td>0.230</td>
      <td>0.169</td>
      <td>0.162</td>
      <td>0.167</td>
      <td>ESM2 model (3B params)</td>
      <td>&lt;a href='https://www.science.org/doi/abs/10.1126/science.ade2574'&gt;Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>35</th>
      <td>MSA Transformer (single)</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.212</td>
      <td>0.009</td>
      <td>0.190</td>
      <td>0.194</td>
      <td>0.201</td>
      <td>0.217</td>
      <td>0.259</td>
      <td>0.214</td>
      <td>0.225</td>
      <td>0.235</td>
      <td>0.206</td>
      <td>0.258</td>
      <td>0.218</td>
      <td>0.260</td>
      <td>0.192</td>
      <td>0.213</td>
      <td>0.218</td>
      <td>0.227</td>
      <td>0.269</td>
      <td>MSA Transformer (single MSA sample)</td>
      <td>&lt;a href='http://proceedings.mlr.press/v139/rao21a.html'&gt;Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., &amp; Rives, A. (2021). MSA Transformer. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>36</th>
      <td>SaProt (35M)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.211</td>
      <td>0.006</td>
      <td>0.168</td>
      <td>0.198</td>
      <td>0.225</td>
      <td>0.154</td>
      <td>0.311</td>
      <td>0.172</td>
      <td>0.209</td>
      <td>0.263</td>
      <td>0.218</td>
      <td>0.261</td>
      <td>0.240</td>
      <td>0.147</td>
      <td>0.205</td>
      <td>0.274</td>
      <td>0.161</td>
      <td>0.171</td>
      <td>0.196</td>
      <td>SaProt (35M)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'&gt;Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>37</th>
      <td>GEMME</td>
      <td>Alignment-based model</td>
      <td>0.211</td>
      <td>0.009</td>
      <td>0.196</td>
      <td>0.198</td>
      <td>0.198</td>
      <td>0.221</td>
      <td>0.240</td>
      <td>0.213</td>
      <td>0.228</td>
      <td>0.220</td>
      <td>0.202</td>
      <td>0.231</td>
      <td>0.215</td>
      <td>0.284</td>
      <td>0.186</td>
      <td>0.225</td>
      <td>0.179</td>
      <td>0.207</td>
      <td>0.281</td>
      <td>GEMME model</td>
      <td>&lt;a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'&gt;Laine, É., Karami, Y., &amp; Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>38</th>
      <td>ESM-1v (ensemble)</td>
      <td>Protein language model</td>
      <td>0.210</td>
      <td>0.008</td>
      <td>0.173</td>
      <td>0.213</td>
      <td>0.210</td>
      <td>0.190</td>
      <td>0.265</td>
      <td>0.182</td>
      <td>0.210</td>
      <td>0.249</td>
      <td>0.209</td>
      <td>0.236</td>
      <td>0.230</td>
      <td>0.203</td>
      <td>0.193</td>
      <td>0.211</td>
      <td>0.159</td>
      <td>0.144</td>
      <td>0.159</td>
      <td>ESM-1v (ensemble of 5 independently-trained models)</td>
      <td>&lt;a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'&gt;Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., &amp; Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>39</th>
      <td>ProSST (K=20)</td>
      <td>Hybrid - Structure &amp; PLM</td>
      <td>0.210</td>
      <td>0.005</td>
      <td>0.162</td>
      <td>0.243</td>
      <td>0.187</td>
      <td>0.175</td>
      <td>0.280</td>
      <td>0.224</td>
      <td>0.199</td>
      <td>0.241</td>
      <td>0.207</td>
      <td>0.223</td>
      <td>0.236</td>
      <td>0.203</td>
      <td>0.200</td>
      <td>0.236</td>
      <td>0.190</td>
      <td>0.179</td>
      <td>0.260</td>
      <td>ProSST (K=20)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'&gt;Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>40</th>
      <td>ESM2 (15B)</td>
      <td>Protein language model</td>
      <td>0.207</td>
      <td>0.008</td>
      <td>0.180</td>
      <td>0.196</td>
      <td>0.199</td>
      <td>0.193</td>
      <td>0.270</td>
      <td>0.184</td>
      <td>0.217</td>
      <td>0.242</td>
      <td>0.209</td>
      <td>0.242</td>
      <td>0.239</td>
      <td>0.192</td>
      <td>0.189</td>
      <td>0.219</td>
      <td>0.155</td>
      <td>0.150</td>
      <td>0.168</td>
      <td>ESM2 model (15B params)</td>
      <td>&lt;a href='https://www.science.org/doi/abs/10.1126/science.ade2574'&gt;Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>41</th>
      <td>CARP (640M)</td>
      <td>Protein language model</td>
      <td>0.207</td>
      <td>0.007</td>
      <td>0.176</td>
      <td>0.206</td>
      <td>0.212</td>
      <td>0.178</td>
      <td>0.265</td>
      <td>0.174</td>
      <td>0.209</td>
      <td>0.240</td>
      <td>0.215</td>
      <td>0.231</td>
      <td>0.222</td>
      <td>0.180</td>
      <td>0.189</td>
      <td>0.214</td>
      <td>0.159</td>
      <td>0.148</td>
      <td>0.173</td>
      <td>CARP model (640M params)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'&gt;Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>42</th>
      <td>VespaG</td>
      <td>Protein language model</td>
      <td>0.205</td>
      <td>0.008</td>
      <td>0.186</td>
      <td>0.196</td>
      <td>0.190</td>
      <td>0.202</td>
      <td>0.251</td>
      <td>0.192</td>
      <td>0.221</td>
      <td>0.224</td>
      <td>0.197</td>
      <td>0.243</td>
      <td>0.219</td>
      <td>0.238</td>
      <td>0.177</td>
      <td>0.228</td>
      <td>0.191</td>
      <td>0.215</td>
      <td>0.255</td>
      <td>VespaG model</td>
      <td>&lt;a href='https://doi.org/10.1101/2024.04.24.590982'&gt;Marquet, C., Schlensok, J., Abakarova, M., Rost, B., &amp; Laine, E. (2024). Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>43</th>
      <td>ESM2 (150M)</td>
      <td>Protein language model</td>
      <td>0.204</td>
      <td>0.007</td>
      <td>0.168</td>
      <td>0.204</td>
      <td>0.196</td>
      <td>0.153</td>
      <td>0.299</td>
      <td>0.162</td>
      <td>0.195</td>
      <td>0.267</td>
      <td>0.222</td>
      <td>0.249</td>
      <td>0.226</td>
      <td>0.129</td>
      <td>0.188</td>
      <td>0.234</td>
      <td>0.140</td>
      <td>0.148</td>
      <td>0.193</td>
      <td>ESM2 model (150M params)</td>
      <td>&lt;a href='https://www.science.org/doi/abs/10.1126/science.ade2574'&gt;Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>44</th>
      <td>Wavenet</td>
      <td>Alignment-based model</td>
      <td>0.203</td>
      <td>0.008</td>
      <td>0.167</td>
      <td>0.204</td>
      <td>0.190</td>
      <td>0.201</td>
      <td>0.252</td>
      <td>0.175</td>
      <td>0.215</td>
      <td>0.234</td>
      <td>0.203</td>
      <td>0.219</td>
      <td>0.211</td>
      <td>0.243</td>
      <td>0.182</td>
      <td>0.236</td>
      <td>0.166</td>
      <td>0.153</td>
      <td>0.188</td>
      <td>Wavenet model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41467-021-22732-w'&gt;Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., &amp; Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>45</th>
      <td>ESM-1b</td>
      <td>Protein language model</td>
      <td>0.202</td>
      <td>0.008</td>
      <td>0.180</td>
      <td>0.178</td>
      <td>0.197</td>
      <td>0.173</td>
      <td>0.282</td>
      <td>0.161</td>
      <td>0.214</td>
      <td>0.248</td>
      <td>0.209</td>
      <td>0.241</td>
      <td>0.243</td>
      <td>0.169</td>
      <td>0.183</td>
      <td>0.213</td>
      <td>0.159</td>
      <td>0.144</td>
      <td>0.186</td>
      <td>ESM-1b (w/ Brandes et al. extensions)</td>
      <td>[1] Original model: &lt;a href='https://www.biorxiv.org/content/10.1101/622803v4'&gt;Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., &amp; Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.&lt;/a&gt; [2] Extensions: &lt;a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'&gt;Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., &amp; Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>46</th>
      <td>Progen2 M</td>
      <td>Protein language model</td>
      <td>0.202</td>
      <td>0.008</td>
      <td>0.172</td>
      <td>0.190</td>
      <td>0.221</td>
      <td>0.197</td>
      <td>0.229</td>
      <td>0.177</td>
      <td>0.210</td>
      <td>0.217</td>
      <td>0.204</td>
      <td>0.202</td>
      <td>0.205</td>
      <td>0.224</td>
      <td>0.185</td>
      <td>0.190</td>
      <td>0.147</td>
      <td>0.129</td>
      <td>0.134</td>
      <td>Progen2 medium model (760M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>47</th>
      <td>Progen2 L</td>
      <td>Protein language model</td>
      <td>0.201</td>
      <td>0.008</td>
      <td>0.185</td>
      <td>0.175</td>
      <td>0.223</td>
      <td>0.200</td>
      <td>0.224</td>
      <td>0.193</td>
      <td>0.208</td>
      <td>0.214</td>
      <td>0.204</td>
      <td>0.203</td>
      <td>0.216</td>
      <td>0.214</td>
      <td>0.184</td>
      <td>0.193</td>
      <td>0.173</td>
      <td>0.152</td>
      <td>0.178</td>
      <td>Progen2 large model (2.7B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>48</th>
      <td>Site-Independent</td>
      <td>Alignment-based model</td>
      <td>0.201</td>
      <td>0.007</td>
      <td>0.170</td>
      <td>0.198</td>
      <td>0.179</td>
      <td>0.198</td>
      <td>0.261</td>
      <td>0.209</td>
      <td>0.207</td>
      <td>0.226</td>
      <td>0.202</td>
      <td>0.246</td>
      <td>0.194</td>
      <td>0.245</td>
      <td>0.192</td>
      <td>0.247</td>
      <td>0.180</td>
      <td>0.194</td>
      <td>0.272</td>
      <td>Site-Independent model</td>
      <td>&lt;a href='https://www.nature.com/articles/nbt.3769'&gt;Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., &amp; Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>49</th>
      <td>VESPA</td>
      <td>Protein language model</td>
      <td>0.200</td>
      <td>0.008</td>
      <td>0.180</td>
      <td>0.192</td>
      <td>0.184</td>
      <td>0.204</td>
      <td>0.242</td>
      <td>0.192</td>
      <td>0.218</td>
      <td>0.217</td>
      <td>0.190</td>
      <td>0.231</td>
      <td>0.218</td>
      <td>0.247</td>
      <td>0.176</td>
      <td>0.206</td>
      <td>0.201</td>
      <td>0.205</td>
      <td>0.246</td>
      <td>VESPA model</td>
      <td>&lt;a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'&gt;Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., &amp; Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>50</th>
      <td>Progen2 XL</td>
      <td>Protein language model</td>
      <td>0.198</td>
      <td>0.009</td>
      <td>0.181</td>
      <td>0.179</td>
      <td>0.184</td>
      <td>0.211</td>
      <td>0.237</td>
      <td>0.194</td>
      <td>0.221</td>
      <td>0.212</td>
      <td>0.185</td>
      <td>0.231</td>
      <td>0.226</td>
      <td>0.251</td>
      <td>0.180</td>
      <td>0.215</td>
      <td>0.189</td>
      <td>0.157</td>
      <td>0.198</td>
      <td>Progen2 xlarge model (6.4B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>51</th>
      <td>Progen2 Base</td>
      <td>Protein language model</td>
      <td>0.198</td>
      <td>0.008</td>
      <td>0.183</td>
      <td>0.182</td>
      <td>0.205</td>
      <td>0.199</td>
      <td>0.219</td>
      <td>0.190</td>
      <td>0.206</td>
      <td>0.212</td>
      <td>0.205</td>
      <td>0.195</td>
      <td>0.203</td>
      <td>0.218</td>
      <td>0.184</td>
      <td>0.190</td>
      <td>0.154</td>
      <td>0.140</td>
      <td>0.151</td>
      <td>Progen2 base model (760M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>52</th>
      <td>Tranception L no retrieval</td>
      <td>Protein language model</td>
      <td>0.197</td>
      <td>0.009</td>
      <td>0.181</td>
      <td>0.185</td>
      <td>0.191</td>
      <td>0.210</td>
      <td>0.216</td>
      <td>0.199</td>
      <td>0.209</td>
      <td>0.205</td>
      <td>0.191</td>
      <td>0.201</td>
      <td>0.208</td>
      <td>0.253</td>
      <td>0.182</td>
      <td>0.208</td>
      <td>0.193</td>
      <td>0.166</td>
      <td>0.232</td>
      <td>Tranception Large model (700M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>53</th>
      <td>RITA L</td>
      <td>Protein language model</td>
      <td>0.195</td>
      <td>0.009</td>
      <td>0.173</td>
      <td>0.178</td>
      <td>0.203</td>
      <td>0.205</td>
      <td>0.215</td>
      <td>0.181</td>
      <td>0.208</td>
      <td>0.205</td>
      <td>0.199</td>
      <td>0.185</td>
      <td>0.196</td>
      <td>0.246</td>
      <td>0.178</td>
      <td>0.195</td>
      <td>0.135</td>
      <td>0.140</td>
      <td>0.147</td>
      <td>RITA large model (680M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>54</th>
      <td>ESM-1v (single)</td>
      <td>Protein language model</td>
      <td>0.194</td>
      <td>0.008</td>
      <td>0.166</td>
      <td>0.175</td>
      <td>0.204</td>
      <td>0.179</td>
      <td>0.245</td>
      <td>0.151</td>
      <td>0.196</td>
      <td>0.239</td>
      <td>0.195</td>
      <td>0.226</td>
      <td>0.214</td>
      <td>0.178</td>
      <td>0.182</td>
      <td>0.199</td>
      <td>0.150</td>
      <td>0.140</td>
      <td>0.150</td>
      <td>ESM-1v (single seed)</td>
      <td>&lt;a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'&gt;Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., &amp; Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>55</th>
      <td>RITA XL</td>
      <td>Protein language model</td>
      <td>0.193</td>
      <td>0.008</td>
      <td>0.170</td>
      <td>0.173</td>
      <td>0.202</td>
      <td>0.203</td>
      <td>0.216</td>
      <td>0.178</td>
      <td>0.212</td>
      <td>0.198</td>
      <td>0.194</td>
      <td>0.188</td>
      <td>0.201</td>
      <td>0.243</td>
      <td>0.176</td>
      <td>0.202</td>
      <td>0.140</td>
      <td>0.143</td>
      <td>0.174</td>
      <td>RITA xlarge model (1.2B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>56</th>
      <td>CARP (76M)</td>
      <td>Protein language model</td>
      <td>0.188</td>
      <td>0.007</td>
      <td>0.155</td>
      <td>0.185</td>
      <td>0.195</td>
      <td>0.154</td>
      <td>0.254</td>
      <td>0.151</td>
      <td>0.181</td>
      <td>0.235</td>
      <td>0.197</td>
      <td>0.220</td>
      <td>0.202</td>
      <td>0.145</td>
      <td>0.178</td>
      <td>0.199</td>
      <td>0.141</td>
      <td>0.147</td>
      <td>0.183</td>
      <td>CARP model (76M params)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'&gt;Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>57</th>
      <td>RITA M</td>
      <td>Protein language model</td>
      <td>0.188</td>
      <td>0.009</td>
      <td>0.168</td>
      <td>0.163</td>
      <td>0.199</td>
      <td>0.201</td>
      <td>0.207</td>
      <td>0.179</td>
      <td>0.194</td>
      <td>0.207</td>
      <td>0.194</td>
      <td>0.177</td>
      <td>0.182</td>
      <td>0.250</td>
      <td>0.176</td>
      <td>0.185</td>
      <td>0.137</td>
      <td>0.148</td>
      <td>0.180</td>
      <td>RITA medium model (300M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>58</th>
      <td>ProteinMPNN</td>
      <td>Inverse folding model</td>
      <td>0.186</td>
      <td>0.007</td>
      <td>0.149</td>
      <td>0.146</td>
      <td>0.152</td>
      <td>0.140</td>
      <td>0.342</td>
      <td>0.146</td>
      <td>0.201</td>
      <td>0.272</td>
      <td>0.196</td>
      <td>0.267</td>
      <td>0.229</td>
      <td>0.189</td>
      <td>0.186</td>
      <td>0.244</td>
      <td>0.178</td>
      <td>0.143</td>
      <td>0.223</td>
      <td>ProteinMPNN model</td>
      <td>&lt;a href='https://www.science.org/doi/10.1126/science.add2187'&gt;J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>59</th>
      <td>ESM2 (35M)</td>
      <td>Protein language model</td>
      <td>0.186</td>
      <td>0.007</td>
      <td>0.155</td>
      <td>0.197</td>
      <td>0.181</td>
      <td>0.141</td>
      <td>0.254</td>
      <td>0.153</td>
      <td>0.163</td>
      <td>0.248</td>
      <td>0.197</td>
      <td>0.219</td>
      <td>0.197</td>
      <td>0.124</td>
      <td>0.175</td>
      <td>0.224</td>
      <td>0.138</td>
      <td>0.150</td>
      <td>0.206</td>
      <td>ESM2 model (35M params)</td>
      <td>&lt;a href='https://www.science.org/doi/abs/10.1126/science.ade2574'&gt;Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>60</th>
      <td>Tranception M no retrieval</td>
      <td>Protein language model</td>
      <td>0.185</td>
      <td>0.009</td>
      <td>0.163</td>
      <td>0.176</td>
      <td>0.190</td>
      <td>0.201</td>
      <td>0.194</td>
      <td>0.178</td>
      <td>0.194</td>
      <td>0.191</td>
      <td>0.188</td>
      <td>0.174</td>
      <td>0.180</td>
      <td>0.234</td>
      <td>0.175</td>
      <td>0.182</td>
      <td>0.138</td>
      <td>0.151</td>
      <td>0.181</td>
      <td>Tranception Medium model (300M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>61</th>
      <td>VESPAl</td>
      <td>Protein language model</td>
      <td>0.184</td>
      <td>0.008</td>
      <td>0.172</td>
      <td>0.183</td>
      <td>0.159</td>
      <td>0.192</td>
      <td>0.214</td>
      <td>0.181</td>
      <td>0.197</td>
      <td>0.199</td>
      <td>0.176</td>
      <td>0.216</td>
      <td>0.193</td>
      <td>0.225</td>
      <td>0.155</td>
      <td>0.188</td>
      <td>0.185</td>
      <td>0.193</td>
      <td>0.243</td>
      <td>VESPAl model</td>
      <td>&lt;a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'&gt;Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., &amp; Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>62</th>
      <td>Progen2 S</td>
      <td>Protein language model</td>
      <td>0.183</td>
      <td>0.009</td>
      <td>0.155</td>
      <td>0.166</td>
      <td>0.204</td>
      <td>0.173</td>
      <td>0.218</td>
      <td>0.144</td>
      <td>0.184</td>
      <td>0.213</td>
      <td>0.196</td>
      <td>0.177</td>
      <td>0.193</td>
      <td>0.170</td>
      <td>0.169</td>
      <td>0.178</td>
      <td>0.135</td>
      <td>0.141</td>
      <td>0.147</td>
      <td>Progen2 small model (150M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>63</th>
      <td>Unirep evotuned</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.180</td>
      <td>0.009</td>
      <td>0.163</td>
      <td>0.174</td>
      <td>0.186</td>
      <td>0.187</td>
      <td>0.192</td>
      <td>0.190</td>
      <td>0.188</td>
      <td>0.178</td>
      <td>0.177</td>
      <td>0.182</td>
      <td>0.175</td>
      <td>0.227</td>
      <td>0.158</td>
      <td>0.175</td>
      <td>0.152</td>
      <td>0.150</td>
      <td>0.207</td>
      <td>Unirep model w/ evotuning</td>
      <td>&lt;a href='https://www.nature.com/articles/s41592-019-0598-1'&gt;Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., &amp; Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>64</th>
      <td>MULAN_small</td>
      <td>Protein language model</td>
      <td>0.180</td>
      <td>0.008</td>
      <td>0.137</td>
      <td>0.222</td>
      <td>0.193</td>
      <td>0.143</td>
      <td>0.204</td>
      <td>0.172</td>
      <td>0.166</td>
      <td>0.184</td>
      <td>0.180</td>
      <td>0.169</td>
      <td>0.184</td>
      <td>0.137</td>
      <td>0.169</td>
      <td>0.185</td>
      <td>0.108</td>
      <td>0.138</td>
      <td>0.138</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>65</th>
      <td>RITA S</td>
      <td>Protein language model</td>
      <td>0.178</td>
      <td>0.008</td>
      <td>0.155</td>
      <td>0.169</td>
      <td>0.194</td>
      <td>0.186</td>
      <td>0.186</td>
      <td>0.177</td>
      <td>0.174</td>
      <td>0.192</td>
      <td>0.181</td>
      <td>0.163</td>
      <td>0.166</td>
      <td>0.226</td>
      <td>0.166</td>
      <td>0.168</td>
      <td>0.119</td>
      <td>0.128</td>
      <td>0.151</td>
      <td>RITA small model (85M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>66</th>
      <td>CARP (38M)</td>
      <td>Protein language model</td>
      <td>0.177</td>
      <td>0.007</td>
      <td>0.147</td>
      <td>0.188</td>
      <td>0.177</td>
      <td>0.141</td>
      <td>0.233</td>
      <td>0.148</td>
      <td>0.165</td>
      <td>0.219</td>
      <td>0.185</td>
      <td>0.198</td>
      <td>0.183</td>
      <td>0.141</td>
      <td>0.166</td>
      <td>0.188</td>
      <td>0.141</td>
      <td>0.150</td>
      <td>0.177</td>
      <td>CARP model (38M params)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'&gt;Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>67</th>
      <td>Tranception S no retrieval</td>
      <td>Protein language model</td>
      <td>0.177</td>
      <td>0.008</td>
      <td>0.154</td>
      <td>0.188</td>
      <td>0.182</td>
      <td>0.185</td>
      <td>0.177</td>
      <td>0.173</td>
      <td>0.174</td>
      <td>0.185</td>
      <td>0.174</td>
      <td>0.159</td>
      <td>0.173</td>
      <td>0.215</td>
      <td>0.167</td>
      <td>0.165</td>
      <td>0.128</td>
      <td>0.134</td>
      <td>0.146</td>
      <td>Tranception Small model (85M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>68</th>
      <td>ESM2 (8M)</td>
      <td>Protein language model</td>
      <td>0.153</td>
      <td>0.007</td>
      <td>0.126</td>
      <td>0.178</td>
      <td>0.162</td>
      <td>0.118</td>
      <td>0.181</td>
      <td>0.141</td>
      <td>0.136</td>
      <td>0.174</td>
      <td>0.155</td>
      <td>0.155</td>
      <td>0.152</td>
      <td>0.120</td>
      <td>0.145</td>
      <td>0.178</td>
      <td>0.129</td>
      <td>0.128</td>
      <td>0.163</td>
      <td>ESM2 model (8M params)</td>
      <td>&lt;a href='https://www.science.org/doi/abs/10.1126/science.ade2574'&gt;Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>69</th>
      <td>Unirep</td>
      <td>Protein language model</td>
      <td>0.139</td>
      <td>0.008</td>
      <td>0.116</td>
      <td>0.138</td>
      <td>0.141</td>
      <td>0.124</td>
      <td>0.176</td>
      <td>0.145</td>
      <td>0.129</td>
      <td>0.166</td>
      <td>0.148</td>
      <td>0.158</td>
      <td>0.145</td>
      <td>0.113</td>
      <td>0.138</td>
      <td>0.157</td>
      <td>0.111</td>
      <td>0.128</td>
      <td>0.149</td>
      <td>Unirep model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41592-019-0598-1'&gt;Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., &amp; Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>70</th>
      <td>ProtGPT2</td>
      <td>Protein language model</td>
      <td>0.133</td>
      <td>0.008</td>
      <td>0.113</td>
      <td>0.138</td>
      <td>0.126</td>
      <td>0.122</td>
      <td>0.168</td>
      <td>0.122</td>
      <td>0.136</td>
      <td>0.148</td>
      <td>0.144</td>
      <td>0.156</td>
      <td>0.116</td>
      <td>0.133</td>
      <td>0.125</td>
      <td>0.175</td>
      <td>0.115</td>
      <td>0.116</td>
      <td>0.123</td>
      <td>ProtGPT2 model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41467-022-32007-7'&gt;Ferruz, N., Schmidt, S., &amp; Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>71</th>
      <td>CARP (600K)</td>
      <td>Protein language model</td>
      <td>0.131</td>
      <td>0.008</td>
      <td>0.116</td>
      <td>0.124</td>
      <td>0.132</td>
      <td>0.106</td>
      <td>0.176</td>
      <td>0.130</td>
      <td>0.122</td>
      <td>0.157</td>
      <td>0.140</td>
      <td>0.148</td>
      <td>0.131</td>
      <td>0.115</td>
      <td>0.132</td>
      <td>0.143</td>
      <td>0.107</td>
      <td>0.120</td>
      <td>0.127</td>
      <td>CARP model (600K params)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'&gt;Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.&lt;/a&gt;</td>
    </tr>
  </tbody>
</table>