Model_rank,Model_name,Model type,Average_NDCG,Bootstrap_standard_error_NDCG,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,TranceptEVE L,Hybrid - Alignment & PLM,0.786,0.0,0.794,0.73,0.801,0.766,0.839,0.762,0.781,0.828,0.787,0.816,0.831,0.743,0.8,0.715,0.699,0.664,0.661,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
2,TranceptEVE M,Hybrid - Alignment & PLM,0.784,0.001,0.793,0.727,0.799,0.764,0.837,0.759,0.782,0.823,0.787,0.814,0.824,0.743,0.798,0.707,0.684,0.659,0.66,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
3,TranceptEVE S,Hybrid - Alignment & PLM,0.784,0.001,0.791,0.731,0.799,0.761,0.838,0.759,0.779,0.824,0.786,0.813,0.825,0.74,0.797,0.711,0.685,0.653,0.654,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
4,PoET (200M),Hybrid - Alignment & PLM,0.784,0.003,0.791,0.732,0.788,0.76,0.847,0.762,0.778,0.83,0.781,0.827,0.829,0.744,0.799,0.722,0.717,0.685,0.69,PoET (200M),"<a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'>Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.</a>"
5,EVE (ensemble),Alignment-based model,0.782,0.001,0.791,0.724,0.798,0.76,0.838,0.754,0.779,0.824,0.784,0.81,0.827,0.742,0.796,0.707,0.695,0.664,0.645,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
6,EVE (single),Alignment-based model,0.781,0.001,0.789,0.724,0.796,0.759,0.835,0.752,0.778,0.821,0.782,0.81,0.824,0.74,0.794,0.709,0.691,0.662,0.647,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
7,Tranception L,Hybrid - Alignment & PLM,0.779,0.002,0.794,0.724,0.795,0.759,0.824,0.759,0.773,0.818,0.788,0.807,0.812,0.727,0.793,0.705,0.687,0.653,0.658,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
8,MSA Transformer (ensemble),Hybrid - Alignment & PLM,0.777,0.004,0.789,0.708,0.806,0.749,0.835,0.752,0.773,0.819,0.78,0.812,0.823,0.723,0.791,0.692,0.708,0.691,0.667,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
9,EVmutation,Alignment-based model,0.777,0.002,0.792,0.708,0.792,0.756,0.835,0.748,0.776,0.82,0.778,0.813,0.825,0.734,0.792,0.715,0.691,0.663,0.669,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
10,GEMME,Alignment-based model,0.776,0.002,0.795,0.711,0.793,0.754,0.827,0.758,0.775,0.81,0.778,0.805,0.814,0.743,0.788,0.689,0.682,0.666,0.68,GEMME model,"<a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a>"
11,DeepSequence (ensemble),Alignment-based model,0.776,0.003,0.785,0.716,0.787,0.756,0.835,0.743,0.773,0.821,0.781,0.806,0.821,0.73,0.791,0.704,0.674,0.645,0.644,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
12,VESPA,Protein language model,0.775,0.004,0.789,0.711,0.79,0.75,0.835,0.741,0.773,0.823,0.776,0.81,0.831,0.719,0.789,0.665,0.71,0.659,0.634,VESPA model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
13,ProSST (K=4096),Hybrid - Structure & PLM,0.774,0.008,0.777,0.764,0.776,0.709,0.846,0.745,0.754,0.822,0.778,0.809,0.827,0.654,0.781,0.725,0.708,0.671,0.682,ProSST (K=4096),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
14,DeepSequence (single),Alignment-based model,0.774,0.003,0.786,0.711,0.787,0.753,0.834,0.745,0.771,0.819,0.783,0.807,0.817,0.723,0.79,0.702,0.667,0.643,0.645,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
15,Tranception M,Hybrid - Alignment & PLM,0.769,0.003,0.781,0.718,0.783,0.746,0.816,0.748,0.766,0.8,0.781,0.801,0.788,0.718,0.782,0.692,0.651,0.637,0.653,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
16,SaProt (650M),Hybrid - Structure & PLM,0.768,0.006,0.767,0.72,0.797,0.695,0.861,0.701,0.752,0.834,0.784,0.805,0.831,0.613,0.779,0.706,0.645,0.613,0.618,SaProt (650M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
17,Progen2 XL,Protein language model,0.767,0.004,0.771,0.7,0.786,0.756,0.823,0.73,0.768,0.814,0.772,0.791,0.822,0.718,0.786,0.667,0.653,0.59,0.562,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
18,Tranception S,Hybrid - Alignment & PLM,0.767,0.003,0.774,0.725,0.785,0.739,0.813,0.749,0.758,0.798,0.776,0.797,0.786,0.71,0.778,0.692,0.65,0.631,0.641,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
19,MSA Transformer (single),Hybrid - Alignment & PLM,0.766,0.004,0.781,0.696,0.786,0.745,0.823,0.743,0.767,0.805,0.771,0.809,0.807,0.712,0.782,0.679,0.702,0.687,0.665,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
20,ProtSSN (ensemble),Hybrid - Structure & PLM,0.766,0.005,0.772,0.714,0.776,0.709,0.858,0.711,0.754,0.832,0.776,0.812,0.83,0.645,0.778,0.697,0.634,0.589,0.577,ProtSSN (ensemble of 9 models),"<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
21,MIF-ST,Hybrid - Structure & PLM,0.765,0.004,0.769,0.707,0.79,0.721,0.839,0.724,0.755,0.819,0.769,0.799,0.829,0.671,0.781,0.684,0.674,0.625,0.629,MIF-ST model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
22,"ProtSSN (k=30, h=768)",Hybrid - Structure & PLM,0.765,0.005,0.774,0.708,0.784,0.706,0.854,0.714,0.752,0.829,0.776,0.809,0.828,0.638,0.777,0.693,0.629,0.585,0.573,"ProtSSN (k=30, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
23,"ProtSSN (k=20, h=512)",Hybrid - Structure & PLM,0.764,0.005,0.773,0.711,0.769,0.708,0.856,0.712,0.753,0.83,0.775,0.811,0.828,0.644,0.777,0.694,0.645,0.608,0.59,"ProtSSN (k=20, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
24,"ProtSSN (k=30, h=512)",Hybrid - Structure & PLM,0.763,0.005,0.769,0.713,0.776,0.706,0.854,0.705,0.753,0.828,0.774,0.808,0.826,0.639,0.775,0.688,0.626,0.591,0.569,"ProtSSN (k=30, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
25,"ProtSSN (k=20, h=1280)",Hybrid - Structure & PLM,0.762,0.005,0.772,0.708,0.769,0.708,0.855,0.705,0.751,0.831,0.773,0.812,0.826,0.641,0.776,0.696,0.638,0.591,0.585,"ProtSSN (k=20, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
26,"ProtSSN (k=20, h=768)",Hybrid - Structure & PLM,0.761,0.006,0.769,0.702,0.772,0.708,0.855,0.704,0.751,0.83,0.772,0.807,0.827,0.643,0.775,0.697,0.627,0.586,0.597,"ProtSSN (k=20, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
27,"ProtSSN (k=10, h=1280)",Hybrid - Structure & PLM,0.761,0.006,0.767,0.706,0.772,0.705,0.856,0.706,0.752,0.827,0.774,0.809,0.824,0.639,0.775,0.679,0.615,0.572,0.559,"ProtSSN (k=10, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
28,ProSST (K=1024),Hybrid - Structure & PLM,0.761,0.007,0.763,0.729,0.774,0.706,0.833,0.734,0.742,0.817,0.76,0.796,0.826,0.651,0.772,0.718,0.696,0.653,0.676,ProSST (K=1024),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
29,Wavenet,Alignment-based model,0.761,0.005,0.759,0.703,0.777,0.739,0.825,0.72,0.761,0.808,0.769,0.782,0.812,0.709,0.777,0.691,0.648,0.591,0.562,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
30,"ProtSSN (k=30, h=1280)",Hybrid - Structure & PLM,0.76,0.005,0.771,0.71,0.767,0.7,0.853,0.71,0.745,0.828,0.772,0.806,0.828,0.627,0.773,0.697,0.631,0.589,0.575,"ProtSSN (k=30, h=1280)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
31,ProSST (K=512),Hybrid - Structure & PLM,0.759,0.007,0.761,0.743,0.77,0.699,0.825,0.726,0.736,0.812,0.765,0.789,0.814,0.632,0.766,0.704,0.682,0.636,0.646,ProSST (K=512),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
32,"ProtSSN (k=10, h=768)",Hybrid - Structure & PLM,0.759,0.005,0.77,0.699,0.77,0.705,0.853,0.703,0.752,0.826,0.774,0.803,0.825,0.64,0.774,0.68,0.616,0.568,0.549,"ProtSSN (k=10, h=768)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
33,ESM2 (15B),Protein language model,0.759,0.006,0.75,0.71,0.78,0.727,0.827,0.703,0.753,0.813,0.778,0.775,0.814,0.652,0.776,0.673,0.602,0.543,0.504,ESM2 model (15B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
34,VESPAl,Protein language model,0.759,0.005,0.78,0.697,0.765,0.737,0.814,0.731,0.756,0.803,0.76,0.793,0.815,0.7,0.771,0.643,0.699,0.645,0.626,VESPAl model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
35,"ProtSSN (k=10, h=512)",Hybrid - Structure & PLM,0.758,0.006,0.769,0.7,0.771,0.701,0.849,0.707,0.746,0.825,0.771,0.807,0.822,0.627,0.771,0.69,0.61,0.566,0.547,"ProtSSN (k=10, h=512)","<a href='https://www.biorxiv.org/content/10.1101/2023.12.01.569522v1'>Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong. (2023). Semantical and Topological Protein Encoding Toward Enhanced Bioactivity and Thermostability. bioRxiv.</a>"
36,ProSST (K=2048),Hybrid - Structure & PLM,0.757,0.007,0.741,0.714,0.776,0.683,0.871,0.7,0.745,0.828,0.773,0.799,0.82,0.626,0.769,0.713,0.58,0.549,0.557,ProSST (K=2048),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
37,Progen2 L,Protein language model,0.755,0.006,0.763,0.691,0.791,0.735,0.795,0.718,0.744,0.802,0.773,0.768,0.795,0.668,0.771,0.649,0.634,0.582,0.545,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
38,ESM2 (3B),Protein language model,0.755,0.006,0.748,0.706,0.775,0.71,0.835,0.693,0.743,0.82,0.774,0.785,0.812,0.626,0.772,0.683,0.608,0.558,0.502,ESM2 model (3B params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
39,ProSST (K=128),Hybrid - Structure & PLM,0.754,0.006,0.747,0.747,0.759,0.696,0.821,0.729,0.731,0.801,0.752,0.782,0.811,0.643,0.759,0.72,0.691,0.65,0.664,ProSST (K=128),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
40,RITA XL,Protein language model,0.752,0.005,0.75,0.698,0.784,0.738,0.792,0.699,0.753,0.788,0.773,0.737,0.782,0.702,0.77,0.648,0.578,0.547,0.509,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
41,Progen2 M,Protein language model,0.752,0.006,0.747,0.696,0.786,0.735,0.794,0.69,0.746,0.8,0.772,0.751,0.784,0.679,0.77,0.634,0.583,0.531,0.475,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
42,Tranception L no retrieval,Protein language model,0.752,0.005,0.768,0.692,0.775,0.748,0.776,0.723,0.742,0.792,0.761,0.742,0.794,0.712,0.772,0.663,0.667,0.604,0.597,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
43,ESM-1v (ensemble),Protein language model,0.749,0.006,0.74,0.701,0.776,0.72,0.811,0.677,0.739,0.814,0.77,0.768,0.797,0.641,0.768,0.669,0.603,0.546,0.489,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
44,CARP (640M),Protein language model,0.749,0.005,0.75,0.693,0.771,0.71,0.818,0.695,0.74,0.805,0.774,0.78,0.787,0.633,0.768,0.676,0.607,0.55,0.512,CARP model (640M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
45,ESM-IF1,Inverse folding model,0.749,0.007,0.738,0.698,0.762,0.68,0.864,0.681,0.74,0.828,0.754,0.801,0.828,0.629,0.762,0.721,0.645,0.619,0.652,ESM-IF1 model,"<a href='https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full.pdf+html'>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, Alexander Rives (2022). Learning Inverse Folding from Millions of Predicted Structures. BioRxiv.</a>"
46,Progen2 Base,Protein language model,0.748,0.006,0.752,0.696,0.784,0.731,0.779,0.707,0.74,0.785,0.776,0.749,0.766,0.66,0.767,0.645,0.589,0.546,0.498,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
47,Site-Independent,Alignment-based model,0.748,0.004,0.751,0.7,0.76,0.725,0.802,0.736,0.744,0.776,0.759,0.781,0.77,0.695,0.765,0.691,0.647,0.632,0.639,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
48,RITA L,Protein language model,0.747,0.006,0.741,0.697,0.778,0.735,0.786,0.697,0.746,0.785,0.769,0.737,0.772,0.697,0.764,0.648,0.573,0.545,0.489,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
49,ESM2 (650M),Protein language model,0.747,0.007,0.739,0.705,0.764,0.694,0.834,0.67,0.733,0.82,0.768,0.778,0.808,0.599,0.764,0.686,0.61,0.546,0.502,ESM2 model (650M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
50,ESM-1b,Protein language model,0.747,0.009,0.767,0.668,0.767,0.699,0.832,0.691,0.741,0.816,0.771,0.798,0.808,0.608,0.761,0.679,0.588,0.535,0.531,ESM-1b (w/ Brandes et al. extensions),"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a>"
51,ProSST (K=20),Hybrid - Structure & PLM,0.745,0.007,0.75,0.732,0.759,0.686,0.8,0.723,0.723,0.785,0.745,0.774,0.796,0.628,0.754,0.693,0.682,0.628,0.637,ProSST (K=20),"<a href='https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3'>Mingchen Li, Yang Tan, Xinzhu Ma, Bozitao Zhong, Ziyi Zhou, Huiqun Yu, Wanli Ouyang, Liang Hong, Bingxin Zhou, Pan Tan. (2024). ProSST: Protein language modeling with quantizied structure and disentangled attention. bioRxiv.</a>"
52,MIF,Inverse folding model,0.744,0.006,0.736,0.694,0.78,0.666,0.844,0.706,0.727,0.8,0.755,0.771,0.8,0.632,0.759,0.688,0.65,0.62,0.623,MIF model,"<a href='https://www.biorxiv.org/content/10.1101/2022.05.25.493516v3'>Kevin K. Yang, Hugh Yeh, Niccolo Zanichelli (2023). Masked Inverse folding with Sequence Transfer for Protein Representation Learning. BioRxiv.</a>"
53,Unirep evotuned,Hybrid - Alignment & PLM,0.739,0.005,0.741,0.682,0.778,0.715,0.778,0.725,0.73,0.761,0.747,0.736,0.772,0.692,0.753,0.625,0.631,0.585,0.551,Unirep model w/ evotuning,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
54,SaProt (35M),Hybrid - Structure & PLM,0.735,0.008,0.726,0.684,0.777,0.663,0.826,0.66,0.716,0.805,0.753,0.769,0.792,0.575,0.754,0.718,0.592,0.554,0.511,SaProt (35M),"<a href='https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5'>Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan. (2024). SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv.</a>"
55,ESM-1v (single),Protein language model,0.732,0.009,0.734,0.664,0.764,0.708,0.791,0.652,0.726,0.802,0.753,0.756,0.783,0.621,0.755,0.659,0.601,0.543,0.482,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
56,RITA M,Protein language model,0.732,0.008,0.724,0.671,0.771,0.726,0.768,0.68,0.725,0.782,0.751,0.716,0.761,0.694,0.753,0.631,0.57,0.545,0.516,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
57,ESM2 (150M),Protein language model,0.729,0.008,0.724,0.691,0.743,0.663,0.821,0.65,0.71,0.805,0.758,0.759,0.782,0.559,0.746,0.684,0.575,0.539,0.507,ESM2 model (150M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
58,Tranception M no retrieval,Protein language model,0.727,0.009,0.732,0.67,0.76,0.728,0.745,0.673,0.724,0.763,0.743,0.712,0.753,0.683,0.75,0.64,0.584,0.55,0.518,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
59,CARP (76M),Protein language model,0.72,0.007,0.718,0.689,0.74,0.666,0.79,0.654,0.7,0.784,0.743,0.755,0.754,0.579,0.735,0.658,0.56,0.521,0.492,CARP model (76M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
60,Progen2 S,Protein language model,0.719,0.009,0.712,0.666,0.755,0.696,0.764,0.646,0.709,0.773,0.747,0.711,0.749,0.624,0.741,0.628,0.559,0.521,0.48,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
61,Tranception S no retrieval,Protein language model,0.714,0.007,0.704,0.699,0.747,0.704,0.717,0.668,0.7,0.735,0.722,0.687,0.729,0.66,0.73,0.624,0.566,0.527,0.48,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
62,ProteinMPNN,Inverse folding model,0.714,0.009,0.716,0.633,0.717,0.648,0.854,0.655,0.715,0.799,0.729,0.778,0.787,0.623,0.741,0.695,0.623,0.59,0.594,ProteinMPNN model,"<a href='https://www.science.org/doi/10.1126/science.add2187'>J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran,A. K. Bera, N. P. King,D. Baker (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, Vol 378.</a>"
63,RITA S,Protein language model,0.713,0.008,0.701,0.68,0.748,0.705,0.731,0.665,0.698,0.75,0.729,0.687,0.73,0.666,0.733,0.626,0.554,0.522,0.481,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
64,MULAN_small,Protein language model,0.71,0.007,0.698,0.719,0.749,0.648,0.738,0.661,0.688,0.727,0.722,0.693,0.75,0.554,0.722,0.632,0.549,0.534,0.472,,
65,ESM2 (35M),Protein language model,0.704,0.008,0.702,0.678,0.721,0.641,0.777,0.633,0.671,0.786,0.728,0.722,0.75,0.552,0.723,0.679,0.572,0.536,0.516,ESM2 model (35M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
66,CARP (38M),Protein language model,0.701,0.008,0.709,0.673,0.713,0.647,0.765,0.637,0.679,0.765,0.724,0.722,0.739,0.567,0.718,0.647,0.561,0.525,0.497,CARP model (38M params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
67,ESM2 (8M),Protein language model,0.667,0.008,0.665,0.668,0.697,0.614,0.693,0.622,0.639,0.697,0.679,0.641,0.707,0.54,0.686,0.63,0.561,0.513,0.465,ESM2 model (8M params),"<a href='https://www.science.org/doi/abs/10.1126/science.ade2574'>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, Vol. 379.</a>"
68,ProtGPT2,Protein language model,0.654,0.01,0.67,0.62,0.654,0.619,0.706,0.614,0.644,0.697,0.683,0.666,0.677,0.556,0.681,0.622,0.557,0.51,0.428,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
69,Unirep,Protein language model,0.648,0.009,0.659,0.624,0.671,0.606,0.68,0.619,0.628,0.676,0.668,0.642,0.683,0.524,0.673,0.591,0.543,0.522,0.471,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
70,CARP (600K),Protein language model,0.637,0.009,0.659,0.593,0.655,0.588,0.687,0.605,0.622,0.673,0.654,0.64,0.681,0.53,0.659,0.588,0.55,0.513,0.459,CARP model (600K params),"<a href='https://www.biorxiv.org/content/10.1101/2022.05.19.492714v4'>Kevin K. Yang, Nicolo Fusi, Alex X. Lu (2023). Convolutions are competitive with transformers for protein sequence pretraining. BioRxiv.</a>"
