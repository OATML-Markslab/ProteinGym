<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model_name</th>
      <th>Model type</th>
      <th>Average_Spearman</th>
      <th>Bootstrap_standard_error_Spearman</th>
      <th>Function_Activity</th>
      <th>Function_Binding</th>
      <th>Function_Expression</th>
      <th>Function_OrganismalFitness</th>
      <th>Function_Stability</th>
      <th>Low_MSA_depth</th>
      <th>Medium_MSA_depth</th>
      <th>High_MSA_depth</th>
      <th>Taxa_Human</th>
      <th>Taxa_Other_Eukaryote</th>
      <th>Taxa_Prokaryote</th>
      <th>Taxa_Virus</th>
      <th>Model details</th>
      <th>References</th>
    </tr>
    <tr>
      <th>Model_rank</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>PoET (200M)</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.517</td>
      <td>0.000</td>
      <td>0.590</td>
      <td>NaN</td>
      <td>0.458</td>
      <td>0.525</td>
      <td>0.493</td>
      <td>0.283</td>
      <td>0.503</td>
      <td>0.516</td>
      <td>0.494</td>
      <td>0.499</td>
      <td>0.483</td>
      <td>0.561</td>
      <td>PoET (200M)</td>
      <td>&lt;a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'&gt;Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Progen2 M</td>
      <td>Protein language model</td>
      <td>0.465</td>
      <td>0.038</td>
      <td>0.510</td>
      <td>NaN</td>
      <td>0.464</td>
      <td>0.374</td>
      <td>0.511</td>
      <td>0.208</td>
      <td>0.472</td>
      <td>0.549</td>
      <td>0.527</td>
      <td>0.564</td>
      <td>0.421</td>
      <td>0.338</td>
      <td>Progen2 medium model (760M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Progen2 Base</td>
      <td>Protein language model</td>
      <td>0.464</td>
      <td>0.034</td>
      <td>0.533</td>
      <td>NaN</td>
      <td>0.460</td>
      <td>0.374</td>
      <td>0.489</td>
      <td>0.214</td>
      <td>0.426</td>
      <td>0.556</td>
      <td>0.551</td>
      <td>0.551</td>
      <td>0.331</td>
      <td>0.316</td>
      <td>Progen2 base model (760M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>4</th>
      <td>RITA L</td>
      <td>Protein language model</td>
      <td>0.457</td>
      <td>0.026</td>
      <td>0.466</td>
      <td>NaN</td>
      <td>0.456</td>
      <td>0.419</td>
      <td>0.488</td>
      <td>0.381</td>
      <td>0.457</td>
      <td>0.509</td>
      <td>0.510</td>
      <td>0.510</td>
      <td>0.350</td>
      <td>0.562</td>
      <td>RITA large model (680M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Tranception M no retrieval</td>
      <td>Protein language model</td>
      <td>0.453</td>
      <td>0.021</td>
      <td>0.469</td>
      <td>NaN</td>
      <td>0.408</td>
      <td>0.501</td>
      <td>0.434</td>
      <td>0.422</td>
      <td>0.390</td>
      <td>0.487</td>
      <td>0.480</td>
      <td>0.460</td>
      <td>0.334</td>
      <td>0.460</td>
      <td>Tranception Medium model (300M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>6</th>
      <td>RITA XL</td>
      <td>Protein language model</td>
      <td>0.449</td>
      <td>0.024</td>
      <td>0.475</td>
      <td>NaN</td>
      <td>0.434</td>
      <td>0.393</td>
      <td>0.496</td>
      <td>0.361</td>
      <td>0.471</td>
      <td>0.509</td>
      <td>0.513</td>
      <td>0.511</td>
      <td>0.367</td>
      <td>0.556</td>
      <td>RITA xlarge model (1.2B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Progen2 L</td>
      <td>Protein language model</td>
      <td>0.449</td>
      <td>0.034</td>
      <td>0.504</td>
      <td>NaN</td>
      <td>0.430</td>
      <td>0.360</td>
      <td>0.500</td>
      <td>0.214</td>
      <td>0.450</td>
      <td>0.547</td>
      <td>0.536</td>
      <td>0.537</td>
      <td>0.406</td>
      <td>0.317</td>
      <td>Progen2 large model (2.7B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Tranception L no retrieval</td>
      <td>Protein language model</td>
      <td>0.437</td>
      <td>0.021</td>
      <td>0.430</td>
      <td>NaN</td>
      <td>0.410</td>
      <td>0.445</td>
      <td>0.463</td>
      <td>0.430</td>
      <td>0.390</td>
      <td>0.523</td>
      <td>0.485</td>
      <td>0.490</td>
      <td>0.331</td>
      <td>0.541</td>
      <td>Tranception Large model (700M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>9</th>
      <td>RITA M</td>
      <td>Protein language model</td>
      <td>0.436</td>
      <td>0.015</td>
      <td>0.464</td>
      <td>NaN</td>
      <td>0.381</td>
      <td>0.444</td>
      <td>0.456</td>
      <td>0.366</td>
      <td>0.400</td>
      <td>0.511</td>
      <td>0.501</td>
      <td>0.468</td>
      <td>0.333</td>
      <td>0.501</td>
      <td>RITA medium model (300M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Progen2 XL</td>
      <td>Protein language model</td>
      <td>0.431</td>
      <td>0.024</td>
      <td>0.405</td>
      <td>NaN</td>
      <td>0.359</td>
      <td>0.431</td>
      <td>0.531</td>
      <td>0.382</td>
      <td>0.511</td>
      <td>0.531</td>
      <td>0.524</td>
      <td>0.561</td>
      <td>0.453</td>
      <td>0.465</td>
      <td>Progen2 xlarge model (6.4B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>11</th>
      <td>TranceptEVE M</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.424</td>
      <td>0.017</td>
      <td>0.514</td>
      <td>NaN</td>
      <td>0.426</td>
      <td>0.555</td>
      <td>0.202</td>
      <td>0.531</td>
      <td>0.127</td>
      <td>0.331</td>
      <td>0.313</td>
      <td>0.270</td>
      <td>0.161</td>
      <td>0.114</td>
      <td>TranceptEVE Medium model (Tranception Medium &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'&gt;Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. &amp; Marks, D.S. &amp;  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Progen2 S</td>
      <td>Protein language model</td>
      <td>0.424</td>
      <td>0.035</td>
      <td>0.505</td>
      <td>NaN</td>
      <td>0.363</td>
      <td>0.387</td>
      <td>0.441</td>
      <td>0.242</td>
      <td>0.383</td>
      <td>0.507</td>
      <td>0.493</td>
      <td>0.463</td>
      <td>0.356</td>
      <td>0.328</td>
      <td>Progen2 small model (150M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>13</th>
      <td>TranceptEVE L</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.412</td>
      <td>0.020</td>
      <td>0.474</td>
      <td>NaN</td>
      <td>0.428</td>
      <td>0.477</td>
      <td>0.269</td>
      <td>0.480</td>
      <td>0.162</td>
      <td>0.402</td>
      <td>0.354</td>
      <td>0.306</td>
      <td>0.184</td>
      <td>0.300</td>
      <td>TranceptEVE Large model (Tranception Large &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'&gt;Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. &amp; Marks, D.S. &amp;  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Tranception S no retrieval</td>
      <td>Protein language model</td>
      <td>0.410</td>
      <td>0.020</td>
      <td>0.419</td>
      <td>NaN</td>
      <td>0.372</td>
      <td>0.479</td>
      <td>0.372</td>
      <td>0.427</td>
      <td>0.345</td>
      <td>0.412</td>
      <td>0.384</td>
      <td>0.420</td>
      <td>0.321</td>
      <td>0.392</td>
      <td>Tranception Small model (85M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>15</th>
      <td>RITA S</td>
      <td>Protein language model</td>
      <td>0.397</td>
      <td>0.015</td>
      <td>0.436</td>
      <td>NaN</td>
      <td>0.348</td>
      <td>0.414</td>
      <td>0.390</td>
      <td>0.334</td>
      <td>0.338</td>
      <td>0.448</td>
      <td>0.429</td>
      <td>0.408</td>
      <td>0.304</td>
      <td>0.405</td>
      <td>RITA small model (85M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Tranception L</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.395</td>
      <td>0.020</td>
      <td>0.453</td>
      <td>NaN</td>
      <td>0.434</td>
      <td>0.456</td>
      <td>0.238</td>
      <td>0.434</td>
      <td>0.135</td>
      <td>0.372</td>
      <td>0.326</td>
      <td>0.274</td>
      <td>0.160</td>
      <td>0.264</td>
      <td>Tranception Large model (700M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Tranception M</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.394</td>
      <td>0.019</td>
      <td>0.472</td>
      <td>NaN</td>
      <td>0.400</td>
      <td>0.530</td>
      <td>0.174</td>
      <td>0.509</td>
      <td>0.102</td>
      <td>0.299</td>
      <td>0.284</td>
      <td>0.237</td>
      <td>0.139</td>
      <td>0.098</td>
      <td>Tranception Medium model (300M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Hidden Markov Model</td>
      <td>Alignment-based model</td>
      <td>0.389</td>
      <td>0.019</td>
      <td>0.463</td>
      <td>NaN</td>
      <td>0.302</td>
      <td>0.436</td>
      <td>0.354</td>
      <td>0.202</td>
      <td>0.408</td>
      <td>0.340</td>
      <td>0.393</td>
      <td>0.352</td>
      <td>0.337</td>
      <td>0.359</td>
      <td>Profile Hidden Markov model</td>
      <td>&lt;a href='http://hmmer.org/'&gt;HMMER: biosequence analysis using profile hidden Markov models&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Wavenet</td>
      <td>Alignment-based model</td>
      <td>0.368</td>
      <td>0.044</td>
      <td>0.418</td>
      <td>NaN</td>
      <td>0.232</td>
      <td>0.346</td>
      <td>0.476</td>
      <td>0.291</td>
      <td>0.472</td>
      <td>0.468</td>
      <td>0.477</td>
      <td>0.482</td>
      <td>0.394</td>
      <td>0.489</td>
      <td>Wavenet model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41467-021-22732-w'&gt;Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., &amp; Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>20</th>
      <td>TranceptEVE S</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.357</td>
      <td>0.017</td>
      <td>0.456</td>
      <td>NaN</td>
      <td>0.378</td>
      <td>0.497</td>
      <td>0.096</td>
      <td>0.514</td>
      <td>0.062</td>
      <td>0.194</td>
      <td>0.181</td>
      <td>0.163</td>
      <td>0.113</td>
      <td>0.062</td>
      <td>TranceptEVE Small model (Tranception Small &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Provean</td>
      <td>Alignment-based model</td>
      <td>0.347</td>
      <td>0.034</td>
      <td>0.314</td>
      <td>NaN</td>
      <td>0.328</td>
      <td>0.413</td>
      <td>0.334</td>
      <td>0.361</td>
      <td>0.333</td>
      <td>0.344</td>
      <td>0.318</td>
      <td>0.358</td>
      <td>0.332</td>
      <td>0.386</td>
      <td>Provean model</td>
      <td>&lt;a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'&gt;Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Tranception S</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.340</td>
      <td>0.021</td>
      <td>0.441</td>
      <td>NaN</td>
      <td>0.357</td>
      <td>0.488</td>
      <td>0.074</td>
      <td>0.505</td>
      <td>0.046</td>
      <td>0.169</td>
      <td>0.158</td>
      <td>0.140</td>
      <td>0.094</td>
      <td>0.062</td>
      <td>Tranception Small model (85M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>23</th>
      <td>ProtGPT2</td>
      <td>Protein language model</td>
      <td>0.191</td>
      <td>0.044</td>
      <td>0.194</td>
      <td>NaN</td>
      <td>0.236</td>
      <td>0.099</td>
      <td>0.235</td>
      <td>0.233</td>
      <td>0.188</td>
      <td>0.253</td>
      <td>0.287</td>
      <td>0.233</td>
      <td>0.150</td>
      <td>0.100</td>
      <td>ProtGPT2 model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41467-022-32007-7'&gt;Ferruz, N., Schmidt, S., &amp; HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Unirep</td>
      <td>Protein language model</td>
      <td>0.169</td>
      <td>0.071</td>
      <td>0.085</td>
      <td>NaN</td>
      <td>0.182</td>
      <td>0.065</td>
      <td>0.342</td>
      <td>0.110</td>
      <td>0.267</td>
      <td>0.358</td>
      <td>0.298</td>
      <td>0.364</td>
      <td>0.216</td>
      <td>0.345</td>
      <td>Unirep model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41592-019-0598-1'&gt;Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., &amp; Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.&lt;/a&gt;</td>
    </tr>
  </tbody>
</table>