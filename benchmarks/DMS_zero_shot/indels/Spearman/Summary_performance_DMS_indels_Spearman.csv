Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Model details,References
1,PoET (200M),Hybrid - Alignment & PLM,0.517,0.0,0.59,,0.458,0.525,0.493,0.283,0.503,0.516,0.494,0.499,0.483,0.561,PoET (200M),"<a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'>Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.</a>"
2,Progen2 M,Protein language model,0.465,0.038,0.51,,0.464,0.374,0.511,0.208,0.472,0.549,0.527,0.564,0.421,0.338,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
3,Progen2 Base,Protein language model,0.464,0.034,0.533,,0.46,0.374,0.489,0.214,0.426,0.556,0.551,0.551,0.331,0.316,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
4,RITA L,Protein language model,0.457,0.026,0.466,,0.456,0.419,0.488,0.381,0.457,0.509,0.51,0.51,0.35,0.562,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
5,Tranception M no retrieval,Protein language model,0.453,0.021,0.469,,0.408,0.501,0.434,0.422,0.39,0.487,0.48,0.46,0.334,0.46,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
6,RITA XL,Protein language model,0.449,0.024,0.475,,0.434,0.393,0.496,0.361,0.471,0.509,0.513,0.511,0.367,0.556,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
7,Progen2 L,Protein language model,0.449,0.034,0.504,,0.43,0.36,0.5,0.214,0.45,0.547,0.536,0.537,0.406,0.317,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
8,Tranception L no retrieval,Protein language model,0.437,0.021,0.43,,0.41,0.445,0.463,0.43,0.39,0.523,0.485,0.49,0.331,0.541,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
9,RITA M,Protein language model,0.436,0.015,0.464,,0.381,0.444,0.456,0.366,0.4,0.511,0.501,0.468,0.333,0.501,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
10,Progen2 XL,Protein language model,0.431,0.024,0.405,,0.359,0.431,0.531,0.382,0.511,0.531,0.524,0.561,0.453,0.465,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
11,TranceptEVE M,Hybrid - Alignment & PLM,0.424,0.017,0.514,,0.426,0.555,0.202,0.531,0.127,0.331,0.313,0.27,0.161,0.114,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
12,Progen2 S,Protein language model,0.424,0.035,0.505,,0.363,0.387,0.441,0.242,0.383,0.507,0.493,0.463,0.356,0.328,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
13,TranceptEVE L,Hybrid - Alignment & PLM,0.412,0.02,0.474,,0.428,0.477,0.269,0.48,0.162,0.402,0.354,0.306,0.184,0.3,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
14,Tranception S no retrieval,Protein language model,0.41,0.02,0.419,,0.372,0.479,0.372,0.427,0.345,0.412,0.384,0.42,0.321,0.392,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
15,RITA S,Protein language model,0.397,0.015,0.436,,0.348,0.414,0.39,0.334,0.338,0.448,0.429,0.408,0.304,0.405,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
16,Tranception L,Hybrid - Alignment & PLM,0.395,0.02,0.453,,0.434,0.456,0.238,0.434,0.135,0.372,0.326,0.274,0.16,0.264,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
17,Tranception M,Hybrid - Alignment & PLM,0.394,0.019,0.472,,0.4,0.53,0.174,0.509,0.102,0.299,0.284,0.237,0.139,0.098,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
18,Hidden Markov Model,Alignment-based model,0.389,0.019,0.463,,0.302,0.436,0.354,0.202,0.408,0.34,0.393,0.352,0.337,0.359,Profile Hidden Markov model,<a href='http://hmmer.org/'>HMMER: biosequence analysis using profile hidden Markov models</a>
19,Wavenet,Alignment-based model,0.368,0.044,0.418,,0.232,0.346,0.476,0.291,0.472,0.468,0.477,0.482,0.394,0.489,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
20,TranceptEVE S,Hybrid - Alignment & PLM,0.357,0.017,0.456,,0.378,0.497,0.096,0.514,0.062,0.194,0.181,0.163,0.113,0.062,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
21,Provean,Alignment-based model,0.347,0.034,0.314,,0.328,0.413,0.334,0.361,0.333,0.344,0.318,0.358,0.332,0.386,Provean model,"<a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'>Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.</a>"
22,Tranception S,Hybrid - Alignment & PLM,0.34,0.021,0.441,,0.357,0.488,0.074,0.505,0.046,0.169,0.158,0.14,0.094,0.062,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
23,ProtGPT2,Protein language model,0.191,0.044,0.194,,0.236,0.099,0.235,0.233,0.188,0.253,0.287,0.233,0.15,0.1,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
24,Unirep,Protein language model,0.169,0.071,0.085,,0.182,0.065,0.342,0.11,0.267,0.358,0.298,0.364,0.216,0.345,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
