Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Model details,References
1,Progen2 M,Protein language model,0.467,0.0,0.51,,0.466,0.374,0.517,0.208,0.475,0.556,0.534,0.57,0.427,0.335,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
2,Progen2 Base,Protein language model,0.466,0.01,0.533,,0.462,0.374,0.494,0.214,0.427,0.563,0.558,0.556,0.333,0.312,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
3,RITA L,Protein language model,0.459,0.034,0.466,,0.456,0.419,0.495,0.382,0.462,0.517,0.517,0.516,0.356,0.564,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
4,Tranception M no retrieval,Protein language model,0.455,0.036,0.47,,0.41,0.501,0.439,0.422,0.394,0.492,0.484,0.466,0.34,0.46,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
5,RITA XL,Protein language model,0.452,0.038,0.475,,0.435,0.393,0.503,0.361,0.476,0.517,0.52,0.518,0.375,0.557,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
6,Progen2 L,Protein language model,0.451,0.011,0.505,,0.432,0.36,0.506,0.214,0.454,0.554,0.543,0.542,0.412,0.315,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
7,Tranception L no retrieval,Protein language model,0.439,0.041,0.431,,0.412,0.445,0.469,0.43,0.395,0.529,0.491,0.495,0.339,0.542,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
8,RITA M,Protein language model,0.439,0.03,0.464,,0.382,0.444,0.463,0.366,0.405,0.519,0.508,0.474,0.341,0.503,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
9,Progen2 XL,Protein language model,0.434,0.036,0.406,,0.36,0.431,0.539,0.382,0.516,0.539,0.532,0.568,0.46,0.465,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
10,TranceptEVE M,Hybrid model,0.428,0.045,0.514,,0.428,0.555,0.216,0.531,0.14,0.342,0.323,0.282,0.175,0.126,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
11,Progen2 S,Protein language model,0.425,0.025,0.506,,0.365,0.387,0.444,0.242,0.381,0.514,0.499,0.466,0.355,0.325,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
12,TranceptEVE L,Hybrid model,0.416,0.046,0.475,,0.429,0.477,0.282,0.481,0.175,0.413,0.364,0.318,0.197,0.309,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
13,Tranception S no retrieval,Protein language model,0.412,0.036,0.42,,0.373,0.479,0.376,0.427,0.347,0.417,0.388,0.422,0.327,0.393,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
14,Tranception L,Hybrid model,0.399,0.044,0.454,,0.436,0.456,0.251,0.434,0.148,0.384,0.335,0.287,0.174,0.273,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
15,RITA S,Protein language model,0.399,0.032,0.437,,0.35,0.414,0.395,0.334,0.341,0.455,0.435,0.414,0.309,0.406,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
16,Tranception M,Hybrid model,0.398,0.047,0.473,,0.402,0.53,0.188,0.509,0.116,0.311,0.294,0.25,0.153,0.11,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
17,Hidden Markov Model,Alignment-based model,0.391,0.046,0.464,,0.302,0.436,0.363,0.202,0.416,0.35,0.4,0.362,0.345,0.367,Profile Hidden Markov model,<a href='http://hmmer.org/'>HMMER: biosequence analysis using profile hidden Markov models</a>
18,TranceptEVE S,Hybrid model,0.361,0.049,0.457,,0.38,0.497,0.111,0.514,0.075,0.208,0.192,0.177,0.128,0.074,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
19,Provean,Alignment-based model,0.351,0.046,0.315,,0.329,0.413,0.347,0.361,0.344,0.356,0.328,0.37,0.343,0.396,Provean model,"<a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'>Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.</a>"
20,Tranception S,Hybrid model,0.344,0.053,0.442,,0.358,0.488,0.089,0.505,0.059,0.183,0.169,0.154,0.109,0.073,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
21,Wavenet,Alignment-based model,0.285,0.067,0.419,,0.234,0.346,0.141,0.17,0.247,0.108,0.057,0.25,0.186,0.358,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
22,ProtGPT2,Protein language model,0.194,0.053,0.195,,0.238,0.099,0.242,0.233,0.192,0.263,0.295,0.24,0.155,0.104,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
23,Unirep,Protein language model,0.169,0.06,0.085,,0.184,0.065,0.344,0.11,0.268,0.36,0.3,0.364,0.218,0.348,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
