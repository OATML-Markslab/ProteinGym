Model_rank,Model_name,Model type,Average_AUC,Bootstrap_standard_error_AUC,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Model details,References
1,Progen2 Base,Protein language model,0.772,0.0,0.817,,0.812,0.711,0.749,0.67,0.723,0.782,0.787,0.789,0.666,0.669,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
2,Progen2 M,Protein language model,0.769,0.007,0.797,,0.808,0.713,0.759,0.664,0.744,0.779,0.773,0.797,0.707,0.683,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
3,RITA L,Protein language model,0.764,0.019,0.774,,0.796,0.74,0.746,0.766,0.733,0.757,0.764,0.764,0.669,0.802,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
4,Tranception M no retrieval,Protein language model,0.762,0.02,0.776,,0.772,0.784,0.716,0.783,0.698,0.744,0.749,0.736,0.657,0.744,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
5,RITA XL,Protein language model,0.761,0.02,0.778,,0.784,0.733,0.749,0.762,0.739,0.757,0.764,0.769,0.678,0.794,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
6,Progen2 L,Protein language model,0.761,0.005,0.801,,0.784,0.707,0.752,0.669,0.729,0.779,0.779,0.78,0.7,0.662,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
7,Tranception L no retrieval,Protein language model,0.76,0.024,0.756,,0.784,0.77,0.731,0.809,0.701,0.761,0.753,0.754,0.659,0.792,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
8,RITA M,Protein language model,0.756,0.016,0.78,,0.765,0.748,0.731,0.752,0.703,0.763,0.764,0.746,0.657,0.764,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
9,TranceptEVE M,Hybrid model,0.753,0.024,0.81,,0.783,0.816,0.602,0.841,0.574,0.663,0.667,0.649,0.575,0.555,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
10,TranceptEVE L,Hybrid model,0.75,0.024,0.788,,0.796,0.783,0.635,0.829,0.59,0.701,0.691,0.666,0.584,0.656,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
11,Progen2 XL,Protein language model,0.749,0.02,0.73,,0.747,0.753,0.768,0.773,0.758,0.77,0.771,0.789,0.724,0.751,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
12,Progen2 S,Protein language model,0.749,0.011,0.802,,0.748,0.72,0.726,0.676,0.703,0.758,0.759,0.743,0.685,0.67,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
13,Tranception S no retrieval,Protein language model,0.745,0.018,0.756,,0.761,0.778,0.685,0.778,0.679,0.705,0.698,0.72,0.657,0.706,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
14,Tranception L,Hybrid model,0.742,0.024,0.778,,0.797,0.774,0.62,0.802,0.577,0.686,0.678,0.65,0.572,0.637,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
15,RITA S,Protein language model,0.737,0.014,0.767,,0.74,0.743,0.697,0.729,0.68,0.724,0.722,0.714,0.652,0.724,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
16,Tranception M,Hybrid model,0.735,0.025,0.784,,0.761,0.808,0.588,0.838,0.562,0.648,0.654,0.632,0.565,0.546,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
17,Hidden Markov Model,Alignment-based model,0.731,0.022,0.779,,0.718,0.755,0.672,0.587,0.71,0.672,0.701,0.677,0.664,0.697,Profile Hidden Markov model,<a href='http://hmmer.org/'>HMMER: biosequence analysis using profile hidden Markov models</a>
18,TranceptEVE S,Hybrid model,0.721,0.024,0.784,,0.766,0.787,0.548,0.821,0.544,0.593,0.596,0.596,0.558,0.53,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
19,Provean,Alignment-based model,0.713,0.027,0.7,,0.746,0.737,0.669,0.764,0.673,0.674,0.665,0.688,0.671,0.711,Provean model,"<a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'>Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.</a>"
20,Tranception S,Hybrid model,0.712,0.025,0.773,,0.752,0.783,0.538,0.815,0.536,0.581,0.585,0.585,0.549,0.528,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
21,Wavenet,Alignment-based model,0.667,0.037,0.747,,0.659,0.698,0.563,0.6,0.622,0.551,0.532,0.624,0.582,0.679,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
22,ProtGPT2,Protein language model,0.615,0.031,0.628,,0.658,0.559,0.617,0.678,0.594,0.627,0.656,0.619,0.563,0.552,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
23,Unirep,Protein language model,0.589,0.04,0.569,,0.6,0.516,0.67,0.551,0.634,0.676,0.648,0.684,0.603,0.668,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
