Model_rank,Model_name,Model type,Average_Top_recall,Bootstrap_standard_error_Top_recall,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Model details,References
1,RITA XL,Protein language model,0.304,0.0,0.266,,0.218,0.298,0.436,0.25,0.375,0.461,0.513,0.337,0.375,0.347,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
2,Progen2 M,Protein language model,0.299,0.016,0.274,,0.236,0.248,0.439,0.17,0.376,0.467,0.49,0.401,0.381,0.205,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
3,Progen2 Base,Protein language model,0.294,0.015,0.301,,0.231,0.229,0.413,0.146,0.344,0.451,0.506,0.357,0.32,0.18,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
4,Progen2 XL,Protein language model,0.29,0.013,0.24,,0.169,0.26,0.491,0.25,0.42,0.505,0.516,0.488,0.388,0.255,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
5,Provean,Alignment-based model,0.28,0.02,0.229,,0.162,0.263,0.466,0.196,0.424,0.463,0.441,0.448,0.44,0.337,Provean model,"<a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'>Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.</a>"
6,RITA L,Protein language model,0.274,0.012,0.231,,0.218,0.234,0.412,0.165,0.335,0.45,0.475,0.358,0.306,0.296,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
7,Progen2 L,Protein language model,0.269,0.026,0.192,,0.2,0.248,0.435,0.162,0.36,0.468,0.481,0.372,0.413,0.19,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
8,RITA M,Protein language model,0.265,0.018,0.208,,0.222,0.255,0.377,0.16,0.307,0.417,0.439,0.292,0.324,0.305,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
9,Progen2 S,Protein language model,0.265,0.025,0.243,,0.238,0.223,0.357,0.137,0.262,0.428,0.419,0.3,0.3,0.24,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
10,Tranception M no retrieval,Protein language model,0.264,0.014,0.247,,0.205,0.306,0.299,0.214,0.242,0.349,0.334,0.276,0.286,0.217,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
11,Tranception L no retrieval,Protein language model,0.262,0.011,0.215,,0.184,0.303,0.349,0.266,0.299,0.374,0.38,0.302,0.322,0.297,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
12,TranceptEVE M,Hybrid model,0.259,0.017,0.233,,0.211,0.318,0.276,0.246,0.203,0.342,0.318,0.262,0.249,0.201,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
13,Tranception M,Hybrid model,0.256,0.016,0.267,,0.176,0.312,0.267,0.294,0.201,0.326,0.316,0.252,0.238,0.201,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
14,RITA S,Protein language model,0.255,0.018,0.2,,0.244,0.25,0.325,0.176,0.261,0.37,0.367,0.283,0.287,0.241,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
15,TranceptEVE L,Hybrid model,0.251,0.017,0.213,,0.176,0.301,0.315,0.291,0.256,0.351,0.342,0.283,0.288,0.273,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
16,Tranception L,Hybrid model,0.25,0.015,0.231,,0.178,0.296,0.297,0.269,0.23,0.348,0.336,0.269,0.258,0.263,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
17,Tranception S no retrieval,Protein language model,0.249,0.016,0.252,,0.247,0.269,0.226,0.191,0.201,0.259,0.274,0.175,0.268,0.146,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
18,TranceptEVE S,Hybrid model,0.234,0.018,0.219,,0.216,0.285,0.215,0.228,0.164,0.269,0.247,0.189,0.233,0.181,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
19,Wavenet,Alignment-based model,0.232,0.032,0.156,,0.121,0.226,0.426,0.168,0.428,0.383,0.401,0.408,0.39,0.322,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
20,Tranception S,Hybrid model,0.228,0.018,0.238,,0.23,0.251,0.193,0.201,0.155,0.239,0.228,0.161,0.223,0.154,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
21,Hidden Markov Model,Alignment-based model,0.217,0.037,0.228,,0.13,0.228,0.281,0.138,0.282,0.275,0.316,0.252,0.233,0.247,Profile Hidden Markov model,<a href='http://hmmer.org/'>HMMER: biosequence analysis using profile hidden Markov models</a>
22,ProtGPT2,Protein language model,0.155,0.029,0.139,,0.134,0.108,0.24,0.11,0.163,0.287,0.327,0.194,0.13,0.113,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
23,Unirep,Protein language model,0.154,0.034,0.133,,0.166,0.112,0.206,0.124,0.154,0.238,0.219,0.179,0.147,0.257,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
