Model_rank,Model_name,Model type,Average_MCC,Bootstrap_standard_error_MCC,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Model details,References
1,Progen2 Base,Protein language model,0.365,0.0,0.471,,0.373,0.275,0.343,0.224,0.312,0.383,0.386,0.393,0.249,0.226,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
2,Tranception M no retrieval,Protein language model,0.361,0.031,0.409,,0.33,0.393,0.311,0.417,0.292,0.339,0.334,0.354,0.248,0.335,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
3,Hidden Markov Model,Alignment-based model,0.357,0.028,0.432,,0.338,0.365,0.292,0.091,0.344,0.29,0.299,0.341,0.28,0.269,Profile Hidden Markov model,<a href='http://hmmer.org/'>HMMER: biosequence analysis using profile hidden Markov models</a>
4,Progen2 M,Protein language model,0.356,0.015,0.423,,0.366,0.283,0.354,0.246,0.343,0.367,0.357,0.396,0.299,0.29,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
5,TranceptEVE M,Hybrid model,0.35,0.033,0.45,,0.372,0.421,0.157,0.501,0.083,0.266,0.245,0.196,0.141,0.11,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
6,Progen2 L,Protein language model,0.349,0.009,0.433,,0.339,0.28,0.343,0.241,0.319,0.372,0.372,0.368,0.301,0.231,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
7,RITA L,Protein language model,0.346,0.033,0.375,,0.343,0.312,0.355,0.376,0.329,0.368,0.357,0.389,0.257,0.414,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
8,RITA XL,Protein language model,0.341,0.03,0.401,,0.326,0.289,0.349,0.358,0.324,0.362,0.359,0.378,0.248,0.402,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
9,Tranception L no retrieval,Protein language model,0.341,0.035,0.36,,0.331,0.344,0.328,0.411,0.282,0.366,0.339,0.372,0.225,0.396,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
10,RITA M,Protein language model,0.337,0.023,0.387,,0.31,0.326,0.326,0.38,0.29,0.357,0.364,0.329,0.24,0.385,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
11,TranceptEVE L,Hybrid model,0.336,0.034,0.41,,0.38,0.361,0.194,0.438,0.105,0.302,0.268,0.207,0.142,0.244,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
12,Progen2 S,Protein language model,0.335,0.015,0.45,,0.304,0.293,0.294,0.269,0.286,0.316,0.335,0.318,0.245,0.234,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
13,Tranception S no retrieval,Protein language model,0.326,0.025,0.367,,0.31,0.359,0.269,0.391,0.236,0.311,0.272,0.316,0.228,0.321,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
14,Tranception M,Hybrid model,0.325,0.033,0.417,,0.344,0.406,0.132,0.482,0.06,0.241,0.226,0.166,0.117,0.089,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
15,Tranception L,Hybrid model,0.318,0.034,0.389,,0.374,0.331,0.177,0.404,0.089,0.284,0.246,0.188,0.138,0.216,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
16,RITA S,Protein language model,0.315,0.022,0.383,,0.298,0.313,0.269,0.309,0.242,0.306,0.28,0.305,0.206,0.339,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
17,Progen2 XL,Protein language model,0.31,0.028,0.306,,0.266,0.304,0.364,0.351,0.355,0.356,0.365,0.361,0.316,0.39,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
18,TranceptEVE S,Hybrid model,0.3,0.033,0.427,,0.32,0.378,0.074,0.451,0.032,0.165,0.158,0.1,0.086,0.089,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
19,Wavenet,Alignment-based model,0.295,0.047,0.352,,0.209,0.264,0.355,0.304,0.349,0.351,0.361,0.346,0.305,0.405,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
20,Tranception S,Hybrid model,0.287,0.034,0.402,,0.306,0.38,0.059,0.45,0.024,0.145,0.14,0.09,0.072,0.084,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
21,Provean,Alignment-based model,0.273,0.041,0.263,,0.296,0.305,0.229,0.339,0.242,0.225,0.18,0.249,0.274,0.351,Provean model,"<a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'>Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.</a>"
22,ProtGPT2,Protein language model,0.151,0.045,0.181,,0.194,0.042,0.186,0.236,0.135,0.205,0.239,0.176,0.109,0.066,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
23,Unirep,Protein language model,0.13,0.062,0.098,,0.146,0.037,0.239,0.135,0.194,0.238,0.226,0.252,0.135,0.215,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
