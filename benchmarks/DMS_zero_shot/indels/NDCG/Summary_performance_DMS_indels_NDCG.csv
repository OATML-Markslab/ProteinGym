Model_rank,Model_name,Model type,Average_NDCG,Bootstrap_standard_error_NDCG,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,Model details,References
1,PoET (200M),Hybrid - Alignment & PLM,0.767,0.0,0.808,,0.801,0.65,0.807,0.629,0.816,0.79,0.804,0.791,0.767,0.831,PoET (200M),"<a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'>Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.</a>"
2,Progen2 M,Protein language model,0.763,0.016,0.796,,0.82,0.595,0.84,0.642,0.789,0.861,0.878,0.815,0.775,0.694,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
3,Progen2 Base,Protein language model,0.751,0.017,0.812,,0.814,0.568,0.811,0.623,0.751,0.844,0.88,0.795,0.674,0.701,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
4,Progen2 XL,Protein language model,0.751,0.013,0.765,,0.78,0.58,0.878,0.718,0.834,0.872,0.882,0.864,0.784,0.808,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
5,Progen2 L,Protein language model,0.748,0.016,0.781,,0.802,0.571,0.839,0.637,0.791,0.854,0.87,0.802,0.793,0.693,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
6,Tranception L no retrieval,Protein language model,0.744,0.008,0.79,,0.786,0.643,0.755,0.724,0.73,0.767,0.79,0.686,0.739,0.811,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
7,Tranception M no retrieval,Protein language model,0.741,0.012,0.798,,0.803,0.645,0.719,0.64,0.695,0.747,0.744,0.698,0.704,0.715,Tranception Medium model (300M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
8,Provean,Alignment-based model,0.74,0.015,0.759,,0.77,0.58,0.851,0.706,0.822,0.838,0.836,0.848,0.771,0.829,Provean model,"<a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'>Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.</a>"
9,RITA XL,Protein language model,0.74,0.009,0.746,,0.782,0.612,0.821,0.718,0.769,0.838,0.869,0.769,0.735,0.788,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
10,RITA L,Protein language model,0.736,0.013,0.78,,0.796,0.564,0.805,0.626,0.745,0.835,0.856,0.77,0.689,0.773,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
11,TranceptEVE L,Hybrid - Alignment & PLM,0.727,0.011,0.775,,0.77,0.641,0.722,0.731,0.685,0.749,0.742,0.703,0.679,0.774,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
12,TranceptEVE M,Hybrid - Alignment & PLM,0.726,0.014,0.788,,0.786,0.652,0.679,0.669,0.64,0.725,0.707,0.686,0.649,0.667,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
13,RITA M,Protein language model,0.726,0.015,0.767,,0.795,0.552,0.788,0.605,0.719,0.831,0.842,0.732,0.698,0.773,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
14,Tranception L,Hybrid - Alignment & PLM,0.721,0.011,0.772,,0.768,0.642,0.701,0.718,0.671,0.728,0.727,0.684,0.656,0.766,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
15,Progen2 S,Protein language model,0.718,0.018,0.766,,0.784,0.549,0.773,0.594,0.702,0.821,0.823,0.701,0.724,0.745,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
16,Tranception S no retrieval,Protein language model,0.706,0.011,0.763,,0.788,0.625,0.649,0.71,0.623,0.679,0.693,0.597,0.665,0.675,Tranception Small model (85M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
17,Wavenet,Alignment-based model,0.693,0.031,0.703,,0.709,0.539,0.823,0.627,0.818,0.791,0.802,0.811,0.75,0.828,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
18,Tranception M,Hybrid - Alignment & PLM,0.691,0.019,0.752,,0.728,0.616,0.667,0.684,0.628,0.704,0.695,0.67,0.62,0.674,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
19,TranceptEVE S,Hybrid - Alignment & PLM,0.688,0.012,0.747,,0.766,0.634,0.605,0.738,0.577,0.644,0.646,0.595,0.6,0.624,TranceptEVE Small model (Tranception Small & retrieved EVE model),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
20,RITA S,Protein language model,0.687,0.015,0.699,,0.748,0.579,0.721,0.669,0.659,0.761,0.759,0.687,0.666,0.695,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
21,Tranception S,Hybrid - Alignment & PLM,0.677,0.015,0.741,,0.757,0.617,0.591,0.726,0.564,0.627,0.63,0.581,0.583,0.622,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
22,Hidden Markov Model,Alignment-based model,0.642,0.034,0.672,,0.66,0.529,0.708,0.549,0.74,0.666,0.728,0.66,0.668,0.734,Profile Hidden Markov model,<a href='http://hmmer.org/'>HMMER: biosequence analysis using profile hidden Markov models</a>
23,ProtGPT2,Protein language model,0.537,0.038,0.519,,0.652,0.344,0.634,0.526,0.575,0.645,0.671,0.589,0.525,0.61,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
24,Unirep,Protein language model,0.527,0.045,0.526,,0.656,0.329,0.599,0.504,0.558,0.599,0.571,0.559,0.553,0.707,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
