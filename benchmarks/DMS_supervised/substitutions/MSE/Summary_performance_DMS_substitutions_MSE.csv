Model_rank,Model_name,Model type,Average_MSE,Bootstrap_standard_error_MSE,Average_MSE_fold_random_5,Average_MSE_fold_modulo_5,Average_MSE_fold_contiguous_5,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,References,Model details
1,Kermut,Structure & sequence embedding,0.605,0.0,0.413,0.666,0.737,0.632,0.895,0.528,0.672,0.3,0.437,0.683,0.577,0.519,0.537,0.534,0.595,"<a href='https://openreview.net/forum?id=jM9atrvUii'>Groth, P. M., Kerrn, M. H., Olsen, L., Salomon, J., & Boomsma, W. (2024). Kermut: Composite kernel regression for protein variant effects. Thirty-Eighth Conference on Neural Information Processing Systems</a>",Kermut GP
2,ProteinNPT,Sequence embedding,0.687,0.017,0.441,0.765,0.856,0.695,1.034,0.591,0.751,0.365,0.508,0.743,0.655,0.588,0.609,0.628,0.641,"<a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ProteinNPT Model
3,MSA Transformer Embeddings,Sequence embedding,0.724,0.022,0.529,0.783,0.86,0.696,1.105,0.655,0.755,0.409,0.523,0.748,0.695,0.599,0.638,0.638,0.679,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer Embeddings
4,ESM-1v Embeddings,Sequence embedding,0.789,0.026,0.603,0.848,0.914,0.789,1.179,0.697,0.816,0.462,0.565,0.878,0.742,0.724,0.67,0.695,0.782,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v Embeddings
5,Tranception Embeddings,Sequence embedding,0.801,0.018,0.518,0.849,1.037,0.834,1.121,0.68,0.813,0.558,0.679,0.823,0.769,0.755,0.727,0.765,0.725,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception Embeddings
6,TranceptEVE + One-Hot Encodings,One-hot Encoding,0.88,0.018,0.74,0.936,0.964,0.789,1.223,0.79,0.847,0.749,0.773,0.912,0.842,0.853,0.791,0.864,0.818,"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",TranceptEVE + One-Hot Encodings
7,ESM-1v + One-Hot Encodings,One-hot Encoding,0.885,0.019,0.717,0.958,0.981,0.836,1.185,0.787,0.877,0.742,0.742,0.97,0.866,0.869,0.803,0.856,0.869,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v + One-Hot Encodings
8,MSA_Transformer + One-Hot Encodings,One-hot Encoding,0.891,0.02,0.746,0.953,0.975,0.804,1.268,0.783,0.856,0.745,0.767,0.943,0.848,0.856,0.807,0.866,0.803,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer + One-Hot Encodings
9,DeepSequence + One-Hot Encodings,One-hot Encoding,0.9,0.017,0.763,0.961,0.978,0.827,1.165,0.845,0.877,0.788,0.8,0.942,0.876,0.888,0.824,0.876,0.868,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",DeepSequence + One-Hot Encodings
10,Tranception + One-Hot Encodings,One-hot Encoding,0.905,0.019,0.752,0.969,0.994,0.833,1.272,0.798,0.873,0.749,0.776,0.947,0.87,0.877,0.814,0.88,0.832,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception + One-Hot Encodings
11,One-Hot Encodings,One-hot Encoding,1.064,0.016,0.893,1.137,1.161,1.022,1.319,0.991,1.049,0.939,0.997,1.089,1.026,1.037,1.005,1.056,0.99,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",One-Hot Encodings
